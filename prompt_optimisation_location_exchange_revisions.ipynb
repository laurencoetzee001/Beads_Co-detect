{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14Okg5C9TdBj8d_ZlFR3X_fWqG_QScN2J",
      "authorship_tag": "ABX9TyMyHm4uemtszDJNIH6IJ3eH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurencoetzee001/Beads_Co-detect/blob/main/prompt_optimisation_location_exchange_revisions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - 27,060 Rows Google Colab PRODUCTION\n",
        "========================================================\n",
        "\n",
        "COMPLETE IMPLEMENTATION:\n",
        "✓ All 13 fields from codebook (with proper codes)\n",
        "✓ Enhanced 4a_exchange rules (73% human-AI agreement)\n",
        "✓ Two-layer location extraction (spellings + context)\n",
        "✓ Proper Excel flattening (29 output columns)\n",
        "✓ AGGRESSIVE AUTO-SAVE (every 100 rows)\n",
        "✓ BATCH PROCESSING (2,000 rows per session)\n",
        "✓ Fresh start from Row 0\n",
        "✓ Real-time progress output\n",
        "✓ Google Colab integration\n",
        "\n",
        "All variables needed for analysis included\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Force unbuffered output\n",
        "sys.stdout = os.__stdout__\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "print(\"Checking dependencies...\")\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === GOOGLE DRIVE SETUP ===\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    GDRIVE_BASE = '/content/drive/MyDrive/bead_coding_27k_production'\n",
        "    os.makedirs(GDRIVE_BASE, exist_ok=True)\n",
        "    IN_COLAB = True\n",
        "    print(\"✓ Google Drive mounted\\n\")\n",
        "except:\n",
        "    GDRIVE_BASE = './bead_coding_27k_production'\n",
        "    os.makedirs(GDRIVE_BASE, exist_ok=True)\n",
        "    IN_COLAB = False\n",
        "    print(\"⚠ Running locally\\n\")\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"All_entries_beads_cleaned.xlsx\"\n",
        "OUTPUT_BASE = GDRIVE_BASE\n",
        "LOG_FILE = os.path.join(OUTPUT_BASE, \"processing.log\")\n",
        "STATE_FILE = os.path.join(OUTPUT_BASE, \"session_state.json\")\n",
        "AUTOSAVE_EVERY = 100  # Save to Excel every 100 rows\n",
        "ROWS_PER_SESSION = 2000\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "MAX_SESSION_MINUTES = 690\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "\n",
        "# === COMPREHENSIVE SYSTEM PROMPT (13 fields + enhanced rules) ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract ALL 13 structured data fields. Be conservative - require explicit evidence.\n",
        "\n",
        "RESPONSE FORMAT: Return ONLY a JSON object. NO markdown, NO ```json``` tags, NO extra text.\n",
        "\n",
        "JSON STRUCTURE (13 fields):\n",
        "{\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": \"number or measurement, or null\",\n",
        "    \"currency\": \"currency/commodity, or null\",\n",
        "    \"description\": \"full price text from source\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exact size text from source\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact color text, REQUIRED if code=14\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"names\": \"actual location names\"\n",
        "  },\n",
        "  \"4_location_names_original_spellings\": \"exact historical spellings (e.g. Bontuku, Whydah)\",\n",
        "  \"4_location_context\": \"Location | Geographic Context | Strategic Significance\",\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"description\": \"detailed function text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [array of 1-12],\n",
        "    \"description\": \"exact shape text\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact material text\"\n",
        "  },\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": \"1|2|null\",\n",
        "    \"names\": [\"array of names\"] or null\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [array of 1-31],\n",
        "    \"description\": \"detailed exchange items text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": 1-4 or null,\n",
        "    \"description\": \"exact measurement text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exchange nature text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context\"\n",
        "}\n",
        "\n",
        "FIELD CODES:\n",
        "\n",
        "1_price_HUMAN: status: yes=mentioned, no=not mentioned, xo=exchanged\n",
        "\n",
        "2_size_HUMAN: 1=large, 2=medium, 3=small, 4=various, 5=thin, 6=thick\n",
        "\n",
        "3_colour_HUMAN: 1=red, 2=blue, 3=white, 4=pink, 5=coral, 6=amber, 7=copper, 8=green, 9=yellow, 10=transparent, 11=seed glass, 12=black, 13=multicoloured, 14=other\n",
        "\n",
        "4_location_HUMAN: 1=mountain/hill, 2=lake, 3=river/waterfall, 4=populated place\n",
        "\n",
        "5_function_HUMAN: 1=jewellery/adornment, 2=currency/exchange, 3=ceremonial/religious, 4=status/gift\n",
        "\n",
        "6_origin_of_bead: Text describing geographic origin\n",
        "\n",
        "7_shape_HUMAN: 1=round, 2=tubular, 3=square, 4=oval, 5=oblong, 6=punched, 7=wound, 8=pressed, 9=decorative, 10=faceted, 11=bugle, 12=chevron\n",
        "\n",
        "8_type_bead_HUMAN: 1=glass, 2=clay, 3=metal, 4=stone, 5=coral, 6=amber, 7=bone, 8=ivory, 9=dried seed, 10=ceramic, 11=wooden, 12=porcelain, 13=shell, 14=eggshell\n",
        "\n",
        "9_local_name_HUMAN: exists: 1=yes (provide names), 2=unspecified\n",
        "\n",
        "10_relationship_: 1=wire, 2=cloth, 3=shells, 4=coins, 5=livestock, 6=iron bars, 7=scarabs, 8=precious stones, 9=antiquities, 10=ostrich feathers, 11=ebony/ivory, 12=salt, 13=rubber/gum, 14=medicines, 15=spices/perfumes, 16=wax/seals, 17=leather/hides, 18=weapons, 19=dried food, 20=prints/books, 21=guns/gunpowder, 22=jewellery, 23=textiles, 24=gold/silver, 25=slaves, 26=glass objects, 27=hardware, 28=tobacco, 29=musical instruments, 30=water, 31=alcohol\n",
        "\n",
        "11_units_of_measure: 1=string, 2=plaited/woven string, 3=necklace/bracelet/waist beads, 4=other\n",
        "\n",
        "12_bead_ethnic_: Array of ethnic group names\n",
        "\n",
        "13_nature_of_exchange: 1=consensual, 2=conflictual, 3=unspecified, 4=competitive/bartering, 5=social/gifts, 6=uncommercial\n",
        "\n",
        "=== ENHANCED RULES FOR 4a_EXCHANGE ===\n",
        "\n",
        "For 1_price_HUMAN status field:\n",
        "- \"yes\" = price mentioned\n",
        "- \"no\" = price not mentioned\n",
        "- \"xo\" = exchanged (rare, for historical exchange events)\n",
        "\n",
        "CRITICAL: Only code \"xo\" for explicit past transactions:\n",
        "  ✓ \"traded\", \"sold\", \"bought\", \"exchanged\", \"gave\", \"received\"\n",
        "  ✓ Past tense action\n",
        "  ✓ Clear parties involved\n",
        "  ✓ Specific items exchanged\n",
        "\n",
        "✗ DO NOT code \"xo\" for:\n",
        "  ✗ Value descriptions (\"beads are worth...\")\n",
        "  ✗ Manufacturing references (\"making beads\")\n",
        "  ✗ Hypothetical (\"would trade\")\n",
        "  ✗ General statements (\"natives trade beads\")\n",
        "  ✗ Observational (\"showed beads\")\n",
        "  ✗ Decorative usage\n",
        "\n",
        "=== LOCATION EXTRACTION (TWO LAYERS) ===\n",
        "\n",
        "Layer 1: 4_location_names_original_spellings\n",
        "- Preserve EXACT spellings from historical text\n",
        "- Include diacritics: Bontúku, Dahomey, Whydah\n",
        "- Order by first mention in text\n",
        "\n",
        "Layer 2: 4_location_context\n",
        "- Geographic location: coastal, inland, near river, etc.\n",
        "- Strategic significance: trade hub, route junction, etc.\n",
        "- Political context: territory of X people, colonial region, etc.\n",
        "- Format: \"Location | Geographic | Strategic | Political\"\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Return ONLY the JSON object - NO ```json``` tags, NO markdown, NO extra text\n",
        "2. Use null for missing data (not \"unknown\")\n",
        "3. For arrays: [] if no data, null if not applicable\n",
        "4. ALWAYS include description fields with verbatim text\n",
        "5. When code=14 (other), description is MANDATORY\n",
        "6. Base answers ONLY on provided text\n",
        "7. Preserve exact terminology and details\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical text and extract ALL 13 fields:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY the JSON object. No ```json``` tags, no other text.\"\"\"\n",
        "\n",
        "# === SESSION STATE ===\n",
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.file_path = STATE_FILE\n",
        "        self.state = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        # Fresh start from row 0\n",
        "        return {\n",
        "            'session_num': 1,\n",
        "            'last_row': -1,\n",
        "            'sessions_completed': 0,\n",
        "            'total_tokens_used': 0,\n",
        "            'total_cost': 0,\n",
        "            'batches_completed': []\n",
        "        }\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.file_path, 'w') as f:\n",
        "            json.dump(self.state, f, indent=2)\n",
        "\n",
        "    def get_batch_info(self, total_rows=27060):\n",
        "        session_num = self.state['session_num']\n",
        "        start_row = (session_num - 1) * ROWS_PER_SESSION\n",
        "        end_row = min(session_num * ROWS_PER_SESSION, total_rows)\n",
        "\n",
        "        # If we have a last_row in this batch, resume from there\n",
        "        if self.state['last_row'] >= start_row:\n",
        "            actual_start = self.state['last_row'] + 1\n",
        "        else:\n",
        "            actual_start = start_row\n",
        "\n",
        "        return {\n",
        "            'session': session_num,\n",
        "            'batch_start': start_row,\n",
        "            'batch_end': end_row,\n",
        "            'actual_start': actual_start,\n",
        "            'batch_size': end_row - start_row\n",
        "        }\n",
        "\n",
        "    def update_row(self, row_num):\n",
        "        self.state['last_row'] = row_num\n",
        "        self.save()\n",
        "\n",
        "    def mark_batch_complete(self, total_input_tokens, total_output_tokens, cost):\n",
        "        self.state['sessions_completed'] += 1\n",
        "        self.state['session_num'] += 1\n",
        "        self.state['total_tokens_used'] += total_input_tokens + total_output_tokens\n",
        "        self.state['total_cost'] += cost\n",
        "        self.state['batches_completed'].append({\n",
        "            'session': self.state['sessions_completed'],\n",
        "            'rows': f\"{self.state['last_row']-ROWS_PER_SESSION+1}-{self.state['last_row']}\",\n",
        "            'tokens': total_input_tokens + total_output_tokens,\n",
        "            'cost': cost\n",
        "        })\n",
        "        self.save()\n",
        "\n",
        "# === UTILITIES ===\n",
        "\n",
        "def strip_markdown_json(text):\n",
        "    \"\"\"Remove markdown and extract JSON.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if text.startswith('```'):\n",
        "        lines = text.split('\\n')\n",
        "        if lines[0].strip().startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        text = '\\n'.join(lines).strip()\n",
        "\n",
        "    if not text.startswith('{'):\n",
        "        start = text.find('{')\n",
        "        if start == -1:\n",
        "            return None\n",
        "        text = text[start:]\n",
        "\n",
        "    brace_count = 0\n",
        "    in_string = False\n",
        "    escape_next = False\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if escape_next:\n",
        "            escape_next = False\n",
        "            continue\n",
        "        if char == '\\\\':\n",
        "            escape_next = True\n",
        "            continue\n",
        "        if char == '\"':\n",
        "            in_string = not in_string\n",
        "            continue\n",
        "        if not in_string:\n",
        "            if char == '{':\n",
        "                brace_count += 1\n",
        "            elif char == '}':\n",
        "                brace_count -= 1\n",
        "                if brace_count == 0:\n",
        "                    return text[:i+1]\n",
        "\n",
        "    return None\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate required fields.\"\"\"\n",
        "    required = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing: {', '.join(missing[:3])}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON to Excel format (29 columns).\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1_price_HUMAN (4 columns)\n",
        "    price = json_obj.get('1_price_HUMAN', {})\n",
        "    if isinstance(price, dict):\n",
        "        flat['1_price_status'] = price.get('status')\n",
        "        flat['1_price_amount'] = price.get('amount')\n",
        "        flat['1_price_currency'] = price.get('currency')\n",
        "        flat['1_price_description'] = price.get('description')\n",
        "\n",
        "    # 2_size_HUMAN (2 columns)\n",
        "    size = json_obj.get('2_size_HUMAN', {})\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "\n",
        "    # 3_colour_HUMAN (2 columns)\n",
        "    color = json_obj.get('3_colour_HUMAN', {})\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes', [])\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "\n",
        "    # 4_location_HUMAN (2 columns)\n",
        "    location = json_obj.get('4_location_HUMAN', {})\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes', [])\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "\n",
        "    # NEW: Location enhancements (2 columns)\n",
        "    flat['4_location_names_original_spellings'] = json_obj.get('4_location_names_original_spellings')\n",
        "    flat['4_location_context'] = json_obj.get('4_location_context')\n",
        "\n",
        "    # 5_function_HUMAN (2 columns)\n",
        "    function = json_obj.get('5_function_HUMAN', {})\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes', [])\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "\n",
        "    # 6_origin_of_bead (1 column)\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN (2 columns)\n",
        "    shape = json_obj.get('7_shape_HUMAN', {})\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes', [])\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "\n",
        "    # 8_type_bead_HUMAN (2 columns)\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN', {})\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes', [])\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "\n",
        "    # 9_local_name_HUMAN (2 columns)\n",
        "    local = json_obj.get('9_local_name_HUMAN', {})\n",
        "    if isinstance(local, dict):\n",
        "        flat['9_local_name_exists'] = local.get('exists')\n",
        "        names = local.get('names', [])\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "\n",
        "    # 10_relationship_ (2 columns)\n",
        "    rel = json_obj.get('10_relationship_', {})\n",
        "    if isinstance(rel, dict):\n",
        "        codes = rel.get('codes', [])\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = rel.get('description')\n",
        "\n",
        "    # 11_units_of_measure (2 columns)\n",
        "    units = json_obj.get('11_units_of_measure', {})\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "\n",
        "    # 12_bead_ethnic_ (1 column)\n",
        "    ethnics = json_obj.get('12_bead_ethnic_', [])\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange (2 columns)\n",
        "    nature = json_obj.get('13_nature_of_exchange', {})\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "\n",
        "    # Notes (1 column)\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2500,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost.\"\"\"\n",
        "    return (input_tokens / 1_000_000) * 0.80 + (output_tokens / 1_000_000) * 0.24\n",
        "\n",
        "def log_event(message, level=\"INFO\"):\n",
        "    \"\"\"Log event.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    msg = f\"[{timestamp}] [{level}] {message}\"\n",
        "    print(msg)\n",
        "    sys.stdout.flush()\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(msg + \"\\n\")\n",
        "\n",
        "# === MAIN ===\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEAD TRADE CODING - 27,060 ROWS (PRODUCTION)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"✓ All 13 codebook fields\")\n",
        "    print(\"✓ Enhanced 4a_exchange rules (73% agreement)\")\n",
        "    print(\"✓ Two-layer location extraction\")\n",
        "    print(\"✓ 29 output columns (analysis-ready)\")\n",
        "    print(\"✓ Auto-save every 100 rows\")\n",
        "    print(\"✓ Batch processing (2,000 rows/session)\")\n",
        "    print(\"✓ Fresh start from Row 0\\n\")\n",
        "\n",
        "    if not ANTHROPIC_API_KEY:\n",
        "        print(\"✗ ERROR: ANTHROPIC_API_KEY not set\")\n",
        "        return\n",
        "\n",
        "    # Find file\n",
        "    input_path = None\n",
        "    for loc in [os.path.join(os.getcwd(), INPUT_FILE),\n",
        "                os.path.join('/content', INPUT_FILE),\n",
        "                INPUT_FILE]:\n",
        "        if os.path.exists(loc):\n",
        "            input_path = loc\n",
        "            break\n",
        "\n",
        "    if not input_path:\n",
        "        print(f\"✗ ERROR: Could not find {INPUT_FILE}\")\n",
        "        return\n",
        "\n",
        "    session = SessionState()\n",
        "    batch_info = session.get_batch_info()\n",
        "    total_batches = (27060 + ROWS_PER_SESSION - 1) // ROWS_PER_SESSION\n",
        "\n",
        "    print(f\"Session: {batch_info['session']}/{total_batches}\")\n",
        "    print(f\"Batch: Rows {batch_info['batch_start']}-{batch_info['batch_end']-1}\")\n",
        "    print(f\"Resume from: Row {batch_info['actual_start']}\\n\")\n",
        "\n",
        "    log_event(f\"Session {batch_info['session']}/{total_batches} started - row {batch_info['actual_start']}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(input_path)\n",
        "        print(f\"✓ Loaded {len(df)} rows\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ ERROR: {e}\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"✗ ERROR: Column '{TEXT_COLUMN}' not found\")\n",
        "        return\n",
        "\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "    # Initialize output dataframe with original data\n",
        "    output_df = df.copy()\n",
        "\n",
        "    # Add coding columns\n",
        "    coding_cols = [\n",
        "        '1_price_status', '1_price_amount', '1_price_currency', '1_price_description',\n",
        "        '2_size_code', '2_size_description',\n",
        "        '3_colour_codes', '3_colour_description',\n",
        "        '4_location_codes', '4_location_names',\n",
        "        '4_location_names_original_spellings', '4_location_context',\n",
        "        '5_function_codes', '5_function_description',\n",
        "        '6_origin_of_bead',\n",
        "        '7_shape_codes', '7_shape_description',\n",
        "        '8_type_codes', '8_type_description',\n",
        "        '9_local_name_exists', '9_local_name_names',\n",
        "        '10_relationship_codes', '10_relationship_description',\n",
        "        '11_units_type', '11_units_description',\n",
        "        '12_bead_ethnic_',\n",
        "        '13_nature_code', '13_nature_description',\n",
        "        'notes'\n",
        "    ]\n",
        "    for col in coding_cols:\n",
        "        if col not in output_df.columns:\n",
        "            output_df[col] = None\n",
        "\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    session_start_time = time.time()\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    print(\"PROCESSING:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for row_idx in range(batch_info['actual_start'], batch_info['batch_end']):\n",
        "        elapsed_minutes = (time.time() - session_start_time) / 60\n",
        "        if elapsed_minutes > MAX_SESSION_MINUTES:\n",
        "            print(f\"\\n⏰ TIME LIMIT ({elapsed_minutes:.0f}m)\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            row = df.iloc[row_idx]\n",
        "            entry_text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            if pd.isna(entry_text) or not str(entry_text).strip():\n",
        "                skipped_count += 1\n",
        "                session.update_row(row_idx)\n",
        "                continue\n",
        "\n",
        "            entry_text = str(entry_text).strip()\n",
        "\n",
        "            response = call_claude(client, entry_text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            cleaned = strip_markdown_json(response_text)\n",
        "\n",
        "            if not cleaned:\n",
        "                error_count += 1\n",
        "                session.update_row(row_idx)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(cleaned)\n",
        "                valid, error = validate_json_response(parsed)\n",
        "                if valid:\n",
        "                    flat = flatten_for_excel(parsed)\n",
        "                    # Write to output dataframe\n",
        "                    for key, value in flat.items():\n",
        "                        output_df.at[row_idx, key] = value\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    error_count += 1\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                error_count += 1\n",
        "\n",
        "            session.update_row(row_idx)\n",
        "\n",
        "            # AUTO-SAVE every 100 rows\n",
        "            rows_in_batch = row_idx - batch_info['actual_start'] + 1\n",
        "            if rows_in_batch % AUTOSAVE_EVERY == 0:\n",
        "                elapsed = (time.time() - session_start_time) / 60\n",
        "                cost_so_far = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                avg_tokens = (total_input_tokens + total_output_tokens) / rows_in_batch if rows_in_batch > 0 else 0\n",
        "\n",
        "                # Auto-save to Excel\n",
        "                autosave_file = os.path.join(OUTPUT_BASE, f\"session_{batch_info['session']:02d}_autosave_row_{row_idx}.xlsx\")\n",
        "                output_df.iloc[:row_idx+1].to_excel(autosave_file, index=False)\n",
        "\n",
        "                print(f\"Checkpoint (Row {row_idx}): Success={success_count}, Errors={error_count}, \"\n",
        "                      f\"Tokens/row={avg_tokens:.0f}, Cost=${cost_so_far:.2f}, Time={elapsed:.0f}m | \"\n",
        "                      f\"AUTOSAVED to {os.path.basename(autosave_file)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "            session.update_row(row_idx)\n",
        "            log_event(f\"Row {row_idx}: Exception - {str(e)[:50]}\", \"ERROR\")\n",
        "\n",
        "    # === FINAL SAVE ===\n",
        "\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SESSION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Save final batch file\n",
        "    batch_num = batch_info['session']\n",
        "    output_file = os.path.join(\n",
        "        OUTPUT_BASE,\n",
        "        f\"batch_{batch_num:02d}_rows_{batch_info['actual_start']}-{session.state['last_row']}.xlsx\"\n",
        "    )\n",
        "    output_df.iloc[:session.state['last_row']+1].to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"\\nBatch {batch_num}/{total_batches}:\")\n",
        "    print(f\"  Rows: {batch_info['actual_start']}-{session.state['last_row']}\")\n",
        "    print(f\"  Success: {success_count}\")\n",
        "    print(f\"  Errors: {error_count}\")\n",
        "    print(f\"  Skipped: {skipped_count}\")\n",
        "    print(f\"  Avg tokens/row: {(total_input_tokens+total_output_tokens)/(success_count+error_count if success_count+error_count > 0 else 1):.0f}\")\n",
        "    print(f\"  Session cost: ${final_cost:.2f}\")\n",
        "    print(f\"  Total cost: ${session.state['total_cost'] + final_cost:.2f}\")\n",
        "    print(f\"  Time: {(time.time() - session_start_time)/60:.1f}m\")\n",
        "    print(f\"  Output: {os.path.basename(output_file)}\")\n",
        "    print(f\"  Output columns: {len(output_df.columns)}\")\n",
        "\n",
        "    session.mark_batch_complete(total_input_tokens, total_output_tokens, final_cost)\n",
        "\n",
        "    total_rows = 27060\n",
        "    rows_completed = session.state['last_row'] + 1\n",
        "\n",
        "    print(f\"\\nProgress:\")\n",
        "    print(f\"  Rows: {rows_completed}/{total_rows} ({(rows_completed/total_rows)*100:.1f}%)\")\n",
        "    print(f\"  Sessions: {session.state['sessions_completed']}/{total_batches}\")\n",
        "    print(f\"  Total cost: ${session.state['total_cost']:.2f}\")\n",
        "\n",
        "    if session.state['sessions_completed'] < total_batches:\n",
        "        print(f\"\\n⭐ NEXT SESSION: Resume from row {session.state['last_row'] + 1}\")\n",
        "        print(f\"   Run this script again in a new cell\")\n",
        "    else:\n",
        "        print(f\"\\n✅ ALL COMPLETE!\")\n",
        "        print(f\"  Total rows coded: {rows_completed}\")\n",
        "        print(f\"  Total API cost: ${session.state['total_cost']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rj10IFcqLAH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}