{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14Okg5C9TdBj8d_ZlFR3X_fWqG_QScN2J",
      "authorship_tag": "ABX9TyPixXyGez5rK+4n5cZSw/+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurencoetzee001/Beads_Co-detect/blob/main/prompt_optimisation_location_exchange_revisions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejr8gRYvWQsr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding System - 27,000 Rows Google Colab PRODUCTION\n",
        "==============================================================\n",
        "\n",
        "COMPLETE IMPLEMENTATION WITH ALL REVISIONS:\n",
        "✓ 13-field structure (codes + descriptions for research value)\n",
        "✓ JSON dual-format output (maintained from your working version)\n",
        "✓ Enhanced 4a_exchange rules (73% Human-AI agreement)\n",
        "  - Expert-validated edge cases (23:1 over-identification fix)\n",
        "  - Checkmarks/visual anchors for LLM parsing\n",
        "  - Conservative \"when in doubt, code NO\"\n",
        "✓ Two-layer location extraction (NEW)\n",
        "  - 8_location_names_original_spellings (preserve exact historical spellings)\n",
        "  - 8_location_context (geographic + strategic + political context)\n",
        "✓ Data quality pre-checks (filter corrupted OCR, short entries)\n",
        "✓ 60% token reduction in prompting (~800 tokens/row)\n",
        "✓ 2,000-row batches with 50-row checkpoints\n",
        "✓ Robust Google Drive persistence\n",
        "\n",
        "27,000 rows across ~14 sessions at 800K TPM\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "print(\"Checking dependencies...\")\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === GOOGLE DRIVE SETUP ===\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    GDRIVE_BASE = '/content/drive/MyDrive/bead_coding_27k_final'\n",
        "    os.makedirs(GDRIVE_BASE, exist_ok=True)\n",
        "    print(\"✓ Google Drive mounted\")\n",
        "except:\n",
        "    GDRIVE_BASE = './bead_coding_27k_final'\n",
        "    os.makedirs(GDRIVE_BASE, exist_ok=True)\n",
        "    print(\"⚠ Running locally (not in Colab)\")\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"All_entries_beads_cleaned.xlsx\"\n",
        "OUTPUT_BASE = GDRIVE_BASE\n",
        "LOG_FILE = os.path.join(OUTPUT_BASE, \"processing.log\")\n",
        "STATE_FILE = os.path.join(OUTPUT_BASE, \"session_state.json\")\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "MAX_SESSION_MINUTES = 690  # 11.5 hours buffer\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "ROWS_PER_BATCH = 2000\n",
        "SAVE_EVERY = 50\n",
        "\n",
        "# === OPTIMIZED SYSTEM PROMPT WITH ALL REVISIONS ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "Apply expert-validated codebook v4.0 with enhanced location extraction.\n",
        "\n",
        "TARGET: 73% human-AI agreement rate through conservative interpretation.\n",
        "\n",
        "RESPONSE: Return ONLY JSON. NO markdown, NO ```json``` tags, NO extra text.\n",
        "\n",
        "=== DATA QUALITY PRE-CHECK ===\n",
        "Assess text quality FIRST:\n",
        "- <50 readable characters → {\"quality_issue\": \"too_short\"}\n",
        "- Mostly symbols/OCR errors → {\"quality_issue\": \"corrupted_ocr\"}\n",
        "- No bead content → {\"quality_issue\": \"no_bead_content\"}\n",
        "- Otherwise → proceed with full 15-field coding\n",
        "\n",
        "=== CRITICAL: 4A_EXCHANGE DECISION RULES ===\n",
        "\n",
        "✓ CODE \"xo\" (EXCHANGE OCCURRED) ONLY IF ALL PRESENT:\n",
        "  ✓ Transaction verb: traded, sold, bought, exchanged, gave, received\n",
        "  ✓ Past tense: action already happened\n",
        "  ✓ Clear parties: who gave/received\n",
        "  ✓ Specific items: what was exchanged\n",
        "\n",
        "✗ CODE \"no\" (NO EXCHANGE) FOR:\n",
        "  ✗ Value descriptions: \"beads are worth...\", \"beads cost...\"\n",
        "  ✗ Manufacturing: \"making\", \"creating\", \"fashioned\"\n",
        "  ✗ Hypothetical/future: \"would trade\", \"could buy\"\n",
        "  ✗ General statements: \"natives sell ivory for beads\"\n",
        "  ✗ Observational: \"showed beads\", \"wore beads\", \"displayed\"\n",
        "  ✗ Equipment/decoration: saddlery, clothing, adornment\n",
        "\n",
        "EXPERT-VALIDATED EDGE CASES:\n",
        "  → Gift-giving (\"offered presents\") = xo (valid transaction)\n",
        "  → Intangible (\"bought secret for beads\") = xo (still transaction)\n",
        "  → Historical generalizations = no (not specific event)\n",
        "  → Observational contexts = no (not exchange)\n",
        "  → Equipment beads = no (decorative, not traded)\n",
        "\n",
        "=== TWO-LAYER LOCATION EXTRACTION ===\n",
        "\n",
        "LAYER 1: 8_LOCATION_NAMES_ORIGINAL_SPELLINGS\n",
        "- Comma-separated list of EXACT place names as written in text\n",
        "- Preserve diacritics/accents: Bontúku, Dahomey, Whydah (NOT normalized)\n",
        "- Include parenthetical geographic markers if original text has them\n",
        "- Order by first mention\n",
        "- Research value: Historical placename variants, etymology\n",
        "\n",
        "LAYER 2: 8_LOCATION_CONTEXT\n",
        "- For each location, capture geographic + strategic + political context\n",
        "- Format: Location | Geographic Context | Strategic Significance | Other Details\n",
        "- Examples:\n",
        "  • \"Bontúku | near Kumasi | distribution center for Akan trade | Ashanti territory\"\n",
        "  • \"Cape Coast | Gold Coast, coastal settlement | European trading fort | Atlantic hub\"\n",
        "- Research value: Trade route mapping, network analysis, merchant preferences\n",
        "\n",
        "=== JSON STRUCTURE (15 fields) ===\n",
        "\n",
        "{\n",
        "  \"quality_check\": \"pass\" or \"quality_issue description\",\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": null or \"number/measurement\",\n",
        "    \"currency\": null or \"currency/commodity\",\n",
        "    \"description\": \"full price text from source\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": null or 1-6,\n",
        "    \"description\": \"exact size text\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [],\n",
        "    \"description\": \"exact color text\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [],\n",
        "    \"names\": \"actual location names\"\n",
        "  },\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [],\n",
        "    \"description\": \"detailed function text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [],\n",
        "    \"description\": \"exact shape text\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [],\n",
        "    \"description\": \"exact material text\"\n",
        "  },\n",
        "  \"8_location_names_original_spellings\": \"Bontúku, Whydah, etc. (exact spellings)\",\n",
        "  \"8_location_context\": \"Location | Geographic | Strategic | Other details\",\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": null or 1|2,\n",
        "    \"names\": null or [\"array of names\"]\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [],\n",
        "    \"description\": \"detailed exchange items text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": null or 1-4,\n",
        "    \"description\": \"exact measurement text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": null or 1-6,\n",
        "    \"description\": \"exchange nature text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context\"\n",
        "}\n",
        "\n",
        "=== PROCESSING RULES ===\n",
        "1. Use null for missing/ambiguous data (not \"unknown\")\n",
        "2. For arrays: [] if no data, null if not applicable\n",
        "3. ALWAYS include description fields with verbatim text\n",
        "4. When code=14 (other), description is MANDATORY\n",
        "5. Preserve exact terminology for research value\n",
        "6. Conservative interpretation: when in doubt, code conservative\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze and extract ALL 15 fields (with quality check):\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY the JSON object. No ```json``` tags, no other text.\"\"\"\n",
        "\n",
        "# === SESSION STATE MANAGEMENT ===\n",
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.file_path = STATE_FILE\n",
        "        self.state = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if os.path.exists(self.file_path):\n",
        "            with open(self.file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return {\n",
        "            'session_num': 1,\n",
        "            'last_row': 0,\n",
        "            'sessions_completed': 0,\n",
        "            'total_tokens_used': 0,\n",
        "            'total_cost': 0,\n",
        "            'batches_completed': []\n",
        "        }\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.file_path, 'w') as f:\n",
        "            json.dump(self.state, f, indent=2)\n",
        "\n",
        "    def get_batch_info(self, total_rows=27000):\n",
        "        session_num = self.state['session_num']\n",
        "        start_row = (session_num - 1) * ROWS_PER_BATCH\n",
        "        end_row = min(session_num * ROWS_PER_BATCH, total_rows)\n",
        "\n",
        "        if self.state['last_row'] >= start_row:\n",
        "            actual_start = self.state['last_row'] + 1\n",
        "        else:\n",
        "            actual_start = start_row\n",
        "\n",
        "        return {\n",
        "            'session': session_num,\n",
        "            'batch_start': start_row,\n",
        "            'batch_end': end_row,\n",
        "            'actual_start': actual_start,\n",
        "            'batch_size': end_row - start_row\n",
        "        }\n",
        "\n",
        "    def update_row(self, row_num):\n",
        "        self.state['last_row'] = row_num\n",
        "        self.save()\n",
        "\n",
        "    def mark_batch_complete(self, total_input_tokens, total_output_tokens, cost):\n",
        "        self.state['sessions_completed'] += 1\n",
        "        self.state['session_num'] += 1\n",
        "        self.state['total_tokens_used'] += total_input_tokens + total_output_tokens\n",
        "        self.state['total_cost'] += cost\n",
        "        self.state['batches_completed'].append({\n",
        "            'session': self.state['sessions_completed'],\n",
        "            'rows': f\"{self.state['last_row']-ROWS_PER_BATCH+1}-{self.state['last_row']}\",\n",
        "            'tokens': total_input_tokens + total_output_tokens,\n",
        "            'cost': cost\n",
        "        })\n",
        "        self.save()\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def strip_markdown_json(text):\n",
        "    \"\"\"Remove markdown and extract JSON object.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    if text.startswith('```'):\n",
        "        lines = text.split('\\n')\n",
        "        if lines[0].strip().startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        text = '\\n'.join(lines).strip()\n",
        "\n",
        "    if not text.startswith('{'):\n",
        "        start = text.find('{')\n",
        "        if start == -1:\n",
        "            return text\n",
        "        text = text[start:]\n",
        "\n",
        "    brace_count = 0\n",
        "    in_string = False\n",
        "    escape_next = False\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if escape_next:\n",
        "            escape_next = False\n",
        "            continue\n",
        "        if char == '\\\\':\n",
        "            escape_next = True\n",
        "            continue\n",
        "        if char == '\"':\n",
        "            in_string = not in_string\n",
        "            continue\n",
        "        if not in_string:\n",
        "            if char == '{':\n",
        "                brace_count += 1\n",
        "            elif char == '}':\n",
        "                brace_count -= 1\n",
        "                if brace_count == 0:\n",
        "                    return text[:i+1]\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate required fields exist.\"\"\"\n",
        "    required = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing: {', '.join(missing[:3])}\"\n",
        "\n",
        "    # Check new location fields\n",
        "    if '8_location_names_original_spellings' not in json_obj or '8_location_context' not in json_obj:\n",
        "        return False, \"Missing location enhancement fields\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON to Excel format (29 columns with locations).\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # Quality check\n",
        "    flat['quality_check'] = json_obj.get('quality_check', 'pass')\n",
        "\n",
        "    # 1_price_HUMAN (4 columns)\n",
        "    price = json_obj.get('1_price_HUMAN', {})\n",
        "    if isinstance(price, dict):\n",
        "        flat['1_price_status'] = price.get('status')\n",
        "        flat['1_price_amount'] = price.get('amount')\n",
        "        flat['1_price_currency'] = price.get('currency')\n",
        "        flat['1_price_description'] = price.get('description')\n",
        "\n",
        "    # 2_size_HUMAN (2 columns)\n",
        "    size = json_obj.get('2_size_HUMAN', {})\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "\n",
        "    # 3_colour_HUMAN (2 columns)\n",
        "    color = json_obj.get('3_colour_HUMAN', {})\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes', [])\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "\n",
        "    # 4_location_HUMAN (2 columns)\n",
        "    location = json_obj.get('4_location_HUMAN', {})\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes', [])\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "\n",
        "    # 5_function_HUMAN (2 columns)\n",
        "    function = json_obj.get('5_function_HUMAN', {})\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes', [])\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "\n",
        "    # 6_origin_of_bead (1 column)\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN (2 columns)\n",
        "    shape = json_obj.get('7_shape_HUMAN', {})\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes', [])\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "\n",
        "    # 8_type_bead_HUMAN (2 columns)\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN', {})\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes', [])\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "\n",
        "    # NEW: Two-layer location extraction (2 columns)\n",
        "    flat['8_location_names_original_spellings'] = json_obj.get('8_location_names_original_spellings')\n",
        "    flat['8_location_context'] = json_obj.get('8_location_context')\n",
        "\n",
        "    # 9_local_name_HUMAN (2 columns)\n",
        "    local = json_obj.get('9_local_name_HUMAN', {})\n",
        "    if isinstance(local, dict):\n",
        "        flat['9_local_name_exists'] = local.get('exists')\n",
        "        names = local.get('names', [])\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "\n",
        "    # 10_relationship_ (2 columns)\n",
        "    rel = json_obj.get('10_relationship_', {})\n",
        "    if isinstance(rel, dict):\n",
        "        codes = rel.get('codes', [])\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = rel.get('description')\n",
        "\n",
        "    # 11_units_of_measure (2 columns)\n",
        "    units = json_obj.get('11_units_of_measure', {})\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "\n",
        "    # 12_bead_ethnic_ (1 column)\n",
        "    ethnics = json_obj.get('12_bead_ethnic_', [])\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange (2 columns)\n",
        "    nature = json_obj.get('13_nature_of_exchange', {})\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "\n",
        "    # Notes (1 column)\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with expert-validated codebook.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2500,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku.\"\"\"\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def log_event(message, level=\"INFO\"):\n",
        "    \"\"\"Log to file and print.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    log_message = f\"[{timestamp}] [{level}] {message}\"\n",
        "    print(log_message)\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(log_message + \"\\n\")\n",
        "\n",
        "# === MAIN PROCESSING ===\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEAD TRADE CODING - 27,000 ROWS PRODUCTION (WITH ALL REVISIONS)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"✓ 13-field structure + 2-layer location extraction\")\n",
        "    print(\"✓ Enhanced 4a_exchange rules (73% Human-AI agreement)\")\n",
        "    print(\"✓ Data quality pre-checks\")\n",
        "    print(\"✓ 60% token optimization\")\n",
        "    print(\"✓ 2,000-row batches | 50-row checkpoints\")\n",
        "\n",
        "    if not ANTHROPIC_API_KEY:\n",
        "        print(\"ERROR: ANTHROPIC_API_KEY environment variable not set\")\n",
        "        return\n",
        "\n",
        "    session = SessionState()\n",
        "    batch_info = session.get_batch_info()\n",
        "    total_batches = (27000 + ROWS_PER_BATCH - 1) // ROWS_PER_BATCH\n",
        "\n",
        "    print(f\"\\nSession Information:\")\n",
        "    print(f\"  Current Session: {batch_info['session']}/{total_batches}\")\n",
        "    print(f\"  Batch Range: Rows {batch_info['batch_start']}-{batch_info['batch_end']-1}\")\n",
        "    print(f\"  Starting From: Row {batch_info['actual_start']}\")\n",
        "    print(f\"  Checkpoint Frequency: Every {SAVE_EVERY} rows\")\n",
        "\n",
        "    log_event(f\"Session {batch_info['session']}/{total_batches} started - resuming from row {batch_info['actual_start']}\")\n",
        "\n",
        "    print(f\"\\nLoading data file...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"✓ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "        log_event(f\"Data loaded: {len(df)} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load file: {e}\")\n",
        "        log_event(f\"ERROR: Failed to load file: {e}\", \"ERROR\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"ERROR: Column '{TEXT_COLUMN}' not found\")\n",
        "        return\n",
        "\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    session_start_time = time.time()\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    skipped_count = 0\n",
        "    quality_issues = 0\n",
        "    checkpoint_count = 0\n",
        "\n",
        "    print(f\"\\nStarting processing...\")\n",
        "    print(f\"Model: {MODEL_NAME}, Temperature: 0\")\n",
        "    print(f\"Codebook: Expert-validated v4.0 with location enhancements\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for row_idx in range(batch_info['actual_start'], batch_info['batch_end']):\n",
        "        elapsed_minutes = (time.time() - session_start_time) / 60\n",
        "        if elapsed_minutes > MAX_SESSION_MINUTES:\n",
        "            print(f\"\\n⏰ TIME LIMIT APPROACHING ({elapsed_minutes:.0f}m used)\")\n",
        "            log_event(f\"Session ended at time limit: {elapsed_minutes:.0f}m\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            row = df.iloc[row_idx]\n",
        "            entry_text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            if pd.isna(entry_text) or not str(entry_text).strip():\n",
        "                responses.append(None)\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            entry_text = str(entry_text).strip()\n",
        "\n",
        "            # Call Claude with enhanced codebook\n",
        "            response = call_claude(client, entry_text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            # Strip markdown and parse\n",
        "            cleaned = strip_markdown_json(response_text)\n",
        "\n",
        "            if not cleaned:\n",
        "                responses.append({\"error\": \"Empty response\"})\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(cleaned)\n",
        "\n",
        "                # Check for quality issues\n",
        "                if parsed.get('quality_check') != 'pass':\n",
        "                    quality_issues += 1\n",
        "                    responses.append({\"quality_issue\": parsed.get('quality_check')})\n",
        "                    continue\n",
        "\n",
        "                # Validate\n",
        "                valid, error = validate_json_response(parsed)\n",
        "                if valid:\n",
        "                    flat = flatten_for_excel(parsed)\n",
        "                    responses.append(flat)\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    error_count += 1\n",
        "                    log_event(f\"Row {row_idx}: Validation failed - {error}\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON error\"})\n",
        "                error_count += 1\n",
        "                log_event(f\"Row {row_idx}: JSON parse error\")\n",
        "\n",
        "            # Update state and checkpoint every 50 rows\n",
        "            session.update_row(row_idx)\n",
        "\n",
        "            rows_in_batch = row_idx - batch_info['actual_start'] + 1\n",
        "            if rows_in_batch % SAVE_EVERY == 0:\n",
        "                checkpoint_count += 1\n",
        "                elapsed = (time.time() - session_start_time) / 60\n",
        "                cost_so_far = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                avg_tokens = (total_input_tokens + total_output_tokens) / rows_in_batch if rows_in_batch > 0 else 0\n",
        "                print(f\"Checkpoint {checkpoint_count} (Row {row_idx}): Success={success_count}, \"\n",
        "                      f\"Quality_issues={quality_issues}, Tokens/row={avg_tokens:.0f}, \"\n",
        "                      f\"Cost=${cost_so_far:.2f}, Time={elapsed:.0f}m\")\n",
        "\n",
        "        except Exception as e:\n",
        "            responses.append({\"error\": str(e)[:100]})\n",
        "            error_count += 1\n",
        "            session.update_row(row_idx)\n",
        "            log_event(f\"Row {row_idx}: Exception - {str(e)[:100]}\", \"ERROR\")\n",
        "\n",
        "    # === SAVE RESULTS ===\n",
        "\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SESSION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create output dataframe\n",
        "    batch_df = df.iloc[batch_info['actual_start']:batch_info['actual_start']+len(responses)].copy()\n",
        "\n",
        "    # Add coding columns\n",
        "    for i, resp in enumerate(responses):\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp and \"quality_issue\" not in resp:\n",
        "            for key, value in resp.items():\n",
        "                if key not in batch_df.columns:\n",
        "                    batch_df[key] = None\n",
        "                batch_df.at[i, key] = value\n",
        "\n",
        "    # Save batch output\n",
        "    batch_num = batch_info['session']\n",
        "    output_file = os.path.join(\n",
        "        OUTPUT_BASE,\n",
        "        f\"batch_{batch_num:02d}_rows_{batch_info['actual_start']}-{session.state['last_row']}.xlsx\"\n",
        "    )\n",
        "    batch_df.to_excel(output_file, index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nBatch {batch_num}/{total_batches} Summary:\")\n",
        "    print(f\"  Rows processed: {batch_info['actual_start']}-{session.state['last_row']}\")\n",
        "    print(f\"  Successfully coded: {success_count}\")\n",
        "    print(f\"  Quality issues: {quality_issues}\")\n",
        "    print(f\"  Errors: {error_count}\")\n",
        "    print(f\"  Skipped (empty): {skipped_count}\")\n",
        "    print(f\"  Checkpoints saved: {checkpoint_count}\")\n",
        "    print(f\"  Input tokens: {total_input_tokens:,}\")\n",
        "    print(f\"  Output tokens: {total_output_tokens:,}\")\n",
        "    print(f\"  Avg tokens/row: {(total_input_tokens+total_output_tokens)/(success_count if success_count > 0 else 1):.0f}\")\n",
        "    print(f\"  Session cost: ${final_cost:.2f}\")\n",
        "    print(f\"  Total cost so far: ${session.state['total_cost'] + final_cost:.2f}\")\n",
        "    print(f\"  Processing time: {(time.time() - session_start_time)/60:.1f}m\")\n",
        "    print(f\"\\n  Output file: {os.path.basename(output_file)}\")\n",
        "    print(f\"  Output columns: {len(batch_df.columns)} (original: {len(df.columns)}, new: {len(batch_df.columns) - len(df.columns)})\")\n",
        "\n",
        "    session.mark_batch_complete(total_input_tokens, total_output_tokens, final_cost)\n",
        "\n",
        "    total_rows = 27000\n",
        "    rows_completed = session.state['last_row'] + 1\n",
        "\n",
        "    print(f\"\\nOverall Progress:\")\n",
        "    print(f\"  Rows completed: {rows_completed}/{total_rows} ({(rows_completed/total_rows)*100:.1f}%)\")\n",
        "    print(f\"  Sessions completed: {session.state['sessions_completed']}/{total_batches}\")\n",
        "    print(f\"  Total cost: ${session.state['total_cost']:.2f}\")\n",
        "\n",
        "    if session.state['sessions_completed'] < total_batches:\n",
        "        next_start = session.state['last_row'] + 1\n",
        "        print(f\"\\n⭐️ NEXT SESSION:\")\n",
        "        print(f\"  Will resume from row {next_start}\")\n",
        "    else:\n",
        "        print(f\"\\n✅ ALL BATCHES COMPLETE!\")\n",
        "        print(f\"  Total rows coded: {rows_completed}\")\n",
        "        print(f\"  Total API cost: ${session.state['total_cost']:.2f}\")\n",
        "        print(f\"  Output: 31 columns (original + codes + descriptions + enhanced locations)\")\n",
        "        print(f\"  Data quality: Expert-validated with historical location preservation\")\n",
        "\n",
        "    log_event(f\"Batch {batch_num} complete: {success_count} success, {error_count} errors, \"\n",
        "              f\"{quality_issues} quality issues, {checkpoint_count} checkpoints, cost: ${final_cost:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}