{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZDrO875UwAZy1Pj5eDrXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurencoetzee001/Beads_Co-detect/blob/main/Oct_Stage_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Co-DETECT Bead Exchange Analysis - Google Colab Implementation\n",
        "# Expert-Validated Codebook v4.0 - August 2025\n",
        "# ENHANCED VERSION with Robust Restart Capabilities\n",
        "# Using Claude 3.5 Haiku for Fast & Cost-Efficient Processing\n",
        "# Run this setup cell first to initialize the Co-DETECT system\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import anthropic\n",
        "\n",
        "# Mount Google Drive (run this once)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =============================================================================\n",
        "# CO-DETECT CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration\n",
        "WORKING_DIR = '/content/drive/MyDrive/CoDetectBeadAnalysis'\n",
        "BACKUP_DIR = '/content/drive/MyDrive/colab_backups/codetect'\n",
        "DATASET_FILE = 'Munashe_Cleaned.xlsx'\n",
        "BACKUP_FILENAME = 'Oct_Stage_1_progress'\n",
        "LINES_PER_BACKUP = 50\n",
        "\n",
        "# Co-DETECT Parameters\n",
        "SAMPLE_SIZE = 1500\n",
        "EDGE_CASE_THRESHOLD = 0.7\n",
        "MAX_RETRIES = 3\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# Cost estimation (Claude 3.5 Haiku pricing)\n",
        "COST_PER_1K_TOKENS_INPUT = 0.001  # $1 per million tokens\n",
        "COST_PER_1K_TOKENS_OUTPUT = 0.005  # $5 per million tokens\n",
        "ESTIMATED_TOKENS_PER_TEXT = 800\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(WORKING_DIR, exist_ok=True)\n",
        "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "os.chdir(WORKING_DIR)\n",
        "\n",
        "# Global counters and tracking (ENHANCED for restart)\n",
        "if 'progress_counter' not in globals():\n",
        "    progress_counter = 0\n",
        "if 'codetect_results' not in globals():\n",
        "    codetect_results = []\n",
        "if 'processed_indices' not in globals():\n",
        "    processed_indices = set()  # Track which row indices have been processed\n",
        "if 'session_start_time' not in globals():\n",
        "    session_start_time = None\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT-VALIDATED CODEBOOK v4.0\n",
        "# =============================================================================\n",
        "\n",
        "EXPERT_CODEBOOK = \"\"\"\n",
        "# Bead Exchange Historical Data Analysis Codebook v4.0 (Expert-Validated + Historian-Designed - August 2025)\n",
        "\n",
        "## CRITICAL INSTRUCTIONS FOR AI/API PROCESSING\n",
        "\n",
        "**PRIORITY**: This codebook combines the complete historian-designed structure with expert-validated edge case resolutions from Co-DETECT analysis. Follow ALL decision rules strictly. When in doubt, be CONSERVATIVE and require explicit evidence.\n",
        "\n",
        "**KEY PRINCIPLE**: Code only what is explicitly stated in the text. Do NOT infer, assume, or interpret beyond what is directly written.\n",
        "\n",
        "## PREPROCESSING DATA QUALITY RULES (Expert-Validated)\n",
        "\n",
        "### Rule P1: Text Length and Quality Filtering\n",
        "- **Entries with <50 readable characters** → `read_entry = \"NaN\"` (corrupted/fragment)\n",
        "- **Entries with >80% non-alphabetic characters** → `read_entry = \"NaN\"` (corrupted OCR)\n",
        "\n",
        "### Rule P2: Content Relevance Filtering\n",
        "- **\"Context page\" entries without bead-related terms** → `read_entry = 0` (not about beads)\n",
        "- **Bead-related terms**: bead, beads, pearl, coral, glass (ornament context), necklace, bracelet, ornament, jewelry, string\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This codebook identifies and classifies historical references to beads and their exchange across cultures and time periods. It provides systematic guidelines for analyzing historical texts to extract information about beads, their characteristics, and their role in economic and cultural exchanges.\n",
        "\n",
        "The unit of observation is a specific exchange rate at a specific location/ethnic group or specific point of time. If multiple exchange rates are stated in the same context (e.g., two exchange rates in two markets), each should be treated as a separate entry.\n",
        "\n",
        "**HISTORIAN EXAMPLE**: \"A slave-boy could be purchased for five fundo, or fifty strings of beads: the same article would now fetch three hundred. A fundo of cheap white porcelain-beads would procure a milk cow; and a goat, or ten hens its equivalent, was to be bought for one khete.\"\n",
        "\n",
        "This passage contains multiple distinct exchange rates that would require separate entries:\n",
        "1. Five fundo (fifty strings of beads) for a slave-boy (historical rate)\n",
        "2. Three hundred strings of beads for a slave-boy (current rate)\n",
        "3. One fundo of cheap white porcelain beads for a milk cow\n",
        "4. One khete of beads for a goat or ten hens\n",
        "\n",
        "## MANDATORY DECISION FLOWCHART (Expert-Enhanced)\n",
        "\n",
        "For EVERY entry, follow this sequence:\n",
        "\n",
        "1. **Data Quality Assessment** (apply preprocessing rules)\n",
        "2. **Read Entry Assessment** (code=\"read_entry\")\n",
        "3. **Exchange Occurrence Check** (code=\"4a_exchange\") - CRITICAL DECISION POINT\n",
        "4. **If exchange=\"no\"**: Set exchange variables to \"NA\" BUT capture contextual details for historical analysis\n",
        "5. **If exchange=\"xo\"**: Code all exchange variables based on explicit evidence\n",
        "6. **Function coding**: Can be done regardless of exchange status\n",
        "7. **Multiple exchange check**: Duplicate row for each distinct exchange rate\n",
        "\n",
        "## Question-by-Question Guidance (Complete Historian Design + Expert Enhancements)\n",
        "\n",
        "### 0. Read Entry (code=\"read_entry\")\n",
        "**Question**: Is this a readable entry about beads?\n",
        "\n",
        "**Values**:\n",
        "- `0`: Entry does not clearly mention beads in meaningful context\n",
        "- `1`: Entry clearly mentions beads with sufficient context for analysis\n",
        "- `2`: Entry mentions beads but has typos (e.g., \"head\" instead of \"bead\")\n",
        "- `NaN`: Corrupted/unreadable text only\n",
        "\n",
        "**ENHANCED RULE**: Code `1` only when beads are central to the passage content. Include ALL types of beads - cultural, trade, equipment, ceremonial.\n",
        "\n",
        "### 1. Function (code=\"1_function\")\n",
        "**Question**: What is the function of the beads in society?\n",
        "\n",
        "#### 1a. Physical Function (code=\"1a_physical_function\")\n",
        "**Question**: What is the physical function?\n",
        "\n",
        "**Values**:\n",
        "- `2`: Aesthetic function (jewelry & fashion, decoration)\n",
        "- `NaN`: Not mentioned/Not applicable\n",
        "\n",
        "**EXPERT-ENHANCED EXAMPLES**:\n",
        "- \"Wearing strings of beads around their loins\" → `2`\n",
        "- \"The women adorned their hair with colorful glass beads\" → `2`\n",
        "- \"The hut was decorated with beaded curtains\" → `2`\n",
        "- **\"Set of double beads on horse equipment\"** → `2` (decorative equipment) - Expert Decision\n",
        "- **\"beads of his rosary\"** → `2` (can have aesthetic component)\n",
        "\n",
        "#### 1b. Trade Function (code=\"1b_trade_function\")\n",
        "**Question**: What is the trade function?\n",
        "\n",
        "**Values**:\n",
        "- `2`: Money (exchange, tax)\n",
        "- `NaN`: Not mentioned/Not applicable\n",
        "\n",
        "**EXAMPLES**:\n",
        "- \"They used beads as currency to purchase supplies\" → `2`\n",
        "- \"Beads were collected as tax by the chieftain\" → `2`\n",
        "- **\"bought the secret for beads\"** → `2` (beads functioning as payment) - Expert Decision\n",
        "\n",
        "#### 1c. Social Function (code=\"1c_social_function\")\n",
        "**Question**: What is the social, religious function of the beads?\n",
        "\n",
        "**Values**:\n",
        "- `3`: Ceremonial (prayer beads) & social function (gifts, status)\n",
        "- `NaN`: Not mentioned/Not applicable\n",
        "\n",
        "**EXPERT-ENHANCED EXAMPLES**:\n",
        "- \"Prayer beads for religious ceremonies\" → `3`\n",
        "- \"Gift of beads to demonstrate status\" → `3`\n",
        "- \"The chief received tribute in the form of rare beads\" → `3`\n",
        "- \"At the wedding ceremony, the bride was given ancestral beads\" → `3`\n",
        "- **\"beads of his rosary\"** → `3` (ceremonial - prayer beads) - Expert Decision\n",
        "- **\"presents of glass beads\"** → `3` (social - gift-giving) - Expert Decision\n",
        "\n",
        "### 2. Nature of Exchange (code=\"2_nature_of_exchange\")\n",
        "**Question**: If beads were exchanged, was the observation consensual or conflictual?\n",
        "\n",
        "**MANDATORY PREREQUISITE**: Only code if 4a_exchange=\"xo\"\n",
        "\n",
        "**Values**:\n",
        "- `1`: Consensual - Willing, mutually agreed exchange where both parties enter freely with roughly equal power and information\n",
        "- `2`: Conflictual - Forced, unequal, or contested exchange with significant power imbalance\n",
        "- `3`: Competitive/haggling (involves negotiation)\n",
        "- `4`: Social (gifts/tributes/presents) - Expert Enhanced\n",
        "- `NaN`: Not mentioned/Not applicable\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"They willingly traded beads for supplies\" → `1`\n",
        "- \"The merchants happily exchanged glass beads for local produce\" → `1`\n",
        "- \"Both parties seemed satisfied with the exchange of beads for cattle\" → `1`\n",
        "- \"Beads were demanded as tribute from the conquered village\" → `2`\n",
        "- \"Under threat of violence, they were forced to accept beads of inferior quality\" → `2`\n",
        "- \"The colonial officials imposed a tax to be paid in blue beads\" → `2`\n",
        "- \"After much bargaining, we agreed on twenty strings of beads for the goat\" → `3`\n",
        "- \"The price began at five strings but after negotiation rose to eight\" → `3`\n",
        "- \"Three days of intense haggling preceded the exchange of beads for ivory\" → `3`\n",
        "- \"A ceremonial gift of beads was presented to the bride's family\" → `4`\n",
        "- \"The visiting dignitary received a necklace of rare beads\" → `4`\n",
        "- \"Annual tribute of decorative beads was given to honor the ancestors\" → `4`\n",
        "\n",
        "**EXPERT ADDITIONS**:\n",
        "- **\"offered presents of glass beads\"** → `4` (social - gift-giving)\n",
        "- **\"bought the secret for beads\"** → `1` (consensual commercial)\n",
        "\n",
        "### 3. Between Groups (code=\"3_between_groups\")\n",
        "**Question**: Who were the people involved in the interaction? Was the interaction observed between different ethnic groups or between local and visiting traveler?\n",
        "\n",
        "**MANDATORY PREREQUISITE**: Only code if 4a_exchange=\"xo\"\n",
        "\n",
        "**Values**:\n",
        "- `1`: Between local and traveler (international travelers)\n",
        "- `2`: Between local and local (inter-ethnic) [add STRING of both groups], i.e., Zulu trading with Xhosa\n",
        "- `3`: Between local and local (intra-ethnic) [add STRING of both groups], i.e., Kikuyu trading with Kikuyu\n",
        "- `4`: Between international travelers only\n",
        "\n",
        "**Decision rules**:\n",
        "- Include names of ethnic groups in addition to the code\n",
        "- Distinguish between inter-ethnic and intra-ethnic exchanges\n",
        "\n",
        "### 4a. Exchange (code=\"4a_exchange\") - CRITICAL DECISION POINT\n",
        "**Question**: Were beads actually exchanged?\n",
        "\n",
        "**Values**:\n",
        "- `no`: No mention (exchange did not actually take place, hearsay)\n",
        "- `xo`: Exchanged\n",
        "\n",
        "**EXPERT-ENHANCED CRITERIA FOR \"xo\":**\n",
        "1. **Explicit transaction verbs**: \"traded\", \"sold\", \"bought\", \"exchanged\", \"gave for\", \"received for\", \"purchased\", \"offered presents\"\n",
        "2. **Clear parties**: Identifiable giver AND receiver\n",
        "3. **Completed action**: Past tense indicating transaction happened\n",
        "4. **Specific items**: What was given AND what was received\n",
        "5. **INTANGIBLE GOODS INCLUDED**: Knowledge, secrets, services count (Expert Decision)\n",
        "\n",
        "**❌ NEVER code \"xo\" for (Expert-Validated):**\n",
        "- **Historical generalizations**: \"when trading first commenced, natives sold ivory for beads\" (Expert Decision)\n",
        "- **Observational descriptions**: \"showed me his beads\", \"displayed the beads\" (Expert Decision)\n",
        "- **Wearing/using descriptions**: \"wore beads\", \"adorned with beads\"\n",
        "- **Price lists without transactions**: \"beads cost £2\"\n",
        "\n",
        "**Decision rules**:\n",
        "- Use `xo` to indicate an actual exchange took place\n",
        "- For every distinct exchange rate at the same market/location, duplicate the row and enter relevant exchange rates separately (Historian Rule)\n",
        "\n",
        "### 4b. Beads Exchanged (code=\"4b_beads_exchanged\")\n",
        "**Question**: Were there specific types of beads used in the exchange? Describe the unit of exchange of beads.\n",
        "\n",
        "**Format**: Provide details including quantity, weight, body measurement, and characteristics of bead used in exchange.\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"A fondo of large white beads with blue eyes\"\n",
        "- \"100 strings of red beads\"\n",
        "- \"Two fathoms of small blue glass beads\"\n",
        "- \"A handful of amber-colored tubular beads\"\n",
        "- \"Five bracelets of multicolored seed beads\"\n",
        "- \"Twenty large coral beads measured from thumb to elbow\"\n",
        "- \"Three strings of small white porcelain beads and one string of large blue glass beads\"\n",
        "- \"One khete of cheap white glass beads\"\n",
        "- \"An arm's length of faceted crystal beads\"\n",
        "- \"A necklace of alternating black and white ceramic beads\"\n",
        "\n",
        "**EXPERT ADDITIONS**:\n",
        "- **\"string of wooden beads\"** (from intangible exchange case)\n",
        "- **\"glass beads\"** (from gift-giving case)\n",
        "\n",
        "**Decision rules**:\n",
        "- If more than one type of bead was exchanged, separate by a comma\n",
        "- Include all relevant characteristics: size, color, shape, type\n",
        "- Include measurement terms (fathom, khete, string, fundo, etc.) when specified\n",
        "- **Expert Addition**: For historical generalizations, still capture bead details even when 4a_exchange=\"no\"\n",
        "\n",
        "### 4c. Exchanged Item (code=\"4c_exchanged_item\")\n",
        "**Question**: What were beads exchanged for?\n",
        "\n",
        "**Format**: Include quantity and item. What was \"bought\" for the amount given in 4b.\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"One cow\"\n",
        "- \"Handful of eggs\"\n",
        "- \"Few francs\"\n",
        "- \"10 goats\"\n",
        "- \"2 lb of flour and 2 eggs\"\n",
        "- \"Dozen bullets\"\n",
        "\n",
        "**EXPERT ADDITIONS (Intangible Goods)**:\n",
        "- **\"the secret for writing amulets\"** (knowledge)\n",
        "- **\"friendship/diplomatic relations\"** (social capital)\n",
        "- **\"ivory\"** (from historical generalizations)\n",
        "\n",
        "**Decision rules**:\n",
        "- If more than one item was exchanged, separate by a comma\n",
        "- Be as specific as possible with quantities and descriptions\n",
        "- **Expert Addition**: Include intangible goods with same detail level as physical goods\n",
        "\n",
        "### 5. Related Items (code=\"5_related_items\")\n",
        "**Question**: List all items that were traded, exchanged or given along as gifts in correlation with beads.\n",
        "\n",
        "#### 5a. Raw Materials (code=\"5a_related_raw_materials\")\n",
        "**Question**: Was the bead connected to raw materials (also as part of jewellery)?\n",
        "\n",
        "**Format**: List all raw material items traded, separated by comma or NaN\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Wire, Iron bars/rods (bronze bars, copper bars, etc.), Precious stones/metals (agates/pearls/corals/amber/red alum/alkali/marble/limestone, etc.), Ebony/ivory, Salt, Rubber/gum, Skins/leather/hides/horns/animal products, Gold/silver/gold dust\n",
        "\n",
        "**EXPERT ADDITION**: **\"copper bracelets\"** (from historical trading patterns case)\n",
        "\n",
        "#### 5b. Jewelry and Fashion (code=\"5b_related_jewerly_fashion\")\n",
        "**Question**: Was the bead connected to jewelry or fashion items?\n",
        "\n",
        "**Format**: List all jewelry and fashion items traded, separated by comma or NaN\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Shells (e.g. cowries, etc.), Ostrich feathers, Wax/seals/stamps, Jewellery/rings/bracelets/necklaces, Cloth/Clothing/textiles (dyes, buttons, silk, European cotton, ornamental threads, muslins, kaftans, wraps, flannel, canvas, brocade, wool, etc.)\n",
        "\n",
        "#### 5c. Consumables and Utilitarian Items (code=\"5c_related_consumerables\")\n",
        "**Question**: Was the bead connected to consumables or utilitarian items?\n",
        "\n",
        "**Format**: List all consumables and utilitarian items traded, separated by comma or NaN\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Coins (other foreign currencies), Livestock (e.g. chicken, goats, horses, donkeys, sheep, cows, etc.), Medicines/remedies/herbal plants, Spices/essences/fragrances/perfumes, Dried food/fruit (tamarinds/dates/honey/grains/etc.), Hardware/manufactures (pewter, zinc, etc.), Tobacco/snuff, Water\n",
        "\n",
        "#### 5d. Decoratives (code=\"5d_related_decoratives\")\n",
        "**Question**: Was the bead connected to decorative items?\n",
        "\n",
        "**Format**: List all decorative items traded, separated by comma or NaN\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Scarabs, Antiquities/furniture items/collectables, Indigenous weapons, spears, shields, Prints/artwork/books/paper/scrolls, Guns/gunpowder, Slaves, Glass objects, Musical instruments\n",
        "\n",
        "### 6. Bead Ethnic Group (code=\"6_bead_ethnic_group\")\n",
        "**Question**: Was an ethnic group mentioned in connection to the exchange?\n",
        "\n",
        "**Format**: Add ethnic group name(s) as STRING. If more than one ethnic group, separate with comma.\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"Zulu, Xhosa\"\n",
        "- \"Turkana\"\n",
        "- \"Wongára women from Bontúku\"\n",
        "- \"Hausa merchants from Sokotu\"\n",
        "\n",
        "**EXPERT ADDITION**: **\"natives, trading adventurers\"** (from historical pattern case)\n",
        "\n",
        "### 7. Market Town (code=\"7_market_town\")\n",
        "**Question**: Is the location of exchange described as a market?\n",
        "\n",
        "**Values**:\n",
        "- `0`: No\n",
        "- `1`: Yes\n",
        "\n",
        "**Decision rules**: Only mark `1` if the location is explicitly described as a market or trading center\n",
        "\n",
        "### 8. Location Name (code=\"8_location_name\")\n",
        "**Question**: Is there a mention of any geographic location?\n",
        "\n",
        "**Format**: Add the place name and description of the location (as detailed as available in text).\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"Between 4° and 5° north, 27° and 28° east\"\n",
        "- \"Unyamwezi, Jenne, north of lake Stefanie\"\n",
        "- \"Reshiat or Rissiat (northern Gallaland in northern Lake Rudolf)\"\n",
        "\n",
        "**EXPERT ADDITION**: **\"White Nile\"** (from historical pattern case)\n",
        "\n",
        "### 9. Place of Manufacture (code=\"9_place_of_manufacture\")\n",
        "**Question**: Is there a mention of the origin of the bead?\n",
        "\n",
        "**Format**: Add origin of the bead (geographic term).\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"Venetian glass beads\"\n",
        "- \"Beads from x region\"\n",
        "\n",
        "**Decision rules**: Origin refers to where beads were manufactured, not where they are used\n",
        "\n",
        "### 10. Beads Observed (code=\"10_beads_observed\")\n",
        "**Question**: List all beads traded at this location (even if not necessarily traded for specific items).\n",
        "\n",
        "#### 10a. Size (code=\"10a_size\")\n",
        "**Format**: Add size (large, medium, small, thin, thick)\n",
        "\n",
        "#### 10b. Color (code=\"10b_color\")\n",
        "**Format**: Add color\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Red, blue, white, pink, coral, amber, copper, transparent, green, yellow, black, multicolored\n",
        "\n",
        "#### 10c. Shape (code=\"10c_shape\")\n",
        "**Format**: Add shape\n",
        "\n",
        "**HISTORIAN EXAMPLES**: Round, tubular (like a long sausage), square, oval, oblong (like a squished sausage), punched with a hole, wound, pressed, decorative, faceted, bugle, chevron\n",
        "\n",
        "#### 10d. Type (code=\"10d_type\")\n",
        "**Format**: Add material type\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- Glass (also seed beads = small glass beads used as currency)\n",
        "- Clay\n",
        "- Metal (brass/copper/silver/gold/iron)\n",
        "- Stone (quartz/agate/carnelian/jasper/amethyst/lapis lazuli/turquoise/malachite)\n",
        "- Coral, bone, ivory, dried seed, ceramic, wooden, porcelain, shell (seashells), eggshell (ostrich)\n",
        "\n",
        "**EXPERT ADDITION**: **wooden** (from prayer bead case)\n",
        "\n",
        "**HISTORIAN COMPLETE ENTRY EXAMPLES**:\n",
        "- \"Small amber glass beads\"\n",
        "- \"Mid-sized oval blue beads\"\n",
        "- \"Transparent square glass beads\"\n",
        "- \"Large round coral beads\"\n",
        "- \"Small red seed beads\"\n",
        "- \"Medium faceted blue glass beads\"\n",
        "- \"Large tubular white porcelain beads\"\n",
        "- \"Small round black glass beads, large oval amber beads\"\n",
        "- \"Tiny multicolored glass seed beads\"\n",
        "- \"Medium wound glass beads with copper inclusions\"\n",
        "- \"Large chevron beads with blue, red, and white patterns\"\n",
        "- \"Small green tubular glass beads, medium round ivory beads\"\n",
        "- \"Large faceted crystal beads with gold flecks\"\n",
        "\n",
        "### 11. Units of Measurement (code=\"11_units_of_measurement\")\n",
        "**Question**: What were the units of measurement for the bead?\n",
        "\n",
        "**Values**:\n",
        "- `1`: String\n",
        "- `2`: Plaited string/woven string\n",
        "- `3`: Necklace, anklet, bracelet, waist beads, headwear\n",
        "- `4`: Other measurement (write in STRING name)\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"Strings of beads\" → `1`\n",
        "- \"Woven strand of beads\" → `2`\n",
        "- \"Beaded necklaces\" → `3`\n",
        "- \"Fathom of beads\" → `4, fathom`\n",
        "- \"Fundo of beads\" → `4, fundo`\n",
        "- \"Khete of beads\" → `4, khete`\n",
        "\n",
        "### 12. Local Name (code=\"12_local_name\")\n",
        "**Question**: Does the bead have an ethnic or other language name?\n",
        "\n",
        "**Format**: If yes, write the STRING of the name of the bead.\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"Aggrey beads, as the natives called them\" → `aggrey`\n",
        "- \"Samesame beads\" → `samesame`\n",
        "\n",
        "### 13. Notes (code=\"13_notes\")\n",
        "**Question**: Any notes and observations from this snippet that would be useful for context, future analysis, and for clarification of the coding?\n",
        "\n",
        "**Format**: Free text field for additional context and observations.\n",
        "\n",
        "**HISTORIAN EXAMPLES**:\n",
        "- \"The exchange took place during a period of drought, which may have affected values\"\n",
        "- \"Text mentions historical change in bead values over time - current rate vs. past rate\"\n",
        "- \"Author notes that the beads were considered especially valuable due to their rarity in this region\"\n",
        "- \"The exchange was part of a larger diplomatic mission, suggesting social aspects beyond commercial value\"\n",
        "- \"Local taboos influenced which colors of beads were acceptable for trade\"\n",
        "- \"Text mentions that exchange rates had been stable for several decades\"\n",
        "- \"The author appears unfamiliar with local customs and may have misinterpreted some aspects of the exchange\"\n",
        "- \"Exchange took place in context of religious festival, potentially affecting rates\"\n",
        "- \"Text indicates these particular beads had special ritual significance beyond their material value\"\n",
        "- \"Multiple transliterated local terms used for different bead types, suggesting complex local classification system\"\n",
        "\n",
        "**EXPERT ADDITIONS**:\n",
        "- **\"Historical generalization about trading patterns between natives and trading adventurers\"**\n",
        "- **\"Prayer beads observed, no exchange occurred\"**\n",
        "- **\"Decorative beads on horse equipment, commercial price list\"**\n",
        "- **\"Gift-giving for diplomatic relations\"**\n",
        "\n",
        "## EXPERT-VALIDATED EDGE CASE EXAMPLES\n",
        "\n",
        "### Historical Generalization Pattern (Expert Decision: 4a_exchange = \"no\")\n",
        "**Text**: \"When trading adventurers first commenced on the White Nile, the natives sold ivory for beads and copper bracelets\"\n",
        "**Expert Coding**:\n",
        "- `4a_exchange = \"no\"` (pattern, not specific transaction)\n",
        "- `4b_beads_exchanged = \"beads\"` (capture context)\n",
        "- `4c_exchanged_item = \"ivory\"` (capture context)\n",
        "- `5a_related_raw_materials = \"copper bracelets\"`\n",
        "- `13_notes = \"Historical generalization about early trading patterns\"`\n",
        "\n",
        "### Intangible Exchange Pattern (Expert Decision: Valid Exchange)\n",
        "**Text**: \"I bought the secret for a string of wooden beads\"\n",
        "**Expert Coding**:\n",
        "- `4a_exchange = \"xo\"` (intangible goods count)\n",
        "- `4c_exchanged_item = \"secret for writing amulets\"`\n",
        "- `10d_type = \"wooden\"`\n",
        "- `11_units_of_measurement = \"1\"` (string)\n",
        "\n",
        "## HISTORIAN CODING PROCESS FLOW\n",
        "\n",
        "1. Determine if the entry contains relevant information about beads (`read_entry`)\n",
        "2. Identify the function of beads in the described context (`1_function`)\n",
        "3. Assess the nature of any exchange described (`2_nature_of_exchange`)\n",
        "4. Document the groups involved in the exchange (`3_between_groups`)\n",
        "5. Note whether beads were actually exchanged (`4a_exchange`)\n",
        "6. Document the specific beads used in exchange (`4b_beads_exchanged`)\n",
        "7. Record what items were received in exchange for beads (`4c_exchanged_item`)\n",
        "8. List all items traded or given as gifts at the location (`5_related_items`)\n",
        "9. Note ethnic groups connected to the exchange (`6_bead_ethnic_group`)\n",
        "10. Indicate if the exchange location was a market (`7_market_town`)\n",
        "11. Document the geographic location (`8_location_name`)\n",
        "12. Note the place of manufacture of the beads (`9_place_of_manufacture`)\n",
        "13. Describe all beads observed in detail (`10_beads_observed`)\n",
        "14. Record how beads were measured or counted (`11_units_of_measurement`)\n",
        "15. Document any local names for beads (`12_local_name`)\n",
        "16. Add any additional relevant observations (`13_notes`)\n",
        "\n",
        "## HISTORIAN SPECIAL NOTES\n",
        "\n",
        "1. **Context is key**: Information about beads might be spread across several sentences or paragraphs.\n",
        "2. **Implicit references**: Sometimes beads are referenced indirectly or as part of a larger exchange.\n",
        "3. **Historical knowledge**: Certain terms (like \"coral beads\") have specific cultural or historical meanings.\n",
        "4. **Multiple passages**: When analyzing a document with multiple references to beads, consider each distinct reference separately.\n",
        "5. **Ambiguity**: When information is ambiguous, note this in your reasoning but select the most likely code.\n",
        "6. **Duplicate entries**: For every distinct exchange rate at the same market/location, duplicate the row and enter relevant exchange rates separately.\n",
        "\n",
        "## HISTORIAN COMMON PITFALLS TO AVOID\n",
        "\n",
        "1. **Missing implicit references** to beads in historical texts\n",
        "2. **Overlooking ethnic groups** mentioned in relation to beads\n",
        "3. **Confusing material with color** (e.g., coral is both a material and implies a color)\n",
        "4. **Missing associated goods** that appear in different parts of the text\n",
        "5. **Ignoring contextual clues** about function or exchange nature\n",
        "6. **Failing to specify location names** beyond just the numeric code\n",
        "7. **Missing tributary or gift exchanges** which should be coded as `xo` for exchange\n",
        "\n",
        "## XML OUTPUT FORMAT (Complete)\n",
        "\n",
        "<read_entry>1</read_entry>\n",
        "<1a_physical_function>2</1a_physical_function>\n",
        "<1b_trade_function>NaN</1b_trade_function>\n",
        "<1c_social_function>3</1c_social_function>\n",
        "<2_nature_of_exchange>4</2_nature_of_exchange>\n",
        "<3_between_groups>1</3_between_groups>\n",
        "<4a_exchange>xo</4a_exchange>\n",
        "<4b_beads_exchanged>glass beads</4b_beads_exchanged>\n",
        "<4c_exchanged_item>diplomatic relations</4c_exchanged_item>\n",
        "<5a_related_raw_materials>copper, iron</5a_related_raw_materials>\n",
        "<5b_related_jewerly_fashion>NaN</5b_related_jewerly_fashion>\n",
        "<5c_related_consumerables>NaN</5c_related_consumerables>\n",
        "<5d_related_decoratives>NaN</5d_related_decoratives>\n",
        "<6_bead_ethnic_group>Sheikh Wadelai's people</6_bead_ethnic_group>\n",
        "<7_market_town>0</7_market_town>\n",
        "<8_location_name>White Nile region</8_location_name>\n",
        "<9_place_of_manufacture>NaN</9_place_of_manufacture>\n",
        "<10a_size>NaN</10a_size>\n",
        "<10b_color>NaN</10b_color>\n",
        "<10c_shape>NaN</10c_shape>\n",
        "<10d_type>glass</10d_type>\n",
        "<11_units_of_measurement>NaN</11_units_of_measurement>\n",
        "<12_local_name>NaN</12_local_name>\n",
        "<13_notes>Gift-giving for diplomatic relations</13_notes>\n",
        "<confidence>0.95</confidence>\n",
        "\n",
        "## FINAL INSTRUCTION\n",
        "\n",
        "Apply preprocessing quality checks first. Code conservatively based on explicit evidence. For every distinct exchange rate at the same location, create separate entries. Capture contextual details even when coding \"no exchange\" for historical pattern analysis.\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED BACKUP SYSTEM FUNCTIONS WITH RESTART CAPABILITY\n",
        "# =============================================================================\n",
        "\n",
        "def save_codetect_backup(data, backup_type='pickle', custom_name=None):\n",
        "    \"\"\"Save Co-DETECT data to backup file with timestamp\"\"\"\n",
        "    global progress_counter, processed_indices\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = custom_name or f\"{BACKUP_FILENAME}_{timestamp}\"\n",
        "\n",
        "    # Enhanced backup includes tracking information\n",
        "    backup_data = {\n",
        "        'results': data if isinstance(data, list) else data,\n",
        "        'progress_counter': progress_counter,\n",
        "        'processed_indices': list(processed_indices),\n",
        "        'timestamp': timestamp,\n",
        "        'sample_size': SAMPLE_SIZE\n",
        "    }\n",
        "\n",
        "    if backup_type == 'pickle':\n",
        "        filepath = os.path.join(BACKUP_DIR, f\"{filename}.pkl\")\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(backup_data, f)\n",
        "    elif backup_type == 'json':\n",
        "        filepath = os.path.join(BACKUP_DIR, f\"{filename}.json\")\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(backup_data, f, indent=2, default=str)\n",
        "    elif backup_type == 'csv' and isinstance(data, pd.DataFrame):\n",
        "        filepath = os.path.join(BACKUP_DIR, f\"{filename}.csv\")\n",
        "        data.to_csv(filepath, index=False)\n",
        "        # Also save tracking info separately\n",
        "        tracking_file = os.path.join(BACKUP_DIR, f\"{filename}_tracking.json\")\n",
        "        with open(tracking_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'progress_counter': progress_counter,\n",
        "                'processed_indices': list(processed_indices),\n",
        "                'timestamp': timestamp\n",
        "            }, f)\n",
        "\n",
        "    print(f\"✅ Backup saved: {os.path.basename(filepath)}\")\n",
        "    print(f\"📊 Progress: {progress_counter}/{SAMPLE_SIZE} texts processed\")\n",
        "    return filepath\n",
        "\n",
        "def auto_backup_codetect(data, force=False):\n",
        "    \"\"\"Automatically backup Co-DETECT data every LINES_PER_BACKUP lines\"\"\"\n",
        "    global progress_counter\n",
        "\n",
        "    if progress_counter % LINES_PER_BACKUP == 0 or force:\n",
        "        return save_codetect_backup(data, backup_type='json', custom_name=f\"codetect_progress_{progress_counter}\")\n",
        "    return None\n",
        "\n",
        "def load_codetect_backup(custom_name=None):\n",
        "    \"\"\"Load the most recent Co-DETECT backup with enhanced tracking\"\"\"\n",
        "    pattern = custom_name or BACKUP_FILENAME\n",
        "\n",
        "    # Find all backup files\n",
        "    backup_files = [f for f in os.listdir(BACKUP_DIR)\n",
        "                   if f.startswith(pattern) and (f.endswith('.pkl') or f.endswith('.json'))]\n",
        "\n",
        "    if not backup_files:\n",
        "        print(\"❌ No Co-DETECT backup files found\")\n",
        "        return None\n",
        "\n",
        "    # Get the most recent file\n",
        "    latest_file = max(backup_files, key=lambda x: os.path.getmtime(os.path.join(BACKUP_DIR, x)))\n",
        "    filepath = os.path.join(BACKUP_DIR, latest_file)\n",
        "\n",
        "    # Load the data\n",
        "    if latest_file.endswith('.pkl'):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            backup_data = pickle.load(f)\n",
        "    elif latest_file.endswith('.json'):\n",
        "        with open(filepath, 'r') as f:\n",
        "            backup_data = json.load(f)\n",
        "\n",
        "    print(f\"✅ Loaded backup: {latest_file}\")\n",
        "\n",
        "    # Handle both old and new backup formats\n",
        "    if isinstance(backup_data, dict) and 'results' in backup_data:\n",
        "        # New format with tracking\n",
        "        return backup_data\n",
        "    else:\n",
        "        # Old format - just results\n",
        "        return {\n",
        "            'results': backup_data,\n",
        "            'progress_counter': len(backup_data) if isinstance(backup_data, list) else 0,\n",
        "            'processed_indices': [],\n",
        "            'timestamp': 'unknown'\n",
        "        }\n",
        "\n",
        "def list_available_backups():\n",
        "    \"\"\"List all available backup files with details\"\"\"\n",
        "    print(\"📁 Available Co-DETECT Backups:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    backup_files = [f for f in os.listdir(BACKUP_DIR)\n",
        "                   if f.startswith(BACKUP_FILENAME) and (f.endswith('.pkl') or f.endswith('.json'))]\n",
        "\n",
        "    if not backup_files:\n",
        "        print(\"❌ No backup files found\")\n",
        "        return []\n",
        "\n",
        "    # Sort by modification time (newest first)\n",
        "    backup_files.sort(key=lambda x: os.path.getmtime(os.path.join(BACKUP_DIR, x)), reverse=True)\n",
        "\n",
        "    backup_info = []\n",
        "    for i, filename in enumerate(backup_files[:10], 1):  # Show last 10 backups\n",
        "        filepath = os.path.join(BACKUP_DIR, filename)\n",
        "        mod_time = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
        "        file_size = os.path.getsize(filepath) / 1024  # KB\n",
        "\n",
        "        # Try to read progress info\n",
        "        try:\n",
        "            if filename.endswith('.json'):\n",
        "                with open(filepath, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict) and 'progress_counter' in data:\n",
        "                        progress = data['progress_counter']\n",
        "                    else:\n",
        "                        progress = len(data) if isinstance(data, list) else 'Unknown'\n",
        "            else:\n",
        "                progress = 'Unknown'\n",
        "        except:\n",
        "            progress = 'Unknown'\n",
        "\n",
        "        print(f\"{i}. {filename}\")\n",
        "        print(f\"   Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"   Size: {file_size:.1f} KB | Progress: {progress}\")\n",
        "        print()\n",
        "\n",
        "        backup_info.append({\n",
        "            'filename': filename,\n",
        "            'filepath': filepath,\n",
        "            'modified': mod_time,\n",
        "            'progress': progress\n",
        "        })\n",
        "\n",
        "    return backup_info\n",
        "\n",
        "def restore_from_backup(backup_name=None):\n",
        "    \"\"\"\n",
        "    ENHANCED: Restore Co-DETECT session from a specific backup\n",
        "    This is the main function to use when restarting after interruption\n",
        "    \"\"\"\n",
        "    global codetect_results, progress_counter, processed_indices, session_start_time\n",
        "\n",
        "    print(\"🔄 RESTORING CO-DETECT SESSION FROM BACKUP\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if backup_name:\n",
        "        # Load specific backup\n",
        "        filepath = os.path.join(BACKUP_DIR, backup_name)\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"❌ Backup file not found: {backup_name}\")\n",
        "            return False\n",
        "\n",
        "        if filepath.endswith('.pkl'):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                backup_data = pickle.load(f)\n",
        "        else:\n",
        "            with open(filepath, 'r') as f:\n",
        "                backup_data = json.load(f)\n",
        "    else:\n",
        "        # Load most recent backup\n",
        "        backup_data = load_codetect_backup()\n",
        "        if not backup_data:\n",
        "            return False\n",
        "\n",
        "    # Extract data from backup\n",
        "    if isinstance(backup_data, dict) and 'results' in backup_data:\n",
        "        # New format\n",
        "        codetect_results = backup_data['results']\n",
        "        progress_counter = backup_data.get('progress_counter', len(codetect_results))\n",
        "        processed_indices = set(backup_data.get('processed_indices', []))\n",
        "        backup_timestamp = backup_data.get('timestamp', 'unknown')\n",
        "\n",
        "        # If processed_indices is empty but we have results, rebuild it\n",
        "        if not processed_indices and codetect_results:\n",
        "            processed_indices = set(r.get('row_index', r.get('text_id', i))\n",
        "                                   for i, r in enumerate(codetect_results))\n",
        "    else:\n",
        "        # Old format\n",
        "        codetect_results = backup_data if isinstance(backup_data, list) else []\n",
        "        progress_counter = len(codetect_results)\n",
        "        # Rebuild processed_indices from results\n",
        "        processed_indices = set(r.get('row_index', r.get('text_id', i))\n",
        "                               for i, r in enumerate(codetect_results))\n",
        "        backup_timestamp = 'unknown'\n",
        "\n",
        "    session_start_time = datetime.now()\n",
        "\n",
        "    print(f\"✅ SESSION RESTORED SUCCESSFULLY\")\n",
        "    print(f\"   Backup timestamp: {backup_timestamp}\")\n",
        "    print(f\"   Texts already processed: {len(codetect_results)}\")\n",
        "    print(f\"   Progress counter: {progress_counter}/{SAMPLE_SIZE}\")\n",
        "    print(f\"   Remaining texts: {SAMPLE_SIZE - progress_counter}\")\n",
        "    print(f\"   Processed indices tracked: {len(processed_indices)}\")\n",
        "    print()\n",
        "    print(\"💡 You can now continue with: resume_codetect_analysis(client, sample_df)\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# =============================================================================\n",
        "# ANTHROPIC API SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def setup_anthropic_client():\n",
        "    \"\"\"Set up Anthropic API client\"\"\"\n",
        "    print(\"🔑 Setting up Anthropic API client...\")\n",
        "\n",
        "    # Get API key from Colab secrets\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "        print(\"✅ API key loaded from Colab secrets\")\n",
        "    except:\n",
        "        # Fallback: enter API key manually\n",
        "        import getpass\n",
        "        api_key = getpass.getpass(\"Enter your Anthropic API key: \")\n",
        "\n",
        "    client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "    # Test the connection\n",
        "    try:\n",
        "        test_response = client.messages.create(\n",
        "            model=\"claude-3-5-haiku-20241022\",\n",
        "            max_tokens=10,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Test\"}]\n",
        "        )\n",
        "        print(\"✅ Anthropic API connection successful!\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error connecting to Anthropic API: {e}\")\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load and preprocess the bead exchange data\"\"\"\n",
        "    print(\"📊 Loading and preparing bead exchange data...\")\n",
        "\n",
        "    file_path = os.path.join(WORKING_DIR, DATASET_FILE)\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"❌ File not found: {file_path}\")\n",
        "        print(f\"Please upload '{DATASET_FILE}' to {WORKING_DIR}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load Excel file\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"✅ Original dataset: {len(df):,} rows\")\n",
        "\n",
        "        # Basic preprocessing\n",
        "        valid_df = df.dropna(subset=['text_page_gp']).copy()\n",
        "        print(f\"✅ Valid text entries: {len(valid_df):,}\")\n",
        "\n",
        "        # Apply expert-validated data quality filters\n",
        "        filtered_df = apply_data_quality_filters(valid_df)\n",
        "\n",
        "        # Create stratified sample\n",
        "        sample_df = create_stratified_sample(filtered_df, SAMPLE_SIZE)\n",
        "\n",
        "        return sample_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def apply_data_quality_filters(df):\n",
        "    \"\"\"Apply expert-validated data quality preprocessing rules\"\"\"\n",
        "    print(\"🔍 Applying expert-validated data quality filters...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Rule P1: Text length filtering\n",
        "    df['text_length'] = df['text_page_gp'].str.len()\n",
        "    df['readable_chars'] = df['text_page_gp'].str.count(r'[a-zA-Z]')\n",
        "    df['readable_ratio'] = df['readable_chars'] / df['text_length']\n",
        "\n",
        "    # Filter very short texts (<50 chars)\n",
        "    quality_df = df[df['text_length'] >= 50].copy()\n",
        "    removed_short = len(df) - len(quality_df)\n",
        "    print(f\"📏 After length filter (≥50 chars): {len(quality_df):,} entries ({removed_short} removed)\")\n",
        "\n",
        "    # Filter heavily corrupted OCR (>80% non-alphabetic)\n",
        "    quality_df = quality_df[quality_df['readable_ratio'] >= 0.2].copy()\n",
        "    removed_corrupted = len(df) - removed_short - len(quality_df)\n",
        "    print(f\"🔤 After OCR quality filter (≥20% readable): {len(quality_df):,} entries ({removed_corrupted} removed)\")\n",
        "\n",
        "    # Rule P2: Content relevance filtering for context pages\n",
        "    bead_terms = ['bead', 'beads', 'pearl', 'coral', 'glass', 'necklace', 'bracelet', 'ornament', 'jewelry']\n",
        "\n",
        "    def has_bead_content(text, page_type):\n",
        "        if pd.isna(text):\n",
        "            return False\n",
        "        text_lower = str(text).lower()\n",
        "        has_bead_terms = any(term in text_lower for term in bead_terms)\n",
        "\n",
        "        # If it's a context page, it must have bead terms\n",
        "        if page_type == 'support' and not has_bead_terms:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    quality_df['has_bead_content'] = quality_df.apply(\n",
        "        lambda row: has_bead_content(row['text_page_gp'], row.get('page_type', '')), axis=1\n",
        "    )\n",
        "\n",
        "    quality_df = quality_df[quality_df['has_bead_content']].copy()\n",
        "    removed_irrelevant = initial_count - removed_short - removed_corrupted - len(quality_df)\n",
        "    print(f\"📋 After content relevance filter: {len(quality_df):,} entries ({removed_irrelevant} removed)\")\n",
        "\n",
        "    total_removed = initial_count - len(quality_df)\n",
        "    print(f\"✅ Total filtered out: {total_removed:,} entries ({(total_removed/initial_count*100):.1f}%)\")\n",
        "\n",
        "    return quality_df\n",
        "\n",
        "def create_stratified_sample(df, sample_size):\n",
        "    \"\"\"Create stratified sample for Co-DETECT analysis\"\"\"\n",
        "    print(f\"🎯 Creating stratified sample of {sample_size:,} texts...\")\n",
        "\n",
        "    # Stratify by country and explorer to ensure diversity\n",
        "    sample_df = df.groupby(['countries', 'explorer_surname'], group_keys=False).apply(\n",
        "        lambda x: x.sample(min(len(x), max(1, int(sample_size * len(x) / len(df)))))\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # If we don't have enough, fill randomly\n",
        "    if len(sample_df) < sample_size:\n",
        "        remaining = df[~df.index.isin(sample_df.index)]\n",
        "        additional_needed = sample_size - len(sample_df)\n",
        "        if len(remaining) >= additional_needed:\n",
        "            additional = remaining.sample(additional_needed)\n",
        "            sample_df = pd.concat([sample_df, additional]).reset_index(drop=True)\n",
        "\n",
        "    # Trim to exact size if over\n",
        "    if len(sample_df) > sample_size:\n",
        "        sample_df = sample_df.sample(sample_size).reset_index(drop=True)\n",
        "\n",
        "    print(f\"✅ Prepared dataset: {len(sample_df):,} texts\")\n",
        "    return sample_df\n",
        "\n",
        "def estimate_costs(sample_size):\n",
        "    \"\"\"Estimate API costs for the analysis\"\"\"\n",
        "    input_tokens = sample_size * ESTIMATED_TOKENS_PER_TEXT\n",
        "    output_tokens = sample_size * 300  # Estimated output tokens per response\n",
        "\n",
        "    input_cost = (input_tokens / 1000) * COST_PER_1K_TOKENS_INPUT\n",
        "    output_cost = (output_tokens / 1000) * COST_PER_1K_TOKENS_OUTPUT\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    print(f\"💰 Estimated cost for {sample_size:,} texts: ${total_cost:.2f}\")\n",
        "    print(f\"   Input tokens: {input_tokens:,} (${input_cost:.2f})\")\n",
        "    print(f\"   Output tokens: {output_tokens:,} (${output_cost:.2f})\")\n",
        "    print(f\"   Expected analysis time: ~{sample_size//25:.0f} minutes (Claude 3.5 Haiku is fast!)\")\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "# =============================================================================\n",
        "# AI ANNOTATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def create_annotation_prompt(text):\n",
        "    \"\"\"Create annotation prompt with expert-validated codebook\"\"\"\n",
        "    return f\"\"\"\n",
        "You are an expert historical researcher analyzing bead exchange patterns. Use the following expert-validated codebook to analyze this historical text about beads.\n",
        "\n",
        "EXPERT-VALIDATED CODEBOOK:\n",
        "{EXPERT_CODEBOOK}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Apply preprocessing data quality rules first\n",
        "2. Follow the mandatory decision flowchart\n",
        "3. Use expert-validated examples for edge cases\n",
        "4. When in doubt, be CONSERVATIVE and require explicit evidence\n",
        "5. Provide a confidence score (0.0-1.0) for your analysis\n",
        "\n",
        "TEXT TO ANALYZE:\n",
        "\"{text}\"\n",
        "\n",
        "Provide your analysis in this XML format:\n",
        "\n",
        "<analysis>\n",
        "<read_entry>[0/1/2/NaN]</read_entry>\n",
        "<4a_exchange>[xo/no/NA/NaN]</4a_exchange>\n",
        "<2_nature_of_exchange>[1/2/3/4/NA/NaN]</2_nature_of_exchange>\n",
        "<3_between_groups>[1/2/3/4/NA/NaN]</3_between_groups>\n",
        "<4b_beads_exchanged>[description or NA/NaN]</4b_beads_exchanged>\n",
        "<4c_exchanged_item>[description or NA/NaN]</4c_exchanged_item>\n",
        "<1a_physical_function>[2/NA/NaN]</1a_physical_function>\n",
        "<1b_trade_function>[2/NA/NaN]</1b_trade_function>\n",
        "<1c_social_function>[3/NA/NaN]</1c_social_function>\n",
        "<6_bead_ethnic_group>[groups or NA/NaN]</6_bead_ethnic_group>\n",
        "<8_location_name>[location or NA/NaN]</8_location_name>\n",
        "<10d_type>[material or NA/NaN]</10d_type>\n",
        "<11_units_of_measurement>[1/2/3/4/NA/NaN]</11_units_of_measurement>\n",
        "<13_notes>[additional context or NA/NaN]</13_notes>\n",
        "<confidence>[0.0-1.0]</confidence>\n",
        "<reasoning>[brief explanation of key decisions]</reasoning>\n",
        "</analysis>\n",
        "\"\"\"\n",
        "\n",
        "def parse_xml_response(response_text):\n",
        "    \"\"\"Parse XML response from AI annotation\"\"\"\n",
        "    import xml.etree.ElementTree as ET\n",
        "\n",
        "    try:\n",
        "        # Extract XML from response\n",
        "        start_tag = response_text.find('<analysis>')\n",
        "        end_tag = response_text.find('</analysis>') + len('</analysis>')\n",
        "\n",
        "        if start_tag == -1 or end_tag == -1:\n",
        "            return None\n",
        "\n",
        "        xml_content = response_text[start_tag:end_tag]\n",
        "        root = ET.fromstring(xml_content)\n",
        "\n",
        "        result = {}\n",
        "        for child in root:\n",
        "            result[child.tag] = child.text if child.text else \"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            # Fallback: try to extract confidence at minimum\n",
        "            import re\n",
        "            conf_match = re.search(r'<confidence>([\\d.]+)</confidence>', response_text)\n",
        "            if conf_match:\n",
        "                return {'confidence': conf_match.group(1)}\n",
        "        except:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "def annotate_single_text(client, text, text_id, row_data):\n",
        "    \"\"\"Annotate a single text using AI with expert-validated codebook\"\"\"\n",
        "    global progress_counter, codetect_results, processed_indices\n",
        "\n",
        "    progress_counter += 1\n",
        "    print(f\"Processing {text_id}/{SAMPLE_SIZE}: \", end=\"\", flush=True)\n",
        "\n",
        "    prompt = create_annotation_prompt(text)\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-5-haiku-20241022\",\n",
        "                max_tokens=1500,\n",
        "                temperature=0.1,  # Low temperature for consistency\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            # Parse XML response\n",
        "            result = parse_xml_response(response_text)\n",
        "\n",
        "            if result and 'confidence' in result:\n",
        "                confidence = float(result['confidence'])\n",
        "                status = \"EDGE CASE\" if confidence < EDGE_CASE_THRESHOLD else \"OK\"\n",
        "                print(f\"{status} (conf: {confidence:.2f})\")\n",
        "\n",
        "                # Add metadata\n",
        "                result['text_id'] = text_id\n",
        "                result['original_text'] = text\n",
        "                result['row_index'] = row_data.name if hasattr(row_data, 'name') else text_id\n",
        "                result['explorer'] = f\"{row_data.get('explorer_first_name', '')} {row_data.get('explorer_surname', '')}\"\n",
        "                result['year'] = row_data.get('year_began', '')\n",
        "                result['countries'] = row_data.get('countries', '')\n",
        "\n",
        "                # Add to global results and tracking\n",
        "                codetect_results.append(result)\n",
        "                processed_indices.add(result['row_index'])\n",
        "\n",
        "                # Auto-backup progress\n",
        "                auto_backup_codetect(codetect_results)\n",
        "\n",
        "                return result\n",
        "            else:\n",
        "                print(\"Error parsing response: \" + str(response_text)[:50])\n",
        "                if attempt < MAX_RETRIES - 1:\n",
        "                    print(\"\\nRetrying...\", end=\"\")\n",
        "                    time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)[:50]}\")\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                print(\"\\nRetrying...\", end=\"\")\n",
        "                time.sleep(5)\n",
        "\n",
        "    # Fallback for failed cases\n",
        "    print(\"FAILED\")\n",
        "    failed_result = {\n",
        "        'text_id': text_id,\n",
        "        'confidence': 0.0,\n",
        "        'read_entry': 'NaN',\n",
        "        'error': 'Failed to process after retries',\n",
        "        'original_text': text,\n",
        "        'row_index': row_data.name if hasattr(row_data, 'name') else text_id\n",
        "    }\n",
        "    codetect_results.append(failed_result)\n",
        "    processed_indices.add(failed_result['row_index'])\n",
        "    return failed_result\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN CO-DETECT FUNCTIONS WITH RESTART CAPABILITY\n",
        "# =============================================================================\n",
        "\n",
        "def run_annotation_phase(client, sample_df, resume=False):\n",
        "    \"\"\"\n",
        "    Run the annotation phase with automatic backup and resume capability\n",
        "\n",
        "    Args:\n",
        "        client: Anthropic API client\n",
        "        sample_df: DataFrame with texts to annotate\n",
        "        resume: If True, skip already processed texts\n",
        "    \"\"\"\n",
        "    global progress_counter, codetect_results, processed_indices\n",
        "\n",
        "    print(f\"🏷️ Step 2: Annotating texts...\")\n",
        "    print(\"💾 Auto-backup enabled every 50 texts\")\n",
        "\n",
        "    if resume:\n",
        "        print(f\"♻️ RESUME MODE: Skipping {len(processed_indices)} already processed texts\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process texts\n",
        "    texts_to_process = len(sample_df) - len(processed_indices) if resume else len(sample_df)\n",
        "    print(f\"📊 Texts to process: {texts_to_process}\")\n",
        "\n",
        "    for idx, row in sample_df.iterrows():\n",
        "        # Skip if already processed (when resuming)\n",
        "        if resume and idx in processed_indices:\n",
        "            continue\n",
        "\n",
        "        text = row['text_page_gp']\n",
        "        result = annotate_single_text(client, text, idx + 1, row)\n",
        "\n",
        "        # Save major checkpoint every 200 texts\n",
        "        if progress_counter % 200 == 0:\n",
        "            save_codetect_backup(codetect_results, backup_type='json',\n",
        "                               custom_name=f\"codetect_checkpoint_{progress_counter}\")\n",
        "\n",
        "    # Force final backup\n",
        "    save_codetect_backup(codetect_results, backup_type='json',\n",
        "                        custom_name=f\"codetect_final_{progress_counter}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    edge_cases = [r for r in codetect_results if float(r.get('confidence', 0)) < EDGE_CASE_THRESHOLD]\n",
        "\n",
        "    print(f\"\\n✅ Annotation complete: {len(edge_cases)} edge cases found ({len(edge_cases)/len(codetect_results)*100:.1f}%)\")\n",
        "    print(f\"⏱️ Time elapsed: {elapsed_time/60:.1f} minutes\")\n",
        "\n",
        "    return codetect_results\n",
        "\n",
        "def resume_codetect_analysis(client, sample_df):\n",
        "    \"\"\"\n",
        "    MAIN FUNCTION TO RESUME INTERRUPTED ANALYSIS\n",
        "\n",
        "    Use this after calling restore_from_backup() to continue where you left off\n",
        "\n",
        "    Args:\n",
        "        client: Anthropic API client (from setup_anthropic_client())\n",
        "        sample_df: DataFrame with texts (from load_and_prepare_data())\n",
        "    \"\"\"\n",
        "    global codetect_results, progress_counter, processed_indices\n",
        "\n",
        "    print(\"🔄 RESUMING CO-DETECT ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"📊 Current status:\")\n",
        "    print(f\"   Already processed: {len(codetect_results)} texts\")\n",
        "    print(f\"   Progress counter: {progress_counter}\")\n",
        "    print(f\"   Remaining: {SAMPLE_SIZE - len(processed_indices)} texts\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if len(processed_indices) >= SAMPLE_SIZE:\n",
        "        print(\"✅ All texts already processed!\")\n",
        "        print(\"💡 Run analyze_edge_cases() and create_analysis_visualizations() to continue\")\n",
        "        return codetect_results\n",
        "\n",
        "    proceed = input(\"\\n▶️  Continue annotation from where we left off? (y/n): \")\n",
        "    if proceed.lower() != 'y':\n",
        "        print(\"Analysis paused.\")\n",
        "        return codetect_results\n",
        "\n",
        "    # Continue annotation with resume=True\n",
        "    results = run_annotation_phase(client, sample_df, resume=True)\n",
        "\n",
        "    print(\"\\n✅ RESUME COMPLETE!\")\n",
        "    print(f\"📊 Total texts processed: {len(results)}\")\n",
        "    print(\"💡 Next steps:\")\n",
        "    print(\"   1. analyze_edge_cases()\")\n",
        "    print(\"   2. create_analysis_visualizations()\")\n",
        "    print(\"   3. generate_analysis_report()\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_edge_cases():\n",
        "    \"\"\"Analyze edge cases and create clusters\"\"\"\n",
        "    global codetect_results\n",
        "\n",
        "    edge_cases = [r for r in codetect_results if float(r.get('confidence', 0)) < EDGE_CASE_THRESHOLD]\n",
        "\n",
        "    if len(edge_cases) == 0:\n",
        "        print(\"🎉 No edge cases found! Codebook performing excellently.\")\n",
        "        return [], []\n",
        "\n",
        "    print(f\"🔍 Step 3: Clustering {len(edge_cases)} edge cases...\")\n",
        "\n",
        "    if len(edge_cases) < 3:\n",
        "        print(\"⚠️ Too few edge cases for clustering\")\n",
        "        return edge_cases, []\n",
        "\n",
        "    # Extract texts for clustering\n",
        "    texts = [case.get('original_text', '') for case in edge_cases]\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "    text_vectors = vectorizer.fit_transform(texts)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    n_clusters = min(max(2, len(edge_cases) // 5), 5)\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(text_vectors)\n",
        "\n",
        "    print(f\"✅ Created {n_clusters} balanced clusters\")\n",
        "\n",
        "    # Group edge cases by cluster\n",
        "    clustered_cases = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_cases = [edge_cases[j] for j in range(len(edge_cases)) if cluster_labels[j] == i]\n",
        "        if cluster_cases:\n",
        "            clustered_cases.append({\n",
        "                'cluster_id': i + 1,\n",
        "                'size': len(cluster_cases),\n",
        "                'cases': cluster_cases\n",
        "            })\n",
        "\n",
        "    return edge_cases, clustered_cases\n",
        "\n",
        "def create_analysis_visualizations():\n",
        "    \"\"\"Create comprehensive visualizations of Co-DETECT results\"\"\"\n",
        "    global codetect_results\n",
        "\n",
        "    print(\"📊 Step 5: Creating visualizations...\")\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    df_results = pd.DataFrame(codetect_results)\n",
        "    df_results['confidence'] = pd.to_numeric(df_results['confidence'], errors='coerce')\n",
        "    df_results['is_edge_case'] = df_results['confidence'] < EDGE_CASE_THRESHOLD\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=['Confidence Distribution', 'Edge Case Analysis',\n",
        "                       'Exchange Patterns', 'Function Patterns'],\n",
        "        specs=[[{'type': 'histogram'}, {'type': 'bar'}],\n",
        "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
        "    )\n",
        "\n",
        "    # 1. Confidence distribution\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=df_results['confidence'], nbinsx=20, name='Confidence Distribution'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # 2. Edge case analysis\n",
        "    edge_case_counts = df_results['is_edge_case'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=['Normal Cases', 'Edge Cases'],\n",
        "               y=[edge_case_counts.get(False, 0), edge_case_counts.get(True, 0)],\n",
        "               name='Case Distribution'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # 3. Exchange patterns\n",
        "    exchange_counts = df_results['4a_exchange'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=exchange_counts.index, y=exchange_counts.values, name='Exchange Patterns'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # 4. Function patterns\n",
        "    function_counts = df_results['1c_social_function'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=function_counts.index, y=function_counts.values, name='Social Functions'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=f\"Co-DETECT Analysis Results - Expert-Validated Codebook v4.0 (n={len(codetect_results):,})\",\n",
        "        showlegend=False,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "def generate_analysis_report():\n",
        "    \"\"\"Generate comprehensive Co-DETECT analysis report\"\"\"\n",
        "    global codetect_results\n",
        "\n",
        "    print(\"📋 Generating comprehensive analysis report...\")\n",
        "\n",
        "    df_results = pd.DataFrame(codetect_results)\n",
        "    df_results['confidence'] = pd.to_numeric(df_results['confidence'], errors='coerce')\n",
        "\n",
        "    total_cases = len(codetect_results)\n",
        "    edge_cases = [r for r in codetect_results if float(r.get('confidence', 0)) < EDGE_CASE_THRESHOLD]\n",
        "    edge_count = len(edge_cases)\n",
        "    edge_percentage = (edge_count / total_cases) * 100\n",
        "\n",
        "    # Calculate expected reduction (scaling from 800 sample to 1500 sample)\n",
        "    expected_original_edges = int(30)  # Expected ~30 edge cases in 1500 sample at 2% rate\n",
        "\n",
        "    report = f\"\"\"\n",
        "# Co-DETECT Analysis Report - Iteration 2\n",
        "## Expert-Validated Codebook v4.0\n",
        "\n",
        "### Executive Summary\n",
        "- **Total texts analyzed**: {total_cases:,}\n",
        "- **Edge cases identified**: {edge_count} ({edge_percentage:.1f}%)\n",
        "- **Target achievement**: {'✅ ACHIEVED' if edge_percentage < 1.0 else '🎯 IN PROGRESS'} (Target: <1.0%)\n",
        "- **Improvement estimate**: {expected_original_edges - edge_count} fewer edge cases vs. baseline ({((expected_original_edges - edge_count)/expected_original_edges*100):.1f}% reduction)\n",
        "- **Sample size increase**: 1,500 texts (vs. 800 in previous iterations)\n",
        "\n",
        "### Confidence Score Analysis\n",
        "- **Mean confidence**: {df_results['confidence'].mean():.3f}\n",
        "- **Median confidence**: {df_results['confidence'].median():.3f}\n",
        "- **High confidence cases** (≥0.9): {len(df_results[df_results['confidence'] >= 0.9])} ({len(df_results[df_results['confidence'] >= 0.9])/total_cases*100:.1f}%)\n",
        "- **Medium confidence cases** (0.7-0.89): {len(df_results[(df_results['confidence'] >= 0.7) & (df_results['confidence'] < 0.9)])} ({len(df_results[(df_results['confidence'] >= 0.7) & (df_results['confidence'] < 0.9)])/total_cases*100:.1f}%)\n",
        "- **Edge cases** (<0.7): {edge_count} ({edge_percentage:.1f}%)\n",
        "\n",
        "### Exchange Pattern Analysis\n",
        "\"\"\"\n",
        "\n",
        "    # Add exchange analysis\n",
        "    if '4a_exchange' in df_results.columns:\n",
        "        exchange_counts = df_results['4a_exchange'].value_counts()\n",
        "        for exchange_type, count in exchange_counts.items():\n",
        "            percentage = (count / total_cases) * 100\n",
        "            report += f\"- **{exchange_type}**: {count} cases ({percentage:.1f}%)\\n\"\n",
        "\n",
        "    # Add expert validation summary\n",
        "    report += f\"\"\"\n",
        "\n",
        "### Expert Validation Impact\n",
        "This iteration tested expert decisions on 1,500 texts:\n",
        "\n",
        "1. **Historical Generalizations**: Systematic handling of pattern descriptions\n",
        "2. **Intangible Exchanges**: Recognition of knowledge/service exchanges\n",
        "3. **Observational Contexts**: Proper classification without false exchanges\n",
        "4. **Gift-Giving**: Correct identification as social exchanges\n",
        "5. **Data Quality Filtering**: Automatic elimination of corrupted content\n",
        "\n",
        "### Data Quality Filtering Results\n",
        "- **Corrupted OCR filtered**: Entries with <50 chars or >80% symbols\n",
        "- **Irrelevant content filtered**: Context pages without bead references\n",
        "- **Processing efficiency**: Focus on valid bead-related content only\n",
        "\n",
        "### Recommendations for Next Steps\n",
        "\"\"\"\n",
        "\n",
        "    if edge_percentage > 1.0:\n",
        "        report += f\"- **{edge_count} edge cases remain** - analyze for new patterns\\n\"\n",
        "        report += \"- Consider additional expert review of remaining cases\\n\"\n",
        "        report += \"- Potential for final codebook refinement iteration\\n\"\n",
        "    else:\n",
        "        report += \"- **🎉 TARGET ACHIEVED!** Edge case rate below 1.0%\\n\"\n",
        "        report += \"- **Ready for full dataset application**\\n\"\n",
        "        report += \"- Consider final validation with human expert spot-checks\\n\"\n",
        "\n",
        "    report += f\"\\n### Cost and Efficiency Analysis\\n\"\n",
        "    report += f\"- **Actual processing time**: {datetime.now().strftime('%H:%M:%S')}\\n\"\n",
        "    report += f\"- **Sample size**: {total_cases:,} texts\\n\"\n",
        "    report += f\"- **Success rate**: {((total_cases - len([r for r in codetect_results if 'error' in r]))/total_cases*100):.1f}%\\n\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def setup_codetect_analysis():\n",
        "    \"\"\"Initialize Co-DETECT analysis environment\"\"\"\n",
        "    print(\"🎯 Co-DETECT Implementation for Bead Exchange Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Expert-Validated Codebook v4.0 with Edge Case Optimizations\")\n",
        "    print(\"✨ ENHANCED with Robust Restart Capabilities\")\n",
        "    print(\"🚀 Using Claude 3.5 Haiku (Fast & Cost-Efficient)\")\n",
        "    print(f\"📊 Configuration:\")\n",
        "    print(f\"   Sample size: {SAMPLE_SIZE:,} texts\")\n",
        "    print(f\"   Estimated cost: ${estimate_costs(SAMPLE_SIZE):.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize setup\n",
        "    global progress_counter, codetect_results, processed_indices, session_start_time\n",
        "    progress_counter = 0\n",
        "    codetect_results = []\n",
        "    processed_indices = set()\n",
        "    session_start_time = datetime.now()\n",
        "\n",
        "    return True\n",
        "\n",
        "def run_codetect_iteration():\n",
        "    \"\"\"Run complete Co-DETECT analysis iteration\"\"\"\n",
        "\n",
        "    print(\"🚀 Co-DETECT Analysis - Iteration 2 (RESTART-ENABLED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Data preparation\n",
        "    print(\"📊 Step 1: Data Preparation\")\n",
        "    sample_df = load_and_prepare_data()\n",
        "    if sample_df is None:\n",
        "        return None\n",
        "\n",
        "    # Confirm to proceed\n",
        "    proceed = input(f\"\\n💰 Estimated cost: ${estimate_costs(SAMPLE_SIZE):.2f}\\nProceed with analysis? (y/n): \")\n",
        "    if proceed.lower() != 'y':\n",
        "        print(\"Analysis cancelled.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Setup AI client\n",
        "    client = setup_anthropic_client()\n",
        "    if client is None:\n",
        "        return None\n",
        "\n",
        "    # Step 3: Run annotation with auto-backup\n",
        "    results = run_annotation_phase(client, sample_df)\n",
        "\n",
        "    # Step 4: Analyze edge cases\n",
        "    edge_cases, clustered_cases = analyze_edge_cases()\n",
        "\n",
        "    # Step 5: Create visualizations\n",
        "    fig = create_analysis_visualizations()\n",
        "\n",
        "    # Step 6: Generate report\n",
        "    report = generate_analysis_report()\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📋 FINAL ANALYSIS REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(report)\n",
        "\n",
        "    # Step 7: Save final results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_results = {\n",
        "        'results': codetect_results,\n",
        "        'edge_cases': edge_cases,\n",
        "        'clustered_cases': clustered_cases,\n",
        "        'report': report,\n",
        "        'sample_df': sample_df.to_dict('records'),\n",
        "        'config': {\n",
        "            'sample_size': SAMPLE_SIZE,\n",
        "            'edge_case_threshold': EDGE_CASE_THRESHOLD,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "    }\n",
        "\n",
        "    save_codetect_backup(final_results, backup_type='json', custom_name=f\"codetect_final_{timestamp}\")\n",
        "\n",
        "    # Also save as CSV for easy analysis\n",
        "    results_df = pd.DataFrame(codetect_results)\n",
        "    save_codetect_backup(results_df, backup_type='csv', custom_name=f\"codetect_results_{timestamp}\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# =============================================================================\n",
        "# RECOVERY AND UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def check_progress():\n",
        "    \"\"\"Check current progress of Co-DETECT analysis\"\"\"\n",
        "    global codetect_results, progress_counter, processed_indices, session_start_time\n",
        "\n",
        "    if len(codetect_results) == 0:\n",
        "        print(\"📊 No progress yet - run setup_codetect_analysis() to begin\")\n",
        "        return\n",
        "\n",
        "    edge_cases = [r for r in codetect_results if float(r.get('confidence', 0)) < EDGE_CASE_THRESHOLD]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"📊 CURRENT CO-DETECT PROGRESS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"   Texts processed: {len(codetect_results)}/{SAMPLE_SIZE}\")\n",
        "    print(f\"   Progress: {(len(codetect_results)/SAMPLE_SIZE*100):.1f}%\")\n",
        "    print(f\"   Edge cases so far: {len(edge_cases)} ({len(edge_cases)/len(codetect_results)*100:.1f}%)\")\n",
        "    print(f\"   Average confidence: {np.mean([float(r.get('confidence', 0)) for r in codetect_results]):.3f}\")\n",
        "    print(f\"   Unique indices tracked: {len(processed_indices)}\")\n",
        "\n",
        "    if session_start_time:\n",
        "        elapsed = (datetime.now() - session_start_time).total_seconds() / 60\n",
        "        print(f\"   Session duration: {elapsed:.1f} minutes\")\n",
        "\n",
        "    if len(codetect_results) > 0:\n",
        "        try:\n",
        "            latest_backup = max([f for f in os.listdir(BACKUP_DIR) if f.startswith('codetect_progress')],\n",
        "                               key=lambda x: os.path.getmtime(os.path.join(BACKUP_DIR, x)), default=None)\n",
        "            if latest_backup:\n",
        "                backup_time = datetime.fromtimestamp(os.path.getmtime(os.path.join(BACKUP_DIR, latest_backup)))\n",
        "                print(f\"   Latest backup: {latest_backup}\")\n",
        "                print(f\"   Backup time: {backup_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUTION INSTRUCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"✅ Co-DETECT Analysis System Initialized!\")\n",
        "print(\"🔄 RESTART-ENABLED VERSION\")\n",
        "print(f\"📁 Working directory: {WORKING_DIR}\")\n",
        "print(f\"📄 Dataset file: {DATASET_FILE}\")\n",
        "print(f\"💾 Backup directory: {BACKUP_DIR}\")\n",
        "print(f\"📊 Sample size: {SAMPLE_SIZE:,} texts\")\n",
        "\n",
        "print(\"\\n🚀 TO START NEW CO-DETECT ANALYSIS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. Ensure 'Munashe_Cleaned.xlsx' is in your Google Drive CoDetectBeadAnalysis folder\")\n",
        "print(\"2. Add ANTHROPIC_API_KEY to Colab secrets\")\n",
        "print(\"3. Run the following commands in order:\")\n",
        "print()\n",
        "print(\"   # Initialize the analysis\")\n",
        "print(\"   setup_codetect_analysis()\")\n",
        "print()\n",
        "print(\"   # Run the complete analysis\")\n",
        "print(\"   results = run_codetect_iteration()\")\n",
        "print()\n",
        "\n",
        "print(\"\\n🔄 TO RESTART FROM INTERRUPTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. List available backups:\")\n",
        "print(\"   list_available_backups()\")\n",
        "print()\n",
        "print(\"2. Restore from most recent backup:\")\n",
        "print(\"   restore_from_backup()\")\n",
        "print()\n",
        "print(\"3. Load your data:\")\n",
        "print(\"   sample_df = load_and_prepare_data()\")\n",
        "print()\n",
        "print(\"4. Setup API client:\")\n",
        "print(\"   client = setup_anthropic_client()\")\n",
        "print()\n",
        "print(\"5. Resume analysis:\")\n",
        "print(\"   results = resume_codetect_analysis(client, sample_df)\")\n",
        "print()\n",
        "\n",
        "print(\"\\n🔧 UTILITY FUNCTIONS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"   check_progress()                    # Check current progress\")\n",
        "print(\"   list_available_backups()            # See all saved backups\")\n",
        "print(\"   restore_from_backup()               # Restore from latest backup\")\n",
        "print(\"   restore_from_backup('filename.json') # Restore from specific backup\")\n",
        "\n",
        "print(f\"\\n🎯 Expected Results with Expert-Validated Codebook:\")\n",
        "print(f\"- Edge case reduction from ~30 (2.0%) to <15 (1.0%)\")\n",
        "print(\"- Better handling of historical generalizations\")\n",
        "print(\"- Proper recognition of intangible exchanges\")\n",
        "print(\"- Systematic data quality filtering\")\n",
        "print(f\"- Estimated cost: ~${estimate_costs(SAMPLE_SIZE):.2f} (3x cheaper with Haiku!)\")\n",
        "print(f\"- Estimated time: ~{SAMPLE_SIZE//25:.0f} minutes (3x faster with Haiku!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✨ ENHANCED FEATURES:\")\n",
        "print(\"- 💾 Automatic backup every 50 texts\")\n",
        "print(\"- 🔄 Resume from exact stopping point\")\n",
        "print(\"- 📊 Progress tracking with processed_indices\")\n",
        "print(\"- 🎯 Skip already-completed texts\")\n",
        "print(\"- 💡 Clear restore instructions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# =============================================================================\n",
        "# Example usage for restart scenario:\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "# ========================================\n",
        "# SCENARIO 1: Starting Fresh\n",
        "# ========================================\n",
        "setup_codetect_analysis()\n",
        "results = run_codetect_iteration()\n",
        "\n",
        "# ========================================\n",
        "# SCENARIO 2: Server Interrupted at text 237\n",
        "# ========================================\n",
        "\n",
        "# Step 1: Check what backups are available\n",
        "list_available_backups()\n",
        "\n",
        "# Step 2: Restore from the most recent backup\n",
        "restore_from_backup()\n",
        "\n",
        "# Step 3: Load your data\n",
        "sample_df = load_and_prepare_data()\n",
        "\n",
        "# Step 4: Setup API client\n",
        "client = setup_anthropic_client()\n",
        "\n",
        "# Step 5: Resume from where you left off\n",
        "# This will automatically skip the 237 texts already processed\n",
        "results = resume_codetect_analysis(client, sample_df)\n",
        "\n",
        "# ========================================\n",
        "# SCENARIO 3: Check Progress Anytime\n",
        "# ========================================\n",
        "check_progress()\n",
        "\n",
        "# ========================================\n",
        "# SCENARIO 4: Restore from Specific Backup\n",
        "# ========================================\n",
        "restore_from_backup('codetect_progress_250.json')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YxlOZO2ytAXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}