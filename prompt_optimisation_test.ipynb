{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WhndT_gH9Qwgcr4AhaR7vZxLfQN0CE6a",
      "authorship_tag": "ABX9TyOerhgRf9/faaOALnSt0jcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurencoetzee001/Beads_Co-detect/blob/main/prompt_optimisation_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - Test Version for Munashe_Cleaned.xlsx\n",
        "==========================================================\n",
        "\n",
        "Tests:\n",
        "- Output format and structure\n",
        "- JSON parsing and validation\n",
        "- Excel output mapping\n",
        "- Cost estimation for 27k rows\n",
        "- Error handling\n",
        "- Processing speed\n",
        "\n",
        "Single session, all 1,453 rows\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === API KEY SETUP ===\n",
        "# Option 1: Paste your API key directly here (between the quotes)\n",
        "API_KEY_DIRECT = \"\"  # Paste like: \"sk-ant-api03-YOUR_KEY_HERE\"\n",
        "\n",
        "# Option 2: Or set via environment variable before running\n",
        "# export ANTHROPIC_API_KEY=\"sk-ant-api03-YOUR_KEY_HERE\"\n",
        "\n",
        "# Get API key from either source\n",
        "ANTHROPIC_API_KEY = API_KEY_DIRECT or os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "\n",
        "# If still not set, prompt user\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"API KEY REQUIRED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nYou need an Anthropic API key to run this script.\")\n",
        "    print(\"Get one at: https://console.anthropic.com\")\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"  1. Paste it directly in the script (line 33: API_KEY_DIRECT)\")\n",
        "    print(\"  2. Set environment variable: export ANTHROPIC_API_KEY='your-key'\")\n",
        "    print(\"  3. Enter it now (will only be used for this run)\")\n",
        "    print()\n",
        "\n",
        "    user_input = input(\"Enter your API key (or press Enter to exit): \").strip()\n",
        "    if user_input:\n",
        "        ANTHROPIC_API_KEY = user_input\n",
        "        print(\"✓ API key accepted for this session\")\n",
        "    else:\n",
        "        print(\"\\nExiting. Please set your API key and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"Munashe_Cleaned.xlsx\"\n",
        "OUTPUT_DIR = \"./test_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"claude-3-5-haiku-20241022\"\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "MAX_ROWS = 1453  # All rows for test\n",
        "\n",
        "# === EFFICIENT SYSTEM PROMPT ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract structured data about bead exchanges. Apply conservative decision rules - require explicit evidence, do NOT infer.\n",
        "\n",
        "RESPONSE SCHEMA (RETURN VALID JSON ONLY - no markdown):\n",
        "{\n",
        "  \"4a_exchange\": \"xo or no\",\n",
        "  \"4b_beads_exchanged\": \"description or null\",\n",
        "  \"4c_exchanged_item\": \"what traded for or null\",\n",
        "  \"6_bead_ethnic_group\": \"groups or null\",\n",
        "  \"8_location_name\": \"location or null\",\n",
        "  \"9_place_of_manufacture\": \"where made or null\",\n",
        "  \"10_beads_observed\": \"physical description or null\",\n",
        "  \"12_local_name\": \"indigenous names or null\",\n",
        "  \"13_notes\": \"research context or null\"\n",
        "}\n",
        "\n",
        "CRITICAL RULE FOR 4a_exchange:\n",
        "- \"xo\" = ONLY explicit trade/barter transactions (e.g., \"traded 20 beads for goats\")\n",
        "- \"no\" = manufacturing, adornment, displays, ceremonies, value statements, general demand, payment systems\n",
        "\n",
        "PROCESSING RULES:\n",
        "1. Apply rules individually to each row (don't infer from context)\n",
        "2. Use null for missing information (not \"unknown\")\n",
        "3. Preserve descriptive detail for research value\n",
        "4. Return VALID JSON ONLY - no extra text\n",
        "5. Base answers ONLY on the text provided\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical excerpt:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Extract data into the JSON schema. Return valid JSON only.\"\"\"\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate required fields.\"\"\"\n",
        "    if \"4a_exchange\" not in json_obj:\n",
        "        return False, \"Missing required field: 4a_exchange\"\n",
        "\n",
        "    if json_obj[\"4a_exchange\"] not in [\"xo\", \"no\"]:\n",
        "        return False, f\"Invalid 4a_exchange: {json_obj['4a_exchange']}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with retry logic.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=1200,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku.\"\"\"\n",
        "    # Haiku pricing: $0.80 per million input, $0.24 per million output\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def extrapolate_27k_cost(cost_per_row):\n",
        "    \"\"\"Estimate cost for 27,000 rows based on test.\"\"\"\n",
        "    return cost_per_row * 27000\n",
        "\n",
        "def print_section(title):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# === MAIN TEST ===\n",
        "\n",
        "def main():\n",
        "    print_section(\"BEAD TRADE CODING - TEST VERSION\")\n",
        "    print(f\"Input file: {INPUT_FILE}\")\n",
        "    print(f\"Rows to test: {MAX_ROWS}\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "    # Check API key\n",
        "    if not ANTHROPIC_API_KEY:\n",
        "        print(\"ERROR: ANTHROPIC_API_KEY not set (this shouldn't happen)\")\n",
        "        return\n",
        "\n",
        "    # Verify API key format\n",
        "    if not ANTHROPIC_API_KEY.startswith(\"sk-ant-\"):\n",
        "        print(f\"WARNING: API key doesn't look right. Should start with 'sk-ant-'\")\n",
        "        print(f\"Current value starts with: {ANTHROPIC_API_KEY[:10]}\")\n",
        "        confirm = input(\"\\nContinue anyway? (y/n): \").strip().lower()\n",
        "        if confirm != 'y':\n",
        "            return\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading data file...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Verify text column\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"ERROR: Column '{TEXT_COLUMN}' not found\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # Initialize\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    session_start_time = time.time()\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Sample tracking for quality check\n",
        "    sample_responses = []\n",
        "    exchange_values = {\"xo\": 0, \"no\": 0}\n",
        "\n",
        "    # Process rows\n",
        "    print_section(\"PROCESSING\")\n",
        "    print(f\"Starting test on {MAX_ROWS} rows...\")\n",
        "    print(f\"Progress updates every 100 rows\\n\")\n",
        "\n",
        "    for row_idx in range(MAX_ROWS):\n",
        "        try:\n",
        "            row = df.iloc[row_idx]\n",
        "            entry_text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            # Skip empty\n",
        "            if pd.isna(entry_text) or not str(entry_text).strip():\n",
        "                responses.append(None)\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            entry_text = str(entry_text).strip()\n",
        "\n",
        "            # Call Claude\n",
        "            response = call_claude(client, entry_text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text.strip()\n",
        "\n",
        "            # Parse JSON\n",
        "            try:\n",
        "                parsed_json = json.loads(response_text)\n",
        "\n",
        "                # Validate\n",
        "                is_valid, error = validate_json_response(parsed_json)\n",
        "                if is_valid:\n",
        "                    responses.append(parsed_json)\n",
        "                    success_count += 1\n",
        "\n",
        "                    # Track exchange values\n",
        "                    exchange_val = parsed_json.get(\"4a_exchange\")\n",
        "                    if exchange_val in [\"xo\", \"no\"]:\n",
        "                        exchange_values[exchange_val] += 1\n",
        "\n",
        "                    # Save first 5 for inspection\n",
        "                    if len(sample_responses) < 5:\n",
        "                        sample_responses.append({\n",
        "                            \"row\": row_idx,\n",
        "                            \"response\": parsed_json,\n",
        "                            \"text_preview\": entry_text[:100]\n",
        "                        })\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    error_count += 1\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON parse error\"})\n",
        "                error_count += 1\n",
        "\n",
        "            # Progress\n",
        "            if (row_idx + 1) % 100 == 0:\n",
        "                elapsed = (time.time() - session_start_time) / 60\n",
        "                cost_so_far = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                rows_done = row_idx + 1\n",
        "                rate = rows_done / elapsed if elapsed > 0 else 0\n",
        "                eta_total = MAX_ROWS / rate if rate > 0 else 0\n",
        "\n",
        "                print(f\"Row {row_idx+1}/{MAX_ROWS} | Success: {success_count} | \"\n",
        "                      f\"Cost: ${cost_so_far:.2f} | Rate: {rate:.1f} rows/min | ETA: {eta_total:.0f}m\")\n",
        "\n",
        "        except Exception as e:\n",
        "            responses.append({\"error\": str(e)[:100]})\n",
        "            error_count += 1\n",
        "\n",
        "    # === ANALYSIS ===\n",
        "\n",
        "    elapsed_time = (time.time() - session_start_time) / 60\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "    cost_per_row = final_cost / (MAX_ROWS - skipped_count) if (MAX_ROWS - skipped_count) > 0 else 0\n",
        "    extrapolated_27k = extrapolate_27k_cost(cost_per_row)\n",
        "\n",
        "    print_section(\"TEST RESULTS\")\n",
        "\n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"  Total rows: {MAX_ROWS}\")\n",
        "    print(f\"  Successfully coded: {success_count}\")\n",
        "    print(f\"  Errors: {error_count}\")\n",
        "    print(f\"  Skipped (empty): {skipped_count}\")\n",
        "    print(f\"  Success rate: {(success_count/(MAX_ROWS-skipped_count)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nToken Usage:\")\n",
        "    print(f\"  Input tokens: {total_input_tokens:,}\")\n",
        "    print(f\"  Output tokens: {total_output_tokens:,}\")\n",
        "    print(f\"  Total tokens: {total_input_tokens + total_output_tokens:,}\")\n",
        "    print(f\"  Avg tokens per row: {(total_input_tokens + total_output_tokens)/(MAX_ROWS-skipped_count):.0f}\")\n",
        "\n",
        "    print(f\"\\nCost Analysis:\")\n",
        "    print(f\"  Test cost: ${final_cost:.2f}\")\n",
        "    print(f\"  Cost per row: ${cost_per_row:.4f}\")\n",
        "    print(f\"  EXTRAPOLATED 27,000 rows: ${extrapolated_27k:.2f}\")\n",
        "\n",
        "    print(f\"\\nTiming:\")\n",
        "    print(f\"  Total time: {elapsed_time:.1f} minutes\")\n",
        "    print(f\"  Rows per minute: {(MAX_ROWS-skipped_count)/elapsed_time:.1f}\")\n",
        "    print(f\"  Time per row: {(elapsed_time/(MAX_ROWS-skipped_count))*1000:.1f}ms\")\n",
        "    print(f\"  Extrapolated for 27,000 rows: {(elapsed_time/(MAX_ROWS-skipped_count))*27000/60:.1f} hours\")\n",
        "\n",
        "    print(f\"\\nCoding Distribution (4a_exchange):\")\n",
        "    print(f\"  'xo' (transactions): {exchange_values['xo']} ({(exchange_values['xo']/success_count*100):.1f}%)\")\n",
        "    print(f\"  'no' (other mentions): {exchange_values['no']} ({(exchange_values['no']/success_count*100):.1f}%)\")\n",
        "\n",
        "    # === OUTPUT FORMAT CHECK ===\n",
        "\n",
        "    print_section(\"OUTPUT FORMAT VALIDATION\")\n",
        "\n",
        "    # Check what fields are being extracted\n",
        "    all_fields = set()\n",
        "    for resp in responses:\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "            all_fields.update(resp.keys())\n",
        "\n",
        "    print(f\"\\nFields extracted: {len(all_fields)}\")\n",
        "    for field in sorted(all_fields):\n",
        "        count = sum(1 for r in responses if r and isinstance(r, dict) and field in r and r[field] is not None)\n",
        "        pct = (count / success_count * 100) if success_count > 0 else 0\n",
        "        print(f\"  {field}: {count} rows ({pct:.1f}%)\")\n",
        "\n",
        "    # === SAMPLE INSPECTION ===\n",
        "\n",
        "    print_section(\"SAMPLE RESPONSES (First 5)\")\n",
        "\n",
        "    for i, sample in enumerate(sample_responses, 1):\n",
        "        print(f\"\\nRow {sample['row']}:\")\n",
        "        print(f\"  Text: {sample['text_preview']}...\")\n",
        "        print(f\"  Response:\")\n",
        "        for key, value in sample['response'].items():\n",
        "            if value:\n",
        "                preview = str(value)[:60]\n",
        "                print(f\"    {key}: {preview}\")\n",
        "\n",
        "    # === CREATE TEST OUTPUT ===\n",
        "\n",
        "    print_section(\"SAVING OUTPUT\")\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.iloc[:len(responses)].copy()\n",
        "\n",
        "    # Add response columns\n",
        "    for i, resp in enumerate(responses):\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "            for key, value in resp.items():\n",
        "                if key not in output_df.columns:\n",
        "                    output_df[key] = None\n",
        "                output_df.at[i, key] = value\n",
        "\n",
        "    # Save output\n",
        "    output_file = os.path.join(OUTPUT_DIR, \"test_output_1453_rows.xlsx\")\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "    print(f\"\\nOutput saved: {output_file}\")\n",
        "    print(f\"  Rows: {len(output_df)}\")\n",
        "    print(f\"  Columns: {len(output_df.columns)} (original: {len(df.columns)}, new: {len(output_df.columns) - len(df.columns)})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    report_file = os.path.join(OUTPUT_DIR, \"test_report.txt\")\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(\"BEAD TRADE CODING - TEST REPORT\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "        f.write(f\"Test date: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(f\"Input file: {INPUT_FILE}\\n\")\n",
        "        f.write(f\"Rows processed: {MAX_ROWS}\\n\\n\")\n",
        "        f.write(f\"RESULTS:\\n\")\n",
        "        f.write(f\"  Success rate: {(success_count/(MAX_ROWS-skipped_count)*100):.1f}%\\n\")\n",
        "        f.write(f\"  Test cost: ${final_cost:.2f}\\n\")\n",
        "        f.write(f\"  Extrapolated 27k cost: ${extrapolated_27k:.2f}\\n\")\n",
        "        f.write(f\"  Processing time: {elapsed_time:.1f} minutes\\n\")\n",
        "        f.write(f\"  Extrapolated 27k time: {(elapsed_time/(MAX_ROWS-skipped_count))*27000/60:.1f} hours\\n\")\n",
        "        f.write(f\"  Cost per row: ${cost_per_row:.4f}\\n\")\n",
        "\n",
        "    print(f\"\\nReport saved: {report_file}\")\n",
        "\n",
        "    # === FINAL ASSESSMENT ===\n",
        "\n",
        "    print_section(\"READY FOR PRODUCTION?\")\n",
        "\n",
        "    checks = [\n",
        "        (\"Success rate > 85%\", success_count/(MAX_ROWS-skipped_count)*100 > 85),\n",
        "        (\"Fields being extracted\", len(all_fields) >= 5),\n",
        "        (\"Reasonable cost\", extrapolated_27k < 50),\n",
        "        (\"Acceptable speed\", (elapsed_time/(MAX_ROWS-skipped_count))*27000/60 < 40),\n",
        "        (\"Output file created\", os.path.exists(output_file))\n",
        "    ]\n",
        "\n",
        "    all_pass = True\n",
        "    for check_name, result in checks:\n",
        "        status = \"PASS\" if result else \"FAIL\"\n",
        "        print(f\"  [{status}] {check_name}\")\n",
        "        if not result:\n",
        "            all_pass = False\n",
        "\n",
        "    print()\n",
        "    if all_pass:\n",
        "        print(\"✓ All checks passed. Script is ready for 27,000 row production run.\")\n",
        "        print(f\"\\nExpected for full 27,000 rows:\")\n",
        "        print(f\"  Processing time: ~{(elapsed_time/(MAX_ROWS-skipped_count))*27000/60:.0f} hours (across {((elapsed_time/(MAX_ROWS-skipped_count))*27000/60)/11:.0f} sessions)\")\n",
        "        print(f\"  Total cost: ~${extrapolated_27k:.2f}\")\n",
        "        print(f\"  Success rate: ~{(success_count/(MAX_ROWS-skipped_count)*100):.0f}%\")\n",
        "    else:\n",
        "        print(\"✗ Some checks failed. Review issues before running production.\")\n",
        "\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JY4uNI078WT",
        "outputId": "c0909a76-1fca-41b7-90b7-65987aecd472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BEAD TRADE CODING - TEST VERSION\n",
            "================================================================================\n",
            "Input file: Munashe_Cleaned.xlsx\n",
            "Rows to test: 1453\n",
            "Model: claude-3-5-haiku-20241022\n",
            "\n",
            "Loading data file...\n",
            "Loaded 1453 rows, 23 columns\n",
            "\n",
            "================================================================================\n",
            "PROCESSING\n",
            "================================================================================\n",
            "Starting test on 1453 rows...\n",
            "Progress updates every 100 rows\n",
            "\n",
            "Row 100/1453 | Success: 100 | Cost: $0.07 | Rate: 19.6 rows/min | ETA: 74m\n",
            "Row 200/1453 | Success: 200 | Cost: $0.14 | Rate: 21.0 rows/min | ETA: 69m\n",
            "Row 300/1453 | Success: 300 | Cost: $0.21 | Rate: 20.8 rows/min | ETA: 70m\n",
            "Row 400/1453 | Success: 400 | Cost: $0.29 | Rate: 21.0 rows/min | ETA: 69m\n",
            "Row 500/1453 | Success: 500 | Cost: $0.40 | Rate: 20.3 rows/min | ETA: 72m\n",
            "Row 600/1453 | Success: 600 | Cost: $0.47 | Rate: 20.5 rows/min | ETA: 71m\n",
            "Row 700/1453 | Success: 699 | Cost: $0.55 | Rate: 20.6 rows/min | ETA: 71m\n",
            "Row 800/1453 | Success: 799 | Cost: $0.68 | Rate: 20.1 rows/min | ETA: 72m\n",
            "Row 900/1453 | Success: 899 | Cost: $0.75 | Rate: 19.9 rows/min | ETA: 73m\n",
            "Row 1000/1453 | Success: 999 | Cost: $0.82 | Rate: 20.1 rows/min | ETA: 72m\n",
            "Row 1100/1453 | Success: 1099 | Cost: $0.90 | Rate: 20.1 rows/min | ETA: 72m\n",
            "Row 1200/1453 | Success: 1199 | Cost: $0.97 | Rate: 20.2 rows/min | ETA: 72m\n",
            "Row 1300/1453 | Success: 1297 | Cost: $1.05 | Rate: 20.4 rows/min | ETA: 71m\n",
            "Row 1400/1453 | Success: 1397 | Cost: $1.13 | Rate: 20.3 rows/min | ETA: 71m\n",
            "\n",
            "================================================================================\n",
            "TEST RESULTS\n",
            "================================================================================\n",
            "\n",
            "Processing Summary:\n",
            "  Total rows: 1453\n",
            "  Successfully coded: 1450\n",
            "  Errors: 3\n",
            "  Skipped (empty): 0\n",
            "  Success rate: 99.8%\n",
            "\n",
            "Token Usage:\n",
            "  Input tokens: 1,421,931\n",
            "  Output tokens: 229,446\n",
            "  Total tokens: 1,651,377\n",
            "  Avg tokens per row: 1137\n",
            "\n",
            "Cost Analysis:\n",
            "  Test cost: $1.19\n",
            "  Cost per row: $0.0008\n",
            "  EXTRAPOLATED 27,000 rows: $22.16\n",
            "\n",
            "Timing:\n",
            "  Total time: 71.3 minutes\n",
            "  Rows per minute: 20.4\n",
            "  Time per row: 49.1ms\n",
            "  Extrapolated for 27,000 rows: 22.1 hours\n",
            "\n",
            "Coding Distribution (4a_exchange):\n",
            "  'xo' (transactions): 638 (44.0%)\n",
            "  'no' (other mentions): 812 (56.0%)\n",
            "\n",
            "================================================================================\n",
            "OUTPUT FORMAT VALIDATION\n",
            "================================================================================\n",
            "\n",
            "Fields extracted: 9\n",
            "  10_beads_observed: 1052 rows (72.6%)\n",
            "  12_local_name: 322 rows (22.2%)\n",
            "  13_notes: 1431 rows (98.7%)\n",
            "  4a_exchange: 1450 rows (100.0%)\n",
            "  4b_beads_exchanged: 666 rows (45.9%)\n",
            "  4c_exchanged_item: 615 rows (42.4%)\n",
            "  6_bead_ethnic_group: 880 rows (60.7%)\n",
            "  8_location_name: 1091 rows (75.2%)\n",
            "  9_place_of_manufacture: 125 rows (8.6%)\n",
            "\n",
            "================================================================================\n",
            "SAMPLE RESPONSES (First 5)\n",
            "================================================================================\n",
            "\n",
            "Row 0:\n",
            "  Text: SOKU 177 | ance of a clown. There were commonly two stiff plaits hanging down, one in front of each ...\n",
            "  Response:\n",
            "    4a_exchange: no\n",
            "    8_location_name: Soku\n",
            "    10_beads_observed: bright-coloured beads suspended from plaits and ear piercing\n",
            "    13_notes: Descriptive ethnographic text about clothing and adornment, \n",
            "\n",
            "Row 1:\n",
            "  Text: BONTUKU 231 his curious craft are nearly as simple as those of the tailor whose house we have just v...\n",
            "  Response:\n",
            "    4a_exchange: no\n",
            "    6_bead_ethnic_group: Bontuku\n",
            "    8_location_name: Bontuku\n",
            "    9_place_of_manufacture: local workshop\n",
            "    10_beads_observed: tiny, many-coloured beads used for ornamenting mats\n",
            "    13_notes: Description of glass armlet maker's workshop, using recycled\n",
            "\n",
            "Row 2:\n",
            "  Text: | | 232 TRAVELS AND LIFE IN ASHANTI AND JAMAN The first proceeding is to stir up the dull embers wit...\n",
            "  Response:\n",
            "    4a_exchange: no\n",
            "    6_bead_ethnic_group: Ashanti, Jaman, Coast tribes\n",
            "    10_beads_observed: glass beads, molten surface, adhering to armlet\n",
            "    12_local_name: Nyami, Kwesi, Quas:shie, Ekuaa sua, Akwassia, Nyankupon\n",
            "    13_notes: Detailed description of glass bead manufacturing process usi\n",
            "\n",
            "Row 3:\n",
            "  Text: BONTUKU 233 to the furnace, where the beads quickly melt down into a uniform, many-coloured mass, co...\n",
            "  Response:\n",
            "    4a_exchange: xo\n",
            "    4b_beads_exchanged: armlets made from glass, with red, blue, white colors\n",
            "    4c_exchanged_item: 20 cowrie-shells per armlet\n",
            "    6_bead_ethnic_group: Wongdras\n",
            "    8_location_name: Bontiku\n",
            "    9_place_of_manufacture: Mahama's workshop\n",
            "    10_beads_observed: glass armlets, quoit-shaped, worn in pairs above elbow, marb\n",
            "    13_notes: Manufactured from broken green glass lampshade, highly value\n",
            "\n",
            "Row 4:\n",
            "  Text: BONTUKU 237 on the stalls; although I was told that the larger business transactions are conducted a...\n",
            "  Response:\n",
            "    4a_exchange: no\n",
            "    6_bead_ethnic_group: ['Hausa', 'Sokotu', 'Kanu', 'Kachina', 'Bornus', 'Wongaras',\n",
            "    8_location_name: Bontuku\n",
            "    13_notes: Merchants sitting in market booths, fingering prayer beads, \n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "\n",
            "Output saved: ./test_output/test_output_1453_rows.xlsx\n",
            "  Rows: 1453\n",
            "  Columns: 32 (original: 23, new: 9)\n",
            "\n",
            "Report saved: ./test_output/test_report.txt\n",
            "\n",
            "================================================================================\n",
            "READY FOR PRODUCTION?\n",
            "================================================================================\n",
            "  [PASS] Success rate > 85%\n",
            "  [PASS] Fields being extracted\n",
            "  [PASS] Reasonable cost\n",
            "  [PASS] Acceptable speed\n",
            "  [PASS] Output file created\n",
            "\n",
            "✓ All checks passed. Script is ready for 27,000 row production run.\n",
            "\n",
            "Expected for full 27,000 rows:\n",
            "  Processing time: ~22 hours (across 2 sessions)\n",
            "  Total cost: ~$22.16\n",
            "  Success rate: ~100%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - COMPLETE VERSION with ALL 13 Fields\n",
        "========================================================\n",
        "\n",
        "Codes ALL 13 fields from the codebook:\n",
        "1. 1_price_HUMAN (price/exchange info)\n",
        "2. 2_size_HUMAN (size classification)\n",
        "3. 3_colour_HUMAN (color codes)\n",
        "4. 4_location_HUMAN (location type)\n",
        "5. 5_function_HUMAN (function in society)\n",
        "6. 6_origin_of_bead (geographic origin)\n",
        "7. 7_shape_HUMAN (shape)\n",
        "8. 8_type_bead_HUMAN (material)\n",
        "9. 9_local_name_HUMAN (ethnic/language name)\n",
        "10. 10_relationship_ (other exchange forms)\n",
        "11. 11_units_of_measure (measurement units)\n",
        "12. 12_bead_ethnic_ (ethnic group)\n",
        "13. 13_nature_of_exchange (consensual/conflictual)\n",
        "\n",
        "Single session, all 1,453 rows\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === API KEY SETUP ===\n",
        "API_KEY_DIRECT = \"\"  # Paste like: \"sk-ant-api03-YOUR_KEY_HERE\"\n",
        "ANTHROPIC_API_KEY = API_KEY_DIRECT or os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"API KEY REQUIRED\")\n",
        "    print(\"=\"*80)\n",
        "    user_input = input(\"Enter your API key (or press Enter to exit): \").strip()\n",
        "    if user_input:\n",
        "        ANTHROPIC_API_KEY = user_input\n",
        "        print(\"✓ API key accepted for this session\")\n",
        "    else:\n",
        "        print(\"\\nExiting. Please set your API key and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"Munashe_Cleaned.xlsx\"\n",
        "OUTPUT_DIR = \"./test_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "MAX_ROWS = 1453\n",
        "\n",
        "# === COMPREHENSIVE SYSTEM PROMPT WITH ALL 13 FIELDS ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract ALL 13 structured data fields about bead exchanges. Apply conservative rules - require explicit evidence.\n",
        "\n",
        "RESPONSE SCHEMA (RETURN VALID JSON ONLY):\n",
        "{\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": \"number or measurement if mentioned, else null\",\n",
        "    \"currency\": \"currency/commodity name if mentioned, else null\",\n",
        "    \"description\": \"full price/exchange description from text\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": [1, 2, 3, 4, 5, or 6] or null,\n",
        "    \"description\": \"exact size description from text\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [array of color codes],\n",
        "    \"description\": \"exact color description from text (especially if code=14/other)\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [array of location type codes],\n",
        "    \"names\": \"actual location names mentioned\"\n",
        "  },\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [array of function codes],\n",
        "    \"description\": \"detailed function description from text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [array of shape codes],\n",
        "    \"description\": \"exact shape description from text (include if unusual)\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [array of material codes],\n",
        "    \"description\": \"exact material description from text (especially if not standard)\"\n",
        "  },\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": \"1|2\",\n",
        "    \"names\": [\"list of names\"] or null\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [array of relationship codes],\n",
        "    \"description\": \"detailed description of exchange items from text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": code or null,\n",
        "    \"description\": \"exact measurement description from text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": code or null,\n",
        "    \"description\": \"description of exchange nature from text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context not captured above\"\n",
        "}\n",
        "\n",
        "FIELD DEFINITIONS:\n",
        "\n",
        "1_price_HUMAN (Field 28):\n",
        "- status: \"yes\" = price mentioned, \"no\" = no mention, \"xo\" = exchanged\n",
        "- amount: number of beads (5, 10, 20, 50) or body measurement (thumb to elbow)\n",
        "- currency: what was exchanged\n",
        "- description: ALWAYS include the full price/exchange text from the source\n",
        "\n",
        "2_size_HUMAN (Field 29): Classify size\n",
        "- code: 1=large, 2=medium, 3=small, 4=various, 5=thin, 6=thick\n",
        "- description: ALWAYS include exact size description from text (e.g., \"thumbnail-sized\", \"as big as a pigeon's egg\")\n",
        "\n",
        "3_colour_HUMAN (Field 30): Array of color codes + description\n",
        "- codes: 1=red, 2=blue, 3=white, 4=pink, 5=coral, 6=amber, 7=copper, 8=green\n",
        "  9=yellow, 10=transparent, 11=seed(glass), 12=black, 13=multicoloured, 14=other\n",
        "- description: ALWAYS include exact color text from source. CRITICAL for code 14 (other) or unusual colors\n",
        "\n",
        "4_location_HUMAN (Field 31): Array of location types + names\n",
        "- codes: 1=mountain/hill/peak, 2=lake, 3=river/waterfall, 4=populated place\n",
        "- names: ALWAYS include actual location names from text (e.g., \"Kumasi\", \"Niger River\")\n",
        "\n",
        "5_function_HUMAN (Field 32): Array of functions + description\n",
        "- codes: 1=jewellery/adornment/clothing, 2=currency/exchange/tax\n",
        "  3=ceremonial/religious/tribute, 4=class/status symbol/gift/social exchange\n",
        "- description: ALWAYS include detailed function text from source\n",
        "\n",
        "6_origin_of_bead (Field 33):\n",
        "- ALWAYS include full geographic origin text (e.g., \"Venetian glass beads from Murano\")\n",
        "\n",
        "7_shape_HUMAN (Field 34): Array of shape codes + description\n",
        "- codes: 1=round, 2=tubular, 3=square, 4=oval, 5=oblong, 6=punched\n",
        "  7=wound, 8=pressed, 9=decorative, 10=faceted, 11=bugle, 12=chevron\n",
        "- description: ALWAYS include exact shape text, especially for unusual shapes not in codes\n",
        "\n",
        "8_type_bead_HUMAN (Field 35): Array of material codes + description\n",
        "- codes: 1=glass/seed beads, 2=clay, 3=metal(brass/copper/silver/gold/iron)\n",
        "  4=stone(quartz/agate/carnelian/jasper/amethyst/lapis/turquoise/malachite)\n",
        "  5=coral, 6=amber, 7=bone, 8=ivory, 9=dried seed, 10=ceramic\n",
        "  11=wooden, 12=porcelain, 13=shell(seashells), 14=eggshell(ostrich)\n",
        "- description: ALWAYS include exact material text, CRITICAL for materials not in standard list\n",
        "\n",
        "9_local_name_HUMAN (Field 36):\n",
        "- exists: \"1\" = yes (provide names), \"2\" = unspecified\n",
        "- names: ALWAYS include all ethnic/language names mentioned exactly as written\n",
        "\n",
        "10_relationship_ (Field 37): Array of connected exchange forms + description\n",
        "- codes: 1=wire, 2=cloth, 3=shells(cowries), 4=coins, 5=livestock, 6=iron bars/rods\n",
        "  7=scarabs, 8=precious stones/agates, 9=antiquities/furniture, 10=ostrich feathers\n",
        "  11=ebony/ivory, 12=salt, 13=rubber/gum, 14=medicines/remedies/herbal plants\n",
        "  15=spices/essences/fragrances/perfumes/oil, 16=wax/stamps/seals\n",
        "  17=skin/leather/hides/horns/animal products, 18=indigenous weapons/spears/shields\n",
        "  19=dried food/fruits/consumables, 20=prints/artwork/books/paper/scrolls\n",
        "  21=guns/gunpowder, 22=jewellery, 23=textiles/clothing, 24=gold/silver/gold dust\n",
        "  25=slaves, 26=glass objects, 27=hardware/manufactures, 28=tobacco/snuff\n",
        "  29=musical instruments, 30=water, 31=alcohol\n",
        "- description: ALWAYS include detailed description of ALL exchange items from text\n",
        "\n",
        "11_units_of_measure (Field 38):\n",
        "- type: 1=string, 2=plaited/woven string, 3=necklace/anklet/bracelet/waist beads/headwear, 4=other\n",
        "- description: ALWAYS include exact measurement description, CRITICAL when type=4\n",
        "\n",
        "12_bead_ethnic_ (Field 39):\n",
        "- ALWAYS include all ethnic group names mentioned exactly as written\n",
        "\n",
        "13_nature_of_exchange (Field 40): Code + description\n",
        "- code: 1=consensual, 2=conflictual, 3=unspecified, 4=competitive/bartering/haggling\n",
        "  5=social(gifts/tributes), 6=uncommercial\n",
        "- description: ALWAYS include text describing the exchange nature\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Return ONLY valid JSON (no markdown, no extra text)\n",
        "2. Use null for missing information (not \"unknown\" or \"not mentioned\")\n",
        "3. For arrays, return empty array [] if no data, or null if not applicable\n",
        "4. **ALWAYS include description fields - preserve the original text verbatim**\n",
        "5. **When using \"other\" categories or unusual values, description field is MANDATORY**\n",
        "6. Base answers ONLY on provided text\n",
        "7. Apply each field's rules independently\n",
        "8. Descriptions preserve research value - never skip them\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical excerpt and extract ALL 13 fields:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return valid JSON with all 13 fields.\"\"\"\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate required fields exist.\"\"\"\n",
        "    required_fields = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required_fields if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing fields: {', '.join(missing)}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_json_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON structure for Excel output with BOTH codes and descriptions.\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1_price_HUMAN - flatten nested object\n",
        "    if json_obj.get('1_price_HUMAN'):\n",
        "        price = json_obj['1_price_HUMAN']\n",
        "        if isinstance(price, dict):\n",
        "            flat['1_price_status'] = price.get('status')\n",
        "            flat['1_price_amount'] = price.get('amount')\n",
        "            flat['1_price_currency'] = price.get('currency')\n",
        "            flat['1_price_description'] = price.get('description')\n",
        "        else:\n",
        "            flat['1_price_status'] = str(price) if price else None\n",
        "            flat['1_price_amount'] = None\n",
        "            flat['1_price_currency'] = None\n",
        "            flat['1_price_description'] = None\n",
        "    else:\n",
        "        flat['1_price_status'] = None\n",
        "        flat['1_price_amount'] = None\n",
        "        flat['1_price_currency'] = None\n",
        "        flat['1_price_description'] = None\n",
        "\n",
        "    # 2_size_HUMAN - code + description\n",
        "    size = json_obj.get('2_size_HUMAN')\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "    else:\n",
        "        flat['2_size_code'] = size  # Handle old format\n",
        "        flat['2_size_description'] = None\n",
        "\n",
        "    # 3_colour_HUMAN - codes + description\n",
        "    color = json_obj.get('3_colour_HUMAN')\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes')\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "    else:\n",
        "        flat['3_colour_codes'] = ','.join(map(str, color)) if color else None\n",
        "        flat['3_colour_description'] = None\n",
        "\n",
        "    # 4_location_HUMAN - codes + names\n",
        "    location = json_obj.get('4_location_HUMAN')\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes')\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "    else:\n",
        "        flat['4_location_codes'] = ','.join(map(str, location)) if location else None\n",
        "        flat['4_location_names'] = None\n",
        "\n",
        "    # 5_function_HUMAN - codes + description\n",
        "    function = json_obj.get('5_function_HUMAN')\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes')\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "    else:\n",
        "        flat['5_function_codes'] = ','.join(map(str, function)) if function else None\n",
        "        flat['5_function_description'] = None\n",
        "\n",
        "    # 6_origin_of_bead - text\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN - codes + description\n",
        "    shape = json_obj.get('7_shape_HUMAN')\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes')\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "    else:\n",
        "        flat['7_shape_codes'] = ','.join(map(str, shape)) if shape else None\n",
        "        flat['7_shape_description'] = None\n",
        "\n",
        "    # 8_type_bead_HUMAN - codes + description\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN')\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes')\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "    else:\n",
        "        flat['8_type_codes'] = ','.join(map(str, bead_type)) if bead_type else None\n",
        "        flat['8_type_description'] = None\n",
        "\n",
        "    # 9_local_name_HUMAN - flatten nested object\n",
        "    local_name = json_obj.get('9_local_name_HUMAN')\n",
        "    if isinstance(local_name, dict):\n",
        "        flat['9_local_name_exists'] = local_name.get('exists')\n",
        "        names = local_name.get('names')\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "    else:\n",
        "        flat['9_local_name_exists'] = None\n",
        "        flat['9_local_name_names'] = None\n",
        "\n",
        "    # 10_relationship_ - codes + description\n",
        "    relationship = json_obj.get('10_relationship_')\n",
        "    if isinstance(relationship, dict):\n",
        "        codes = relationship.get('codes')\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = relationship.get('description')\n",
        "    else:\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, relationship)) if relationship else None\n",
        "        flat['10_relationship_description'] = None\n",
        "\n",
        "    # 11_units_of_measure - flatten nested object\n",
        "    units = json_obj.get('11_units_of_measure')\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "    else:\n",
        "        flat['11_units_type'] = None\n",
        "        flat['11_units_description'] = None\n",
        "\n",
        "    # 12_bead_ethnic_ - array to semicolon-separated string\n",
        "    ethnics = json_obj.get('12_bead_ethnic_')\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange - code + description\n",
        "    nature = json_obj.get('13_nature_of_exchange')\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "    else:\n",
        "        flat['13_nature_code'] = nature\n",
        "        flat['13_nature_description'] = None\n",
        "\n",
        "    # notes - text\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with retry logic.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2000,  # Increased for more complex response\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku.\"\"\"\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def print_section(title):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# === MAIN TEST ===\n",
        "\n",
        "def main():\n",
        "    print_section(\"BEAD TRADE CODING - COMPLETE 13 FIELDS VERSION\")\n",
        "    print(f\"Input file: {INPUT_FILE}\")\n",
        "    print(f\"Rows to test: {MAX_ROWS}\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Fields: ALL 13 from codebook\")\n",
        "\n",
        "    # Verify API key format\n",
        "    if not ANTHROPIC_API_KEY.startswith(\"sk-ant-\"):\n",
        "        print(f\"WARNING: API key doesn't look right. Should start with 'sk-ant-'\")\n",
        "        confirm = input(\"\\nContinue anyway? (y/n): \").strip().lower()\n",
        "        if confirm != 'y':\n",
        "            return\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading data file...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load file: {e}\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"ERROR: Column '{TEXT_COLUMN}' not found\")\n",
        "        return\n",
        "\n",
        "    # Initialize\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    session_start_time = time.time()\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    skipped_count = 0\n",
        "    last_processed_row_idx = -1\n",
        "    last_response_text = None\n",
        "    last_error = None\n",
        "\n",
        "    # Process rows\n",
        "    print_section(\"PROCESSING\")\n",
        "    print(f\"Starting processing on {MAX_ROWS} rows...\")\n",
        "    print(f\"Progress updates every 100 rows\\n\")\n",
        "\n",
        "    try:\n",
        "        for row_idx in range(MAX_ROWS):\n",
        "            last_processed_row_idx = row_idx\n",
        "            row = df.iloc[row_idx]\n",
        "            entry_text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            # Skip empty\n",
        "            if pd.isna(entry_text) or not str(entry_text).strip():\n",
        "                responses.append(None)\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            entry_text = str(entry_text).strip()\n",
        "\n",
        "            # Call Claude\n",
        "            response = call_claude(client, entry_text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text.strip()\n",
        "            last_response_text = response_text # Store for potential error inspection\n",
        "\n",
        "            # Parse JSON\n",
        "            try:\n",
        "                parsed_json = json.loads(response_text)\n",
        "\n",
        "                # Validate\n",
        "                is_valid, error = validate_json_response(parsed_json)\n",
        "                if is_valid:\n",
        "                    # Flatten for Excel\n",
        "                    flat_json = flatten_json_for_excel(parsed_json)\n",
        "                    responses.append(flat_json)\n",
        "                    success_count += 1\n",
        "                    last_error = None # Clear error if successful\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    error_count += 1\n",
        "                    last_error = error\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON parse error\"})\n",
        "                error_count += 1\n",
        "                last_error = f\"JSON parse error: {e}\"\n",
        "\n",
        "\n",
        "            # Progress\n",
        "            if (row_idx + 1) % 100 == 0:\n",
        "                elapsed = (time.time() - session_start_time) / 60\n",
        "                cost_so_far = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                rows_done = row_idx + 1\n",
        "                rate = rows_done / elapsed if elapsed > 0 else 0\n",
        "\n",
        "                print(f\"Row {row_idx+1}/{MAX_ROWS} | Success: {success_count} | \"\n",
        "                      f\"Errors: {error_count} | Cost: ${cost_so_far:.2f} | Rate: {rate:.1f} rows/min\")\n",
        "\n",
        "        # === ANALYSIS ===\n",
        "\n",
        "        elapsed_time = (time.time() - session_start_time) / 60\n",
        "        final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "        print_section(\"TEST RESULTS\")\n",
        "\n",
        "        print(f\"\\nProcessing Summary:\")\n",
        "        print(f\"  Total rows: {MAX_ROWS}\")\n",
        "        print(f\"  Successfully coded: {success_count}\")\n",
        "        print(f\"  Errors: {error_count}\")\n",
        "        print(f\"  Skipped (empty): {skipped_count}\")\n",
        "        print(f\"  Success rate: {(success_count/(MAX_ROWS-skipped_count)*100):.1f}%\" if (MAX_ROWS-skipped_count) > 0 else \"N/A\")\n",
        "\n",
        "        print(f\"\\nToken Usage:\")\n",
        "        print(f\"  Input tokens: {total_input_tokens:,}\")\n",
        "        print(f\"  Output tokens: {total_output_tokens:,}\")\n",
        "        print(f\"  Total tokens: {total_input_tokens + total_output_tokens:,}\")\n",
        "\n",
        "        print(f\"\\nCost Analysis:\")\n",
        "        print(f\"  Test cost: ${final_cost:.2f}\")\n",
        "        print(f\"  Cost per row: ${final_cost/(MAX_ROWS-skipped_count):.4f}\" if (MAX_ROWS-skipped_count) > 0 else \"N/A\")\n",
        "\n",
        "        print(f\"\\nTiming:\")\n",
        "        print(f\"  Total time: {elapsed_time:.1f} minutes\")\n",
        "        print(f\"  Rows per minute: {(MAX_ROWS-skipped_count)/elapsed_time:.1f}\" if elapsed_time > 0 and (MAX_ROWS-skipped_count) > 0 else \"N/A\")\n",
        "\n",
        "        # === CREATE OUTPUT ===\n",
        "\n",
        "        print_section(\"SAVING OUTPUT\")\n",
        "\n",
        "        # Create output dataframe\n",
        "        output_df = df.iloc[:len(responses)].copy()\n",
        "\n",
        "        # Add response columns\n",
        "        for i, resp in enumerate(responses):\n",
        "            if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "                for key, value in resp.items():\n",
        "                    if key not in output_df.columns:\n",
        "                        output_df[key] = None\n",
        "                    output_df.at[i, key] = value\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.join(OUTPUT_DIR, \"COMPLETE_output_13_fields.xlsx\")\n",
        "        output_df.to_excel(output_file, index=False)\n",
        "\n",
        "        print(f\"\\n✓ Output saved: {output_file}\")\n",
        "        print(f\"  Rows: {len(output_df)}\")\n",
        "        print(f\"  Original columns: {len(df.columns)}\")\n",
        "        print(f\"  New coding columns: {len(output_df.columns) - len(df.columns)}\")\n",
        "        print(f\"  Total columns: {len(output_df.columns)}\")\n",
        "\n",
        "        # Show field coverage\n",
        "        print(f\"\\n13 Field Coverage:\")\n",
        "        coding_fields = [col for col in output_df.columns if col not in df.columns]\n",
        "        for field in sorted(coding_fields):\n",
        "            count = output_df[field].notna().sum()\n",
        "            pct = (count / success_count * 100) if success_count > 0 else 0\n",
        "            print(f\"  {field:30s} | {count:4d} rows ({pct:5.1f}%)\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print_section(\"PROCESS INTERRUPTED\")\n",
        "        print(f\"Processing stopped at row index: {last_processed_row_idx}\")\n",
        "        if last_response_text:\n",
        "            print(f\"Last API response (partial): {last_response_text[:500]}...\")\n",
        "        if last_error:\n",
        "            print(f\"Last recorded error: {last_error}\")\n",
        "        print(\"You can inspect the intermediate 'responses' list if needed.\")\n",
        "    except Exception as e:\n",
        "        print_section(\"AN UNEXPECTED ERROR OCCURRED\")\n",
        "        print(f\"Error at row index: {last_processed_row_idx}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        if last_response_text:\n",
        "            print(f\"Last API response (partial): {last_response_text[:500]}...\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iZp1SmTRhvA",
        "outputId": "0e9c1376-3639-419d-a3e7-4ddc0e99e52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BEAD TRADE CODING - COMPLETE 13 FIELDS VERSION\n",
            "================================================================================\n",
            "Input file: Munashe_Cleaned.xlsx\n",
            "Rows to test: 1453\n",
            "Model: claude-haiku-4-5-20251001\n",
            "Fields: ALL 13 from codebook\n",
            "\n",
            "Loading data file...\n",
            "Loaded 1453 rows, 23 columns\n",
            "\n",
            "================================================================================\n",
            "PROCESSING\n",
            "================================================================================\n",
            "Starting processing on 1453 rows...\n",
            "Progress updates every 100 rows\n",
            "\n",
            "Row 100/1453 | Success: 0 | Errors: 100 | Cost: $0.22 | Rate: 8.1 rows/min\n",
            "\n",
            "================================================================================\n",
            "PROCESS INTERRUPTED\n",
            "================================================================================\n",
            "Processing stopped at row index: 102\n",
            "Last API response (partial): ```json\n",
            "{\n",
            "  \"1_price_HUMAN\": {\n",
            "    \"status\": \"yes\",\n",
            "    \"amount\": 10,\n",
            "    \"currency\": \"large blue beads\",\n",
            "    \"description\": \"We would buy a somb for ten large blue beads their local value corresponds roughly to a calabash containing one or two liters of rice.\"\n",
            "  },\n",
            "  \"2_size_HUMAN\": {\n",
            "    \"code\": 1,\n",
            "    \"description\": \"large blue beads\"\n",
            "  },\n",
            "  \"3_colour_HUMAN\": {\n",
            "    \"codes\": [2, 2],\n",
            "    \"description\": \"ultramarine beads and cerulean blue beads; blue beads; The Gouros are sensitive to color and...\n",
            "Last recorded error: JSON parse error: Expecting value: line 1 column 1 (char 0)\n",
            "You can inspect the intermediate 'responses' list if needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la ./test_output/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cym47pFz-BT",
        "outputId": "66f7994e-836c-450b-f57b-5c5e468612a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1456\n",
            "drwxr-xr-x 2 root root    4096 Oct 16 13:24 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root    4096 Oct 16 12:13 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root 1476864 Oct 16 14:58 test_output_1453_rows.xlsx\n",
            "-rw-r--r-- 1 root root     381 Oct 16 14:58 test_report.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the test report\n",
        "cat ./test_output/test_report.txt\n",
        "\n",
        "# Check what's in the Excel file (columns and sample data)\n",
        "python3 << 'EOF'\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('./test_output/test_output_1453_rows.xlsx')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FILE INFO:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Rows: {len(df)}\")\n",
        "print(f\"Columns: {len(df.columns)}\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COLUMN NAMES:\")\n",
        "print(\"=\"*80)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"SAMPLE OF CODING COLUMNS (First 3 rows):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for error columns\n",
        "error_cols = [col for col in df.columns if 'error' in col.lower()]\n",
        "if error_cols:\n",
        "    print(f\"\\nERROR COLUMNS FOUND: {error_cols}\")\n",
        "    for col in error_cols:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(df[col].head(3))\n",
        "\n",
        "# Check for coding columns\n",
        "coding_cols = [col for col in df.columns if any(x in col for x in ['price', 'size', 'colour', 'shape', 'type'])]\n",
        "if coding_cols:\n",
        "    print(f\"\\nCODING COLUMNS FOUND: {coding_cols[:5]}\")\n",
        "    for col in coding_cols[:3]:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(df[col].head(3))\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "6RUvjy010Sl9",
        "outputId": "a96a94d8-4609-4913-a80b-486ddcf74382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-144328232.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-144328232.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cat ./test_output/test_report.txt\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - COMPLETE & FIXED Version\n",
        "=============================================\n",
        "\n",
        "All 13 fields from codebook with codes + descriptions\n",
        "Bug fixes applied for JSON parsing\n",
        "Ready for production use\n",
        "\n",
        "Features:\n",
        "- All 13 fields coded (1_price through 13_nature_of_exchange)\n",
        "- Dual structure: codes for analysis + descriptions for research\n",
        "- Robust JSON parsing (handles markdown)\n",
        "- Comprehensive error handling\n",
        "- Progress tracking and cost reporting\n",
        "\n",
        "Author: Claude\n",
        "Date: October 2025\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "print(\"Checking dependencies...\")\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === API KEY SETUP ===\n",
        "API_KEY_DIRECT = \"\"  # Option 1: Paste your key here: \"sk-ant-api03-...\"\n",
        "\n",
        "ANTHROPIC_API_KEY = API_KEY_DIRECT or os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"API KEY REQUIRED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nGet your key at: https://console.anthropic.com\")\n",
        "    user_input = input(\"\\nEnter your API key (or press Enter to exit): \").strip()\n",
        "    if user_input:\n",
        "        ANTHROPIC_API_KEY = user_input\n",
        "        print(\"✓ API key accepted\")\n",
        "    else:\n",
        "        print(\"\\nExiting. Please set your API key and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"Munashe_Cleaned.xlsx\"\n",
        "OUTPUT_DIR = \"./bead_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"  # Latest Haiku 4.5\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "MAX_ROWS = 1453  # Change to 10 for testing\n",
        "\n",
        "# === COMPREHENSIVE SYSTEM PROMPT ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract ALL 13 structured data fields. Be conservative - require explicit evidence.\n",
        "\n",
        "RESPONSE FORMAT: Return ONLY a JSON object. NO markdown, NO ```json``` tags, NO extra text.\n",
        "\n",
        "JSON STRUCTURE:\n",
        "{\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": \"number or measurement, or null\",\n",
        "    \"currency\": \"currency/commodity, or null\",\n",
        "    \"description\": \"full price text from source\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exact size text from source\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact color text, REQUIRED if code=14\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"names\": \"actual location names\"\n",
        "  },\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"description\": \"detailed function text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [array of 1-12],\n",
        "    \"description\": \"exact shape text\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact material text\"\n",
        "  },\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": \"1|2\",\n",
        "    \"names\": [\"array of names\"] or null\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [array of 1-31],\n",
        "    \"description\": \"detailed exchange items text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": 1-4 or null,\n",
        "    \"description\": \"exact measurement text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exchange nature text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context\"\n",
        "}\n",
        "\n",
        "FIELD CODES:\n",
        "\n",
        "1_price_HUMAN: status: yes=mentioned, no=not mentioned, xo=exchanged\n",
        "\n",
        "2_size_HUMAN: 1=large, 2=medium, 3=small, 4=various, 5=thin, 6=thick\n",
        "\n",
        "3_colour_HUMAN: 1=red, 2=blue, 3=white, 4=pink, 5=coral, 6=amber, 7=copper, 8=green, 9=yellow, 10=transparent, 11=seed glass, 12=black, 13=multicoloured, 14=other\n",
        "\n",
        "4_location_HUMAN: 1=mountain/hill, 2=lake, 3=river/waterfall, 4=populated place\n",
        "\n",
        "5_function_HUMAN: 1=jewellery/adornment, 2=currency/exchange, 3=ceremonial/religious, 4=status/gift\n",
        "\n",
        "6_origin_of_bead: Text describing geographic origin\n",
        "\n",
        "7_shape_HUMAN: 1=round, 2=tubular, 3=square, 4=oval, 5=oblong, 6=punched, 7=wound, 8=pressed, 9=decorative, 10=faceted, 11=bugle, 12=chevron\n",
        "\n",
        "8_type_bead_HUMAN: 1=glass, 2=clay, 3=metal, 4=stone, 5=coral, 6=amber, 7=bone, 8=ivory, 9=dried seed, 10=ceramic, 11=wooden, 12=porcelain, 13=shell, 14=eggshell\n",
        "\n",
        "9_local_name_HUMAN: exists: 1=yes (provide names), 2=unspecified\n",
        "\n",
        "10_relationship_: 1=wire, 2=cloth, 3=shells, 4=coins, 5=livestock, 6=iron bars, 7=scarabs, 8=precious stones, 9=antiquities, 10=ostrich feathers, 11=ebony/ivory, 12=salt, 13=rubber/gum, 14=medicines, 15=spices/perfumes, 16=wax/seals, 17=leather/hides, 18=weapons, 19=dried food, 20=prints/books, 21=guns/gunpowder, 22=jewellery, 23=textiles, 24=gold/silver, 25=slaves, 26=glass objects, 27=hardware, 28=tobacco, 29=musical instruments, 30=water, 31=alcohol\n",
        "\n",
        "11_units_of_measure: 1=string, 2=plaited/woven string, 3=necklace/bracelet/waist beads, 4=other\n",
        "\n",
        "12_bead_ethnic_: Array of ethnic group names\n",
        "\n",
        "13_nature_of_exchange: 1=consensual, 2=conflictual, 3=unspecified, 4=competitive/bartering, 5=social/gifts, 6=uncommercial\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Return ONLY the JSON object - NO ```json``` tags, NO markdown, NO extra text\n",
        "2. Use null for missing data (not \"unknown\")\n",
        "3. For arrays: [] if no data, null if not applicable\n",
        "4. ALWAYS include description fields with verbatim text\n",
        "5. When code=14 (other) or unusual items, description is MANDATORY\n",
        "6. Base answers ONLY on provided text\n",
        "7. Preserve exact terminology and details\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical text and extract ALL 13 fields:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY the JSON object. No ```json``` tags, no other text.\"\"\"\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def strip_markdown_json(text):\n",
        "    \"\"\"Remove markdown code blocks from JSON response.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove ```json ... ``` or ``` ... ```\n",
        "    if text.startswith('```'):\n",
        "        lines = text.split('\\n')\n",
        "        # Remove first line (```json or ```)\n",
        "        if lines[0].strip().startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        # Remove last line (```)\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        text = '\\n'.join(lines).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate all required fields exist.\"\"\"\n",
        "    required = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing fields: {', '.join(missing[:3])}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON to Excel-friendly format.\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1_price_HUMAN (4 columns)\n",
        "    price = json_obj.get('1_price_HUMAN', {})\n",
        "    if isinstance(price, dict):\n",
        "        flat['1_price_status'] = price.get('status')\n",
        "        flat['1_price_amount'] = price.get('amount')\n",
        "        flat['1_price_currency'] = price.get('currency')\n",
        "        flat['1_price_description'] = price.get('description')\n",
        "    else:\n",
        "        flat['1_price_status'] = flat['1_price_amount'] = flat['1_price_currency'] = flat['1_price_description'] = None\n",
        "\n",
        "    # 2_size_HUMAN (2 columns)\n",
        "    size = json_obj.get('2_size_HUMAN', {})\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "    else:\n",
        "        flat['2_size_code'] = size\n",
        "        flat['2_size_description'] = None\n",
        "\n",
        "    # 3_colour_HUMAN (2 columns)\n",
        "    color = json_obj.get('3_colour_HUMAN', {})\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes', [])\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "    else:\n",
        "        flat['3_colour_codes'] = ','.join(map(str, color)) if color else None\n",
        "        flat['3_colour_description'] = None\n",
        "\n",
        "    # 4_location_HUMAN (2 columns)\n",
        "    location = json_obj.get('4_location_HUMAN', {})\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes', [])\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "    else:\n",
        "        flat['4_location_codes'] = ','.join(map(str, location)) if location else None\n",
        "        flat['4_location_names'] = None\n",
        "\n",
        "    # 5_function_HUMAN (2 columns)\n",
        "    function = json_obj.get('5_function_HUMAN', {})\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes', [])\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "    else:\n",
        "        flat['5_function_codes'] = ','.join(map(str, function)) if function else None\n",
        "        flat['5_function_description'] = None\n",
        "\n",
        "    # 6_origin_of_bead (1 column)\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN (2 columns)\n",
        "    shape = json_obj.get('7_shape_HUMAN', {})\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes', [])\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "    else:\n",
        "        flat['7_shape_codes'] = ','.join(map(str, shape)) if shape else None\n",
        "        flat['7_shape_description'] = None\n",
        "\n",
        "    # 8_type_bead_HUMAN (2 columns)\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN', {})\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes', [])\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "    else:\n",
        "        flat['8_type_codes'] = ','.join(map(str, bead_type)) if bead_type else None\n",
        "        flat['8_type_description'] = None\n",
        "\n",
        "    # 9_local_name_HUMAN (2 columns)\n",
        "    local = json_obj.get('9_local_name_HUMAN', {})\n",
        "    if isinstance(local, dict):\n",
        "        flat['9_local_name_exists'] = local.get('exists')\n",
        "        names = local.get('names', [])\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "    else:\n",
        "        flat['9_local_name_exists'] = None\n",
        "        flat['9_local_name_names'] = None\n",
        "\n",
        "    # 10_relationship_ (2 columns)\n",
        "    rel = json_obj.get('10_relationship_', {})\n",
        "    if isinstance(rel, dict):\n",
        "        codes = rel.get('codes', [])\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = rel.get('description')\n",
        "    else:\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, rel)) if rel else None\n",
        "        flat['10_relationship_description'] = None\n",
        "\n",
        "    # 11_units_of_measure (2 columns)\n",
        "    units = json_obj.get('11_units_of_measure', {})\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "    else:\n",
        "        flat['11_units_type'] = None\n",
        "        flat['11_units_description'] = None\n",
        "\n",
        "    # 12_bead_ethnic_ (1 column)\n",
        "    ethnics = json_obj.get('12_bead_ethnic_', [])\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange (2 columns)\n",
        "    nature = json_obj.get('13_nature_of_exchange', {})\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "    else:\n",
        "        flat['13_nature_code'] = nature\n",
        "        flat['13_nature_description'] = None\n",
        "\n",
        "    # Notes (1 column)\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with retry logic.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2500,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku 4.5.\"\"\"\n",
        "    # Haiku 4.5 pricing (estimate - verify actual pricing)\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def print_header(title):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# === MAIN PROCESSING ===\n",
        "\n",
        "def main():\n",
        "    print_header(\"BEAD TRADE CODING - COMPLETE VERSION\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Input: {INPUT_FILE}\")\n",
        "    print(f\"Rows: {MAX_ROWS}\")\n",
        "    print(f\"All 13 fields with codes + descriptions\")\n",
        "\n",
        "    # Verify API key\n",
        "    if not ANTHROPIC_API_KEY.startswith(\"sk-ant-\"):\n",
        "        print(f\"\\n⚠️  Warning: API key format looks unusual\")\n",
        "        print(f\"   Should start with: sk-ant-\")\n",
        "        confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
        "        if confirm != 'y':\n",
        "            return\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading {INPUT_FILE}...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"✓ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading file: {e}\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"✗ Column '{TEXT_COLUMN}' not found\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # Initialize\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    start_time = time.time()\n",
        "    success = 0\n",
        "    errors = 0\n",
        "    skipped = 0\n",
        "\n",
        "    print_header(\"PROCESSING\")\n",
        "    print(f\"Starting... Progress every 100 rows\\n\")\n",
        "\n",
        "    # Process rows\n",
        "    for idx in range(MAX_ROWS):\n",
        "        try:\n",
        "            row = df.iloc[idx]\n",
        "            text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            # Skip empty\n",
        "            if pd.isna(text) or not str(text).strip():\n",
        "                responses.append(None)\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            text = str(text).strip()\n",
        "\n",
        "            # Call Claude\n",
        "            response = call_claude(client, text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            # Strip markdown and parse\n",
        "            cleaned = strip_markdown_json(response_text)\n",
        "\n",
        "            if not cleaned:\n",
        "                responses.append({\"error\": \"Empty response\"})\n",
        "                errors += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(cleaned)\n",
        "\n",
        "                # Validate\n",
        "                valid, error = validate_json_response(parsed)\n",
        "                if valid:\n",
        "                    flat = flatten_for_excel(parsed)\n",
        "                    responses.append(flat)\n",
        "                    success += 1\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    errors += 1\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON error: {str(e)[:40]}\"})\n",
        "                errors += 1\n",
        "\n",
        "                # Debug first 3 errors\n",
        "                if errors <= 3:\n",
        "                    print(f\"\\n⚠️  Parse error at row {idx}:\")\n",
        "                    print(f\"   First 150 chars: {cleaned[:150]}\")\n",
        "                    print(f\"   Error: {str(e)}\\n\")\n",
        "\n",
        "            # Progress\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                elapsed = (time.time() - start_time) / 60\n",
        "                cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
        "\n",
        "                print(f\"Row {idx+1}/{MAX_ROWS} | Success: {success} | Errors: {errors} | \"\n",
        "                      f\"Cost: ${cost:.2f} | Rate: {rate:.1f}/min\")\n",
        "\n",
        "        except Exception as e:\n",
        "            responses.append({\"error\": str(e)[:60]})\n",
        "            errors += 1\n",
        "\n",
        "    # === RESULTS ===\n",
        "\n",
        "    elapsed = (time.time() - start_time) / 60\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "    print_header(\"RESULTS\")\n",
        "\n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"  Total rows: {MAX_ROWS}\")\n",
        "    print(f\"  Successfully coded: {success}\")\n",
        "    print(f\"  Errors: {errors}\")\n",
        "    print(f\"  Skipped (empty): {skipped}\")\n",
        "    print(f\"  Success rate: {(success/(MAX_ROWS-skipped)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nToken Usage:\")\n",
        "    print(f\"  Input: {total_input_tokens:,}\")\n",
        "    print(f\"  Output: {total_output_tokens:,}\")\n",
        "    print(f\"  Total: {total_input_tokens + total_output_tokens:,}\")\n",
        "    print(f\"  Avg per row: {(total_input_tokens + total_output_tokens)/(MAX_ROWS-skipped):.0f}\")\n",
        "\n",
        "    print(f\"\\nCost:\")\n",
        "    print(f\"  Total: ${final_cost:.2f}\")\n",
        "    print(f\"  Per row: ${final_cost/(MAX_ROWS-skipped):.4f}\")\n",
        "\n",
        "    print(f\"\\nTime:\")\n",
        "    print(f\"  Duration: {elapsed:.1f} minutes\")\n",
        "    print(f\"  Rate: {(MAX_ROWS-skipped)/elapsed:.1f} rows/min\")\n",
        "\n",
        "    # === SAVE OUTPUT ===\n",
        "\n",
        "    print_header(\"SAVING OUTPUT\")\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.iloc[:len(responses)].copy()\n",
        "\n",
        "    # Add coding columns\n",
        "    for i, resp in enumerate(responses):\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "            for key, value in resp.items():\n",
        "                if key not in output_df.columns:\n",
        "                    output_df[key] = None\n",
        "                output_df.at[i, key] = value\n",
        "\n",
        "    # Save\n",
        "    output_file = os.path.join(OUTPUT_DIR, \"bead_coded_complete.xlsx\")\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"\\n✓ Output saved: {output_file}\")\n",
        "    print(f\"  Rows: {len(output_df)}\")\n",
        "    print(f\"  Original columns: {len(df.columns)}\")\n",
        "    print(f\"  New columns: {len(output_df.columns) - len(df.columns)}\")\n",
        "    print(f\"  Total columns: {len(output_df.columns)}\")\n",
        "\n",
        "    # Show field coverage\n",
        "    print(f\"\\nField Coverage (27 columns):\")\n",
        "    coding_cols = [c for c in output_df.columns if c not in df.columns and c != 'error']\n",
        "    for col in sorted(coding_cols):\n",
        "        count = output_df[col].notna().sum()\n",
        "        pct = (count / success * 100) if success > 0 else 0\n",
        "        print(f\"  {col:35s} | {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "    # Save report\n",
        "    report_file = os.path.join(OUTPUT_DIR, \"coding_report.txt\")\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(f\"Bead Trade Coding Report\\n\")\n",
        "        f.write(f\"{'='*60}\\n\\n\")\n",
        "        f.write(f\"Date: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(f\"Model: {MODEL_NAME}\\n\")\n",
        "        f.write(f\"Rows processed: {MAX_ROWS}\\n\")\n",
        "        f.write(f\"Success: {success} ({(success/(MAX_ROWS-skipped)*100):.1f}%)\\n\")\n",
        "        f.write(f\"Errors: {errors}\\n\")\n",
        "        f.write(f\"Cost: ${final_cost:.2f}\\n\")\n",
        "        f.write(f\"Time: {elapsed:.1f} minutes\\n\")\n",
        "\n",
        "    print(f\"\\n✓ Report saved: {report_file}\")\n",
        "\n",
        "    print_header(\"COMPLETE\")\n",
        "\n",
        "    if success / (MAX_ROWS - skipped) > 0.85:\n",
        "        print(\"✓ High success rate! Data ready for analysis.\")\n",
        "    else:\n",
        "        print(\"⚠️  Lower success rate. Review errors in output file.\")\n",
        "\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(f\"  1. Open: {output_file}\")\n",
        "    print(f\"  2. Verify: Check sample rows\")\n",
        "    print(f\"  3. Analyze: Use codes for stats, descriptions for context\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "casD27jW5IoI",
        "outputId": "ef71bbe4-45c5-40ca-b2d9-f6b9a197d721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking dependencies...\n",
            "\n",
            "================================================================================\n",
            "BEAD TRADE CODING - COMPLETE VERSION\n",
            "================================================================================\n",
            "Model: claude-haiku-4-5-20251001\n",
            "Input: Munashe_Cleaned.xlsx\n",
            "Rows: 1453\n",
            "All 13 fields with codes + descriptions\n",
            "\n",
            "Loading Munashe_Cleaned.xlsx...\n",
            "✓ Loaded 1453 rows, 23 columns\n",
            "\n",
            "================================================================================\n",
            "PROCESSING\n",
            "================================================================================\n",
            "Starting... Progress every 100 rows\n",
            "\n",
            "\n",
            "⚠️  Parse error at row 38:\n",
            "   First 150 chars: {\n",
            "  \"1_price_HUMAN\": {\n",
            "    \"status\": \"no\",\n",
            "    \"amount\": null,\n",
            "    \"currency\": null,\n",
            "    \"description\": null\n",
            "  },\n",
            "  \"2_size_HUMAN\": {\n",
            "    \"code\": null\n",
            "   Error: Extra data: line 39 column 4 (char 1246)\n",
            "\n",
            "\n",
            "⚠️  Parse error at row 58:\n",
            "   First 150 chars: {\n",
            "  \"1_price_HUMAN\": {\n",
            "    \"status\": \"yes\",\n",
            "    \"amount\": \"6\",\n",
            "    \"currency\": \"beads of cowries\",\n",
            "    \"description\": \"Turkeys are rare, and worth six\n",
            "   Error: Extra data: line 39 column 4 (char 1135)\n",
            "\n",
            "Row 100/1453 | Success: 98 | Errors: 2 | Cost: $0.17 | Rate: 11.5/min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - COMPLETE & FIXED Version\n",
        "=============================================\n",
        "\n",
        "All 13 fields from codebook with codes + descriptions\n",
        "Bug fixes applied for JSON parsing\n",
        "Ready for production use\n",
        "\n",
        "Features:\n",
        "- All 13 fields coded (1_price through 13_nature_of_exchange)\n",
        "- Dual structure: codes for analysis + descriptions for research\n",
        "- Robust JSON parsing (handles markdown)\n",
        "- Comprehensive error handling\n",
        "- Progress tracking and cost reporting\n",
        "\n",
        "Author: Claude\n",
        "Date: October 2025\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "print(\"Checking dependencies...\")\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === API KEY SETUP ===\n",
        "API_KEY_DIRECT = \"\"  # Option 1: Paste your key here: \"sk-ant-api03-...\"\n",
        "\n",
        "ANTHROPIC_API_KEY = API_KEY_DIRECT or os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"API KEY REQUIRED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nGet your key at: https://console.anthropic.com\")\n",
        "    user_input = input(\"\\nEnter your API key (or press Enter to exit): \").strip()\n",
        "    if user_input:\n",
        "        ANTHROPIC_API_KEY = user_input\n",
        "        print(\"✓ API key accepted\")\n",
        "    else:\n",
        "        print(\"\\nExiting. Please set your API key and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"Munashe_Cleaned.xlsx\"\n",
        "OUTPUT_DIR = \"./bead_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"  # Latest Haiku 4.5\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "MAX_ROWS = 1453  # Change to 10 for testing\n",
        "\n",
        "# === COMPREHENSIVE SYSTEM PROMPT ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract ALL 13 structured data fields. Be conservative - require explicit evidence.\n",
        "\n",
        "RESPONSE FORMAT: Return ONLY a JSON object. NO markdown, NO ```json``` tags, NO extra text.\n",
        "\n",
        "JSON STRUCTURE:\n",
        "{\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": \"number or measurement, or null\",\n",
        "    \"currency\": \"currency/commodity, or null\",\n",
        "    \"description\": \"full price text from source\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exact size text from source\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact color text, REQUIRED if code=14\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"names\": \"actual location names\"\n",
        "  },\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"description\": \"detailed function text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [array of 1-12],\n",
        "    \"description\": \"exact shape text\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact material text\"\n",
        "  },\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": \"1|2\",\n",
        "    \"names\": [\"array of names\"] or null\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [array of 1-31],\n",
        "    \"description\": \"detailed exchange items text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": 1-4 or null,\n",
        "    \"description\": \"exact measurement text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exchange nature text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context\"\n",
        "}\n",
        "\n",
        "FIELD CODES:\n",
        "\n",
        "1_price_HUMAN: status: yes=mentioned, no=not mentioned, xo=exchanged\n",
        "\n",
        "2_size_HUMAN: 1=large, 2=medium, 3=small, 4=various, 5=thin, 6=thick\n",
        "\n",
        "3_colour_HUMAN: 1=red, 2=blue, 3=white, 4=pink, 5=coral, 6=amber, 7=copper, 8=green, 9=yellow, 10=transparent, 11=seed glass, 12=black, 13=multicoloured, 14=other\n",
        "\n",
        "4_location_HUMAN: 1=mountain/hill, 2=lake, 3=river/waterfall, 4=populated place\n",
        "\n",
        "5_function_HUMAN: 1=jewellery/adornment, 2=currency/exchange, 3=ceremonial/religious, 4=status/gift\n",
        "\n",
        "6_origin_of_bead: Text describing geographic origin\n",
        "\n",
        "7_shape_HUMAN: 1=round, 2=tubular, 3=square, 4=oval, 5=oblong, 6=punched, 7=wound, 8=pressed, 9=decorative, 10=faceted, 11=bugle, 12=chevron\n",
        "\n",
        "8_type_bead_HUMAN: 1=glass, 2=clay, 3=metal, 4=stone, 5=coral, 6=amber, 7=bone, 8=ivory, 9=dried seed, 10=ceramic, 11=wooden, 12=porcelain, 13=shell, 14=eggshell\n",
        "\n",
        "9_local_name_HUMAN: exists: 1=yes (provide names), 2=unspecified\n",
        "\n",
        "10_relationship_: 1=wire, 2=cloth, 3=shells, 4=coins, 5=livestock, 6=iron bars, 7=scarabs, 8=precious stones, 9=antiquities, 10=ostrich feathers, 11=ebony/ivory, 12=salt, 13=rubber/gum, 14=medicines, 15=spices/perfumes, 16=wax/seals, 17=leather/hides, 18=weapons, 19=dried food, 20=prints/books, 21=guns/gunpowder, 22=jewellery, 23=textiles, 24=gold/silver, 25=slaves, 26=glass objects, 27=hardware, 28=tobacco, 29=musical instruments, 30=water, 31=alcohol\n",
        "\n",
        "11_units_of_measure: 1=string, 2=plaited/woven string, 3=necklace/bracelet/waist beads, 4=other\n",
        "\n",
        "12_bead_ethnic_: Array of ethnic group names\n",
        "\n",
        "13_nature_of_exchange: 1=consensual, 2=conflictual, 3=unspecified, 4=competitive/bartering, 5=social/gifts, 6=uncommercial\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Return ONLY the JSON object - NO ```json``` tags, NO markdown, NO extra text\n",
        "2. Use null for missing data (not \"unknown\")\n",
        "3. For arrays: [] if no data, null if not applicable\n",
        "4. ALWAYS include description fields with verbatim text\n",
        "5. When code=14 (other) or unusual items, description is MANDATORY\n",
        "6. Base answers ONLY on provided text\n",
        "7. Preserve exact terminology and details\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical text and extract ALL 13 fields:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY the JSON object. No ```json``` tags, no other text.\"\"\"\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def strip_markdown_json(text):\n",
        "    \"\"\"Remove markdown code blocks and extract only the first complete JSON object.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove ```json ... ``` or ``` ... ```\n",
        "    if text.startswith('```'):\n",
        "        lines = text.split('\\n')\n",
        "        # Remove first line (```json or ```)\n",
        "        if lines[0].strip().startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        # Remove last line (```)\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        text = '\\n'.join(lines).strip()\n",
        "\n",
        "    # Extract only the first complete JSON object\n",
        "    # Find the first { and its matching }\n",
        "    if not text.startswith('{'):\n",
        "        # Try to find where JSON starts\n",
        "        start = text.find('{')\n",
        "        if start == -1:\n",
        "            return text  # No JSON found\n",
        "        text = text[start:]\n",
        "\n",
        "    # Find the matching closing brace\n",
        "    brace_count = 0\n",
        "    in_string = False\n",
        "    escape_next = False\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if escape_next:\n",
        "            escape_next = False\n",
        "            continue\n",
        "\n",
        "        if char == '\\\\':\n",
        "            escape_next = True\n",
        "            continue\n",
        "\n",
        "        if char == '\"':\n",
        "            in_string = not in_string\n",
        "            continue\n",
        "\n",
        "        if not in_string:\n",
        "            if char == '{':\n",
        "                brace_count += 1\n",
        "            elif char == '}':\n",
        "                brace_count -= 1\n",
        "                if brace_count == 0:\n",
        "                    # Found the matching closing brace\n",
        "                    return text[:i+1]\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate all required fields exist.\"\"\"\n",
        "    required = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing fields: {', '.join(missing[:3])}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON to Excel-friendly format.\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1_price_HUMAN (4 columns)\n",
        "    price = json_obj.get('1_price_HUMAN', {})\n",
        "    if isinstance(price, dict):\n",
        "        flat['1_price_status'] = price.get('status')\n",
        "        flat['1_price_amount'] = price.get('amount')\n",
        "        flat['1_price_currency'] = price.get('currency')\n",
        "        flat['1_price_description'] = price.get('description')\n",
        "    else:\n",
        "        flat['1_price_status'] = flat['1_price_amount'] = flat['1_price_currency'] = flat['1_price_description'] = None\n",
        "\n",
        "    # 2_size_HUMAN (2 columns)\n",
        "    size = json_obj.get('2_size_HUMAN', {})\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "    else:\n",
        "        flat['2_size_code'] = size\n",
        "        flat['2_size_description'] = None\n",
        "\n",
        "    # 3_colour_HUMAN (2 columns)\n",
        "    color = json_obj.get('3_colour_HUMAN', {})\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes', [])\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "    else:\n",
        "        flat['3_colour_codes'] = ','.join(map(str, color)) if color else None\n",
        "        flat['3_colour_description'] = None\n",
        "\n",
        "    # 4_location_HUMAN (2 columns)\n",
        "    location = json_obj.get('4_location_HUMAN', {})\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes', [])\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "    else:\n",
        "        flat['4_location_codes'] = ','.join(map(str, location)) if location else None\n",
        "        flat['4_location_names'] = None\n",
        "\n",
        "    # 5_function_HUMAN (2 columns)\n",
        "    function = json_obj.get('5_function_HUMAN', {})\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes', [])\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "    else:\n",
        "        flat['5_function_codes'] = ','.join(map(str, function)) if function else None\n",
        "        flat['5_function_description'] = None\n",
        "\n",
        "    # 6_origin_of_bead (1 column)\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN (2 columns)\n",
        "    shape = json_obj.get('7_shape_HUMAN', {})\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes', [])\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "    else:\n",
        "        flat['7_shape_codes'] = ','.join(map(str, shape)) if shape else None\n",
        "        flat['7_shape_description'] = None\n",
        "\n",
        "    # 8_type_bead_HUMAN (2 columns)\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN', {})\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes', [])\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "    else:\n",
        "        flat['8_type_codes'] = ','.join(map(str, bead_type)) if bead_type else None\n",
        "        flat['8_type_description'] = None\n",
        "\n",
        "    # 9_local_name_HUMAN (2 columns)\n",
        "    local = json_obj.get('9_local_name_HUMAN', {})\n",
        "    if isinstance(local, dict):\n",
        "        flat['9_local_name_exists'] = local.get('exists')\n",
        "        names = local.get('names', [])\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "    else:\n",
        "        flat['9_local_name_exists'] = None\n",
        "        flat['9_local_name_names'] = None\n",
        "\n",
        "    # 10_relationship_ (2 columns)\n",
        "    rel = json_obj.get('10_relationship_', {})\n",
        "    if isinstance(rel, dict):\n",
        "        codes = rel.get('codes', [])\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = rel.get('description')\n",
        "    else:\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, rel)) if rel else None\n",
        "        flat['10_relationship_description'] = None\n",
        "\n",
        "    # 11_units_of_measure (2 columns)\n",
        "    units = json_obj.get('11_units_of_measure', {})\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "    else:\n",
        "        flat['11_units_type'] = None\n",
        "        flat['11_units_description'] = None\n",
        "\n",
        "    # 12_bead_ethnic_ (1 column)\n",
        "    ethnics = json_obj.get('12_bead_ethnic_', [])\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange (2 columns)\n",
        "    nature = json_obj.get('13_nature_of_exchange', {})\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "    else:\n",
        "        flat['13_nature_code'] = nature\n",
        "        flat['13_nature_description'] = None\n",
        "\n",
        "    # Notes (1 column)\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with retry logic.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2500,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku 4.5.\"\"\"\n",
        "    # Haiku 4.5 pricing (estimate - verify actual pricing)\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def print_header(title):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# === MAIN PROCESSING ===\n",
        "\n",
        "def main():\n",
        "    print_header(\"BEAD TRADE CODING - COMPLETE VERSION\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Input: {INPUT_FILE}\")\n",
        "    print(f\"Rows: {MAX_ROWS}\")\n",
        "    print(f\"All 13 fields with codes + descriptions\")\n",
        "\n",
        "    # Verify API key\n",
        "    if not ANTHROPIC_API_KEY.startswith(\"sk-ant-\"):\n",
        "        print(f\"\\n⚠️  Warning: API key format looks unusual\")\n",
        "        print(f\"   Should start with: sk-ant-\")\n",
        "        confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
        "        if confirm != 'y':\n",
        "            return\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading {INPUT_FILE}...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"✓ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading file: {e}\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"✗ Column '{TEXT_COLUMN}' not found\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # Initialize\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    start_time = time.time()\n",
        "    success = 0\n",
        "    errors = 0\n",
        "    skipped = 0\n",
        "\n",
        "    print_header(\"PROCESSING\")\n",
        "    print(f\"Starting... Progress every 100 rows\\n\")\n",
        "\n",
        "    # Process rows\n",
        "    for idx in range(MAX_ROWS):\n",
        "        try:\n",
        "            row = df.iloc[idx]\n",
        "            text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            # Skip empty\n",
        "            if pd.isna(text) or not str(text).strip():\n",
        "                responses.append(None)\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            text = str(text).strip()\n",
        "\n",
        "            # Call Claude\n",
        "            response = call_claude(client, text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            # Strip markdown and parse\n",
        "            cleaned = strip_markdown_json(response_text)\n",
        "\n",
        "            if not cleaned:\n",
        "                responses.append({\"error\": \"Empty response\"})\n",
        "                errors += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(cleaned)\n",
        "\n",
        "                # Validate\n",
        "                valid, error = validate_json_response(parsed)\n",
        "                if valid:\n",
        "                    flat = flatten_for_excel(parsed)\n",
        "                    responses.append(flat)\n",
        "                    success += 1\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    errors += 1\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON error: {str(e)[:40]}\"})\n",
        "                errors += 1\n",
        "\n",
        "                # Debug first 3 errors\n",
        "                if errors <= 3:\n",
        "                    print(f\"\\n⚠️  Parse error at row {idx}:\")\n",
        "                    print(f\"   First 150 chars: {cleaned[:150]}\")\n",
        "                    print(f\"   Error: {str(e)}\\n\")\n",
        "\n",
        "            # Progress\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                elapsed = (time.time() - start_time) / 60\n",
        "                cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
        "\n",
        "                print(f\"Row {idx+1}/{MAX_ROWS} | Success: {success} | Errors: {errors} | \"\n",
        "                      f\"Cost: ${cost:.2f} | Rate: {rate:.1f}/min\")\n",
        "\n",
        "        except Exception as e:\n",
        "            responses.append({\"error\": str(e)[:60]})\n",
        "            errors += 1\n",
        "\n",
        "    # === RESULTS ===\n",
        "\n",
        "    elapsed = (time.time() - start_time) / 60\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "    print_header(\"RESULTS\")\n",
        "\n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"  Total rows: {MAX_ROWS}\")\n",
        "    print(f\"  Successfully coded: {success}\")\n",
        "    print(f\"  Errors: {errors}\")\n",
        "    print(f\"  Skipped (empty): {skipped}\")\n",
        "    print(f\"  Success rate: {(success/(MAX_ROWS-skipped)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nToken Usage:\")\n",
        "    print(f\"  Input: {total_input_tokens:,}\")\n",
        "    print(f\"  Output: {total_output_tokens:,}\")\n",
        "    print(f\"  Total: {total_input_tokens + total_output_tokens:,}\")\n",
        "    print(f\"  Avg per row: {(total_input_tokens + total_output_tokens)/(MAX_ROWS-skipped):.0f}\")\n",
        "\n",
        "    print(f\"\\nCost:\")\n",
        "    print(f\"  Total: ${final_cost:.2f}\")\n",
        "    print(f\"  Per row: ${final_cost/(MAX_ROWS-skipped):.4f}\")\n",
        "\n",
        "    print(f\"\\nTime:\")\n",
        "    print(f\"  Duration: {elapsed:.1f} minutes\")\n",
        "    print(f\"  Rate: {(MAX_ROWS-skipped)/elapsed:.1f} rows/min\")\n",
        "\n",
        "    # === SAVE OUTPUT ===\n",
        "\n",
        "    print_header(\"SAVING OUTPUT\")\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.iloc[:len(responses)].copy()\n",
        "\n",
        "    # Add coding columns\n",
        "    for i, resp in enumerate(responses):\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "            for key, value in resp.items():\n",
        "                if key not in output_df.columns:\n",
        "                    output_df[key] = None\n",
        "                output_df.at[i, key] = value\n",
        "\n",
        "    # Save\n",
        "    output_file = os.path.join(OUTPUT_DIR, \"bead_coded_complete.xlsx\")\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"\\n✓ Output saved: {output_file}\")\n",
        "    print(f\"  Rows: {len(output_df)}\")\n",
        "    print(f\"  Original columns: {len(df.columns)}\")\n",
        "    print(f\"  New columns: {len(output_df.columns) - len(df.columns)}\")\n",
        "    print(f\"  Total columns: {len(output_df.columns)}\")\n",
        "\n",
        "    # Show field coverage\n",
        "    print(f\"\\nField Coverage (27 columns):\")\n",
        "    coding_cols = [c for c in output_df.columns if c not in df.columns and c != 'error']\n",
        "    for col in sorted(coding_cols):\n",
        "        count = output_df[col].notna().sum()\n",
        "        pct = (count / success * 100) if success > 0 else 0\n",
        "        print(f\"  {col:35s} | {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "    # Save report\n",
        "    report_file = os.path.join(OUTPUT_DIR, \"coding_report.txt\")\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(f\"Bead Trade Coding Report\\n\")\n",
        "        f.write(f\"{'='*60}\\n\\n\")\n",
        "        f.write(f\"Date: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(f\"Model: {MODEL_NAME}\\n\")\n",
        "        f.write(f\"Rows processed: {MAX_ROWS}\\n\")\n",
        "        f.write(f\"Success: {success} ({(success/(MAX_ROWS-skipped)*100):.1f}%)\\n\")\n",
        "        f.write(f\"Errors: {errors}\\n\")\n",
        "        f.write(f\"Cost: ${final_cost:.2f}\\n\")\n",
        "        f.write(f\"Time: {elapsed:.1f} minutes\\n\")\n",
        "\n",
        "    print(f\"\\n✓ Report saved: {report_file}\")\n",
        "\n",
        "    print_header(\"COMPLETE\")\n",
        "\n",
        "    if success / (MAX_ROWS - skipped) > 0.85:\n",
        "        print(\"✓ High success rate! Data ready for analysis.\")\n",
        "    else:\n",
        "        print(\"⚠️  Lower success rate. Review errors in output file.\")\n",
        "\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(f\"  1. Open: {output_file}\")\n",
        "    print(f\"  2. Verify: Check sample rows\")\n",
        "    print(f\"  3. Analyze: Use codes for stats, descriptions for context\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "c2cXIWdN7Vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Bead Trade Coding - COMPLETE & FIXED Version\n",
        "=============================================\n",
        "\n",
        "All 13 fields from codebook with codes + descriptions\n",
        "Bug fixes applied for JSON parsing\n",
        "Ready for production use\n",
        "\n",
        "Features:\n",
        "- All 13 fields coded (1_price through 13_nature_of_exchange)\n",
        "- Dual structure: codes for analysis + descriptions for research\n",
        "- Robust JSON parsing (handles markdown)\n",
        "- Comprehensive error handling\n",
        "- Progress tracking and cost reporting\n",
        "\n",
        "Author: Claude\n",
        "Date: October 2025\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# === INSTALL DEPENDENCIES ===\n",
        "print(\"Checking dependencies...\")\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"pandas\", \"tenacity\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# === API KEY SETUP ===\n",
        "API_KEY_DIRECT = \"\"  # Option 1: Paste your key here: \"sk-ant-api03-...\"\n",
        "\n",
        "ANTHROPIC_API_KEY = API_KEY_DIRECT or os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"API KEY REQUIRED\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nGet your key at: https://console.anthropic.com\")\n",
        "    user_input = input(\"\\nEnter your API key (or press Enter to exit): \").strip()\n",
        "    if user_input:\n",
        "        ANTHROPIC_API_KEY = user_input\n",
        "        print(\"✓ API key accepted\")\n",
        "    else:\n",
        "        print(\"\\nExiting. Please set your API key and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "INPUT_FILE = \"Munashe_Cleaned.xlsx\"\n",
        "OUTPUT_DIR = \"./bead_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"claude-haiku-4-5-20251001\"  # Latest Haiku 4.5\n",
        "TEXT_COLUMN = \"text_page_gp\"\n",
        "MAX_ROWS = 1453  # Change to 10 for testing\n",
        "\n",
        "# === COMPREHENSIVE SYSTEM PROMPT ===\n",
        "SYSTEM_PROMPT = \"\"\"You are a historian analyzing pre-colonial African bead trade records.\n",
        "\n",
        "TASK: Extract ALL 13 structured data fields. Be conservative - require explicit evidence.\n",
        "\n",
        "RESPONSE FORMAT: Return ONLY a JSON object. NO markdown, NO ```json``` tags, NO extra text.\n",
        "\n",
        "JSON STRUCTURE:\n",
        "{\n",
        "  \"1_price_HUMAN\": {\n",
        "    \"status\": \"yes|no|xo\",\n",
        "    \"amount\": \"number or measurement, or null\",\n",
        "    \"currency\": \"currency/commodity, or null\",\n",
        "    \"description\": \"full price text from source\"\n",
        "  },\n",
        "  \"2_size_HUMAN\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exact size text from source\"\n",
        "  },\n",
        "  \"3_colour_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact color text, REQUIRED if code=14\"\n",
        "  },\n",
        "  \"4_location_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"names\": \"actual location names\"\n",
        "  },\n",
        "  \"5_function_HUMAN\": {\n",
        "    \"codes\": [array of 1-4],\n",
        "    \"description\": \"detailed function text\"\n",
        "  },\n",
        "  \"6_origin_of_bead\": \"geographic origin text or null\",\n",
        "  \"7_shape_HUMAN\": {\n",
        "    \"codes\": [array of 1-12],\n",
        "    \"description\": \"exact shape text\"\n",
        "  },\n",
        "  \"8_type_bead_HUMAN\": {\n",
        "    \"codes\": [array of 1-14],\n",
        "    \"description\": \"exact material text\"\n",
        "  },\n",
        "  \"9_local_name_HUMAN\": {\n",
        "    \"exists\": \"1|2\",\n",
        "    \"names\": [\"array of names\"] or null\n",
        "  },\n",
        "  \"10_relationship_\": {\n",
        "    \"codes\": [array of 1-31],\n",
        "    \"description\": \"detailed exchange items text\"\n",
        "  },\n",
        "  \"11_units_of_measure\": {\n",
        "    \"type\": 1-4 or null,\n",
        "    \"description\": \"exact measurement text\"\n",
        "  },\n",
        "  \"12_bead_ethnic_\": [\"array of ethnic group names\"] or null,\n",
        "  \"13_nature_of_exchange\": {\n",
        "    \"code\": 1-6 or null,\n",
        "    \"description\": \"exchange nature text\"\n",
        "  },\n",
        "  \"notes\": \"additional research context\"\n",
        "}\n",
        "\n",
        "FIELD CODES:\n",
        "\n",
        "1_price_HUMAN: status: yes=mentioned, no=not mentioned, xo=exchanged\n",
        "\n",
        "2_size_HUMAN: 1=large, 2=medium, 3=small, 4=various, 5=thin, 6=thick\n",
        "\n",
        "3_colour_HUMAN: 1=red, 2=blue, 3=white, 4=pink, 5=coral, 6=amber, 7=copper, 8=green, 9=yellow, 10=transparent, 11=seed glass, 12=black, 13=multicoloured, 14=other\n",
        "\n",
        "4_location_HUMAN: 1=mountain/hill, 2=lake, 3=river/waterfall, 4=populated place\n",
        "\n",
        "5_function_HUMAN: 1=jewellery/adornment, 2=currency/exchange, 3=ceremonial/religious, 4=status/gift\n",
        "\n",
        "6_origin_of_bead: Text describing geographic origin\n",
        "\n",
        "7_shape_HUMAN: 1=round, 2=tubular, 3=square, 4=oval, 5=oblong, 6=punched, 7=wound, 8=pressed, 9=decorative, 10=faceted, 11=bugle, 12=chevron\n",
        "\n",
        "8_type_bead_HUMAN: 1=glass, 2=clay, 3=metal, 4=stone, 5=coral, 6=amber, 7=bone, 8=ivory, 9=dried seed, 10=ceramic, 11=wooden, 12=porcelain, 13=shell, 14=eggshell\n",
        "\n",
        "9_local_name_HUMAN: exists: 1=yes (provide names), 2=unspecified\n",
        "\n",
        "10_relationship_: 1=wire, 2=cloth, 3=shells, 4=coins, 5=livestock, 6=iron bars, 7=scarabs, 8=precious stones, 9=antiquities, 10=ostrich feathers, 11=ebony/ivory, 12=salt, 13=rubber/gum, 14=medicines, 15=spices/perfumes, 16=wax/seals, 17=leather/hides, 18=weapons, 19=dried food, 20=prints/books, 21=guns/gunpowder, 22=jewellery, 23=textiles, 24=gold/silver, 25=slaves, 26=glass objects, 27=hardware, 28=tobacco, 29=musical instruments, 30=water, 31=alcohol\n",
        "\n",
        "11_units_of_measure: 1=string, 2=plaited/woven string, 3=necklace/bracelet/waist beads, 4=other\n",
        "\n",
        "12_bead_ethnic_: Array of ethnic group names\n",
        "\n",
        "13_nature_of_exchange: 1=consensual, 2=conflictual, 3=unspecified, 4=competitive/bartering, 5=social/gifts, 6=uncommercial\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Return ONLY the JSON object - NO ```json``` tags, NO markdown, NO extra text\n",
        "2. Use null for missing data (not \"unknown\")\n",
        "3. For arrays: [] if no data, null if not applicable\n",
        "4. ALWAYS include description fields with verbatim text\n",
        "5. When code=14 (other) or unusual items, description is MANDATORY\n",
        "6. Base answers ONLY on provided text\n",
        "7. Preserve exact terminology and details\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Analyze this historical text and extract ALL 13 fields:\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY the JSON object. No ```json``` tags, no other text.\"\"\"\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def strip_markdown_json(text):\n",
        "    \"\"\"Remove markdown code blocks and extract only the first complete JSON object.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove ```json ... ``` or ``` ... ```\n",
        "    if text.startswith('```'):\n",
        "        lines = text.split('\\n')\n",
        "        # Remove first line (```json or ```)\n",
        "        if lines[0].strip().startswith('```'):\n",
        "            lines = lines[1:]\n",
        "        # Remove last line (```)\n",
        "        if lines and lines[-1].strip() == '```':\n",
        "            lines = lines[:-1]\n",
        "        text = '\\n'.join(lines).strip()\n",
        "\n",
        "    # Extract only the first complete JSON object\n",
        "    # Find the first { and its matching }\n",
        "    if not text.startswith('{'):\n",
        "        # Try to find where JSON starts\n",
        "        start = text.find('{')\n",
        "        if start == -1:\n",
        "            return text  # No JSON found\n",
        "        text = text[start:]\n",
        "\n",
        "    # Find the matching closing brace\n",
        "    brace_count = 0\n",
        "    in_string = False\n",
        "    escape_next = False\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if escape_next:\n",
        "            escape_next = False\n",
        "            continue\n",
        "\n",
        "        if char == '\\\\':\n",
        "            escape_next = True\n",
        "            continue\n",
        "\n",
        "        if char == '\"':\n",
        "            in_string = not in_string\n",
        "            continue\n",
        "\n",
        "        if not in_string:\n",
        "            if char == '{':\n",
        "                brace_count += 1\n",
        "            elif char == '}':\n",
        "                brace_count -= 1\n",
        "                if brace_count == 0:\n",
        "                    # Found the matching closing brace\n",
        "                    return text[:i+1]\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_json_response(json_obj):\n",
        "    \"\"\"Validate all required fields exist.\"\"\"\n",
        "    required = [\n",
        "        '1_price_HUMAN', '2_size_HUMAN', '3_colour_HUMAN', '4_location_HUMAN',\n",
        "        '5_function_HUMAN', '6_origin_of_bead', '7_shape_HUMAN', '8_type_bead_HUMAN',\n",
        "        '9_local_name_HUMAN', '10_relationship_', '11_units_of_measure',\n",
        "        '12_bead_ethnic_', '13_nature_of_exchange'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required if f not in json_obj]\n",
        "    if missing:\n",
        "        return False, f\"Missing fields: {', '.join(missing[:3])}\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def flatten_for_excel(json_obj):\n",
        "    \"\"\"Flatten nested JSON to Excel-friendly format.\"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1_price_HUMAN (4 columns)\n",
        "    price = json_obj.get('1_price_HUMAN', {})\n",
        "    if isinstance(price, dict):\n",
        "        flat['1_price_status'] = price.get('status')\n",
        "        flat['1_price_amount'] = price.get('amount')\n",
        "        flat['1_price_currency'] = price.get('currency')\n",
        "        flat['1_price_description'] = price.get('description')\n",
        "    else:\n",
        "        flat['1_price_status'] = flat['1_price_amount'] = flat['1_price_currency'] = flat['1_price_description'] = None\n",
        "\n",
        "    # 2_size_HUMAN (2 columns)\n",
        "    size = json_obj.get('2_size_HUMAN', {})\n",
        "    if isinstance(size, dict):\n",
        "        flat['2_size_code'] = size.get('code')\n",
        "        flat['2_size_description'] = size.get('description')\n",
        "    else:\n",
        "        flat['2_size_code'] = size\n",
        "        flat['2_size_description'] = None\n",
        "\n",
        "    # 3_colour_HUMAN (2 columns)\n",
        "    color = json_obj.get('3_colour_HUMAN', {})\n",
        "    if isinstance(color, dict):\n",
        "        codes = color.get('codes', [])\n",
        "        flat['3_colour_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['3_colour_description'] = color.get('description')\n",
        "    else:\n",
        "        flat['3_colour_codes'] = ','.join(map(str, color)) if color else None\n",
        "        flat['3_colour_description'] = None\n",
        "\n",
        "    # 4_location_HUMAN (2 columns)\n",
        "    location = json_obj.get('4_location_HUMAN', {})\n",
        "    if isinstance(location, dict):\n",
        "        codes = location.get('codes', [])\n",
        "        flat['4_location_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['4_location_names'] = location.get('names')\n",
        "    else:\n",
        "        flat['4_location_codes'] = ','.join(map(str, location)) if location else None\n",
        "        flat['4_location_names'] = None\n",
        "\n",
        "    # 5_function_HUMAN (2 columns)\n",
        "    function = json_obj.get('5_function_HUMAN', {})\n",
        "    if isinstance(function, dict):\n",
        "        codes = function.get('codes', [])\n",
        "        flat['5_function_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['5_function_description'] = function.get('description')\n",
        "    else:\n",
        "        flat['5_function_codes'] = ','.join(map(str, function)) if function else None\n",
        "        flat['5_function_description'] = None\n",
        "\n",
        "    # 6_origin_of_bead (1 column)\n",
        "    flat['6_origin_of_bead'] = json_obj.get('6_origin_of_bead')\n",
        "\n",
        "    # 7_shape_HUMAN (2 columns)\n",
        "    shape = json_obj.get('7_shape_HUMAN', {})\n",
        "    if isinstance(shape, dict):\n",
        "        codes = shape.get('codes', [])\n",
        "        flat['7_shape_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['7_shape_description'] = shape.get('description')\n",
        "    else:\n",
        "        flat['7_shape_codes'] = ','.join(map(str, shape)) if shape else None\n",
        "        flat['7_shape_description'] = None\n",
        "\n",
        "    # 8_type_bead_HUMAN (2 columns)\n",
        "    bead_type = json_obj.get('8_type_bead_HUMAN', {})\n",
        "    if isinstance(bead_type, dict):\n",
        "        codes = bead_type.get('codes', [])\n",
        "        flat['8_type_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['8_type_description'] = bead_type.get('description')\n",
        "    else:\n",
        "        flat['8_type_codes'] = ','.join(map(str, bead_type)) if bead_type else None\n",
        "        flat['8_type_description'] = None\n",
        "\n",
        "    # 9_local_name_HUMAN (2 columns)\n",
        "    local = json_obj.get('9_local_name_HUMAN', {})\n",
        "    if isinstance(local, dict):\n",
        "        flat['9_local_name_exists'] = local.get('exists')\n",
        "        names = local.get('names', [])\n",
        "        flat['9_local_name_names'] = '; '.join(names) if names else None\n",
        "    else:\n",
        "        flat['9_local_name_exists'] = None\n",
        "        flat['9_local_name_names'] = None\n",
        "\n",
        "    # 10_relationship_ (2 columns)\n",
        "    rel = json_obj.get('10_relationship_', {})\n",
        "    if isinstance(rel, dict):\n",
        "        codes = rel.get('codes', [])\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, codes)) if codes else None\n",
        "        flat['10_relationship_description'] = rel.get('description')\n",
        "    else:\n",
        "        flat['10_relationship_codes'] = ','.join(map(str, rel)) if rel else None\n",
        "        flat['10_relationship_description'] = None\n",
        "\n",
        "    # 11_units_of_measure (2 columns)\n",
        "    units = json_obj.get('11_units_of_measure', {})\n",
        "    if isinstance(units, dict):\n",
        "        flat['11_units_type'] = units.get('type')\n",
        "        flat['11_units_description'] = units.get('description')\n",
        "    else:\n",
        "        flat['11_units_type'] = None\n",
        "        flat['11_units_description'] = None\n",
        "\n",
        "    # 12_bead_ethnic_ (1 column)\n",
        "    ethnics = json_obj.get('12_bead_ethnic_', [])\n",
        "    flat['12_bead_ethnic_'] = '; '.join(ethnics) if ethnics else None\n",
        "\n",
        "    # 13_nature_of_exchange (2 columns)\n",
        "    nature = json_obj.get('13_nature_of_exchange', {})\n",
        "    if isinstance(nature, dict):\n",
        "        flat['13_nature_code'] = nature.get('code')\n",
        "        flat['13_nature_description'] = nature.get('description')\n",
        "    else:\n",
        "        flat['13_nature_code'] = nature\n",
        "        flat['13_nature_description'] = None\n",
        "\n",
        "    # Notes (1 column)\n",
        "    flat['notes'] = json_obj.get('notes')\n",
        "\n",
        "    return flat\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=1, max=10),\n",
        "    reraise=True\n",
        ")\n",
        "def call_claude(client, entry_text):\n",
        "    \"\"\"Call Claude API with retry logic.\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2500,\n",
        "        temperature=0,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT_TEMPLATE.format(text=entry_text)\n",
        "        }]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost for Haiku 4.5.\"\"\"\n",
        "    # Haiku 4.5 pricing (estimate - verify actual pricing)\n",
        "    input_cost = (input_tokens / 1_000_000) * 0.80\n",
        "    output_cost = (output_tokens / 1_000_000) * 0.24\n",
        "    return input_cost + output_cost\n",
        "\n",
        "def print_header(title):\n",
        "    \"\"\"Print formatted section header.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# === MAIN PROCESSING ===\n",
        "\n",
        "def main():\n",
        "    print_header(\"BEAD TRADE CODING - COMPLETE VERSION\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Input: {INPUT_FILE}\")\n",
        "    print(f\"Rows: {MAX_ROWS}\")\n",
        "    print(f\"All 13 fields with codes + descriptions\")\n",
        "\n",
        "    # Verify API key\n",
        "    if not ANTHROPIC_API_KEY.startswith(\"sk-ant-\"):\n",
        "        print(f\"\\n⚠️  Warning: API key format looks unusual\")\n",
        "        print(f\"   Should start with: sk-ant-\")\n",
        "        confirm = input(\"Continue anyway? (y/n): \").strip().lower()\n",
        "        if confirm != 'y':\n",
        "            return\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading {INPUT_FILE}...\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_FILE)\n",
        "        print(f\"✓ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading file: {e}\")\n",
        "        return\n",
        "\n",
        "    if TEXT_COLUMN not in df.columns:\n",
        "        print(f\"✗ Column '{TEXT_COLUMN}' not found\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        return\n",
        "\n",
        "    # Initialize\n",
        "    client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "    responses = []\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    start_time = time.time()\n",
        "    success = 0\n",
        "    errors = 0\n",
        "    skipped = 0\n",
        "\n",
        "    print_header(\"PROCESSING\")\n",
        "    print(f\"Starting... Progress every 100 rows\\n\")\n",
        "\n",
        "    # Process rows\n",
        "    for idx in range(MAX_ROWS):\n",
        "        try:\n",
        "            row = df.iloc[idx]\n",
        "            text = row.get(TEXT_COLUMN)\n",
        "\n",
        "            # Skip empty\n",
        "            if pd.isna(text) or not str(text).strip():\n",
        "                responses.append(None)\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            text = str(text).strip()\n",
        "\n",
        "            # Call Claude\n",
        "            response = call_claude(client, text)\n",
        "            total_input_tokens += response.usage.input_tokens\n",
        "            total_output_tokens += response.usage.output_tokens\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            # Strip markdown and parse\n",
        "            cleaned = strip_markdown_json(response_text)\n",
        "\n",
        "            if not cleaned:\n",
        "                responses.append({\"error\": \"Empty response\"})\n",
        "                errors += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                parsed = json.loads(cleaned)\n",
        "\n",
        "                # Validate\n",
        "                valid, error = validate_json_response(parsed)\n",
        "                if valid:\n",
        "                    flat = flatten_for_excel(parsed)\n",
        "                    responses.append(flat)\n",
        "                    success += 1\n",
        "                else:\n",
        "                    responses.append({\"error\": error})\n",
        "                    errors += 1\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                responses.append({\"error\": f\"JSON error: {str(e)[:40]}\"})\n",
        "                errors += 1\n",
        "\n",
        "                # Debug first 3 errors\n",
        "                if errors <= 3:\n",
        "                    print(f\"\\n⚠️  Parse error at row {idx}:\")\n",
        "                    print(f\"   First 150 chars: {cleaned[:150]}\")\n",
        "                    print(f\"   Error: {str(e)}\\n\")\n",
        "\n",
        "            # Progress\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                elapsed = (time.time() - start_time) / 60\n",
        "                cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
        "\n",
        "                print(f\"Row {idx+1}/{MAX_ROWS} | Success: {success} | Errors: {errors} | \"\n",
        "                      f\"Cost: ${cost:.2f} | Rate: {rate:.1f}/min\")\n",
        "\n",
        "        except Exception as e:\n",
        "            responses.append({\"error\": str(e)[:60]})\n",
        "            errors += 1\n",
        "\n",
        "    # === RESULTS ===\n",
        "\n",
        "    elapsed = (time.time() - start_time) / 60\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "\n",
        "    print_header(\"RESULTS\")\n",
        "\n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"  Total rows: {MAX_ROWS}\")\n",
        "    print(f\"  Successfully coded: {success}\")\n",
        "    print(f\"  Errors: {errors}\")\n",
        "    print(f\"  Skipped (empty): {skipped}\")\n",
        "    print(f\"  Success rate: {(success/(MAX_ROWS-skipped)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nToken Usage:\")\n",
        "    print(f\"  Input: {total_input_tokens:,}\")\n",
        "    print(f\"  Output: {total_output_tokens:,}\")\n",
        "    print(f\"  Total: {total_input_tokens + total_output_tokens:,}\")\n",
        "    print(f\"  Avg per row: {(total_input_tokens + total_output_tokens)/(MAX_ROWS-skipped):.0f}\")\n",
        "\n",
        "    print(f\"\\nCost:\")\n",
        "    print(f\"  Total: ${final_cost:.2f}\")\n",
        "    print(f\"  Per row: ${final_cost/(MAX_ROWS-skipped):.4f}\")\n",
        "\n",
        "    print(f\"\\nTime:\")\n",
        "    print(f\"  Duration: {elapsed:.1f} minutes\")\n",
        "    print(f\"  Rate: {(MAX_ROWS-skipped)/elapsed:.1f} rows/min\")\n",
        "\n",
        "    # === SAVE OUTPUT ===\n",
        "\n",
        "    print_header(\"SAVING OUTPUT\")\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.iloc[:len(responses)].copy()\n",
        "\n",
        "    # Add coding columns\n",
        "    for i, resp in enumerate(responses):\n",
        "        if resp and isinstance(resp, dict) and \"error\" not in resp:\n",
        "            for key, value in resp.items():\n",
        "                if key not in output_df.columns:\n",
        "                    output_df[key] = None\n",
        "                output_df.at[i, key] = value\n",
        "\n",
        "    # Save\n",
        "    output_file = os.path.join(OUTPUT_DIR, \"bead_coded_complete.xlsx\")\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f\"\\n✓ Output saved: {output_file}\")\n",
        "    print(f\"  Rows: {len(output_df)}\")\n",
        "    print(f\"  Original columns: {len(df.columns)}\")\n",
        "    print(f\"  New columns: {len(output_df.columns) - len(df.columns)}\")\n",
        "    print(f\"  Total columns: {len(output_df.columns)}\")\n",
        "\n",
        "    # Show field coverage\n",
        "    print(f\"\\nField Coverage (27 columns):\")\n",
        "    coding_cols = [c for c in output_df.columns if c not in df.columns and c != 'error']\n",
        "    for col in sorted(coding_cols):\n",
        "        count = output_df[col].notna().sum()\n",
        "        pct = (count / success * 100) if success > 0 else 0\n",
        "        print(f\"  {col:35s} | {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "    # Save report\n",
        "    report_file = os.path.join(OUTPUT_DIR, \"coding_report.txt\")\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(f\"Bead Trade Coding Report\\n\")\n",
        "        f.write(f\"{'='*60}\\n\\n\")\n",
        "        f.write(f\"Date: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(f\"Model: {MODEL_NAME}\\n\")\n",
        "        f.write(f\"Rows processed: {MAX_ROWS}\\n\")\n",
        "        f.write(f\"Success: {success} ({(success/(MAX_ROWS-skipped)*100):.1f}%)\\n\")\n",
        "        f.write(f\"Errors: {errors}\\n\")\n",
        "        f.write(f\"Cost: ${final_cost:.2f}\\n\")\n",
        "        f.write(f\"Time: {elapsed:.1f} minutes\\n\")\n",
        "\n",
        "    print(f\"\\n✓ Report saved: {report_file}\")\n",
        "\n",
        "    print_header(\"COMPLETE\")\n",
        "\n",
        "    if success / (MAX_ROWS - skipped) > 0.85:\n",
        "        print(\"✓ High success rate! Data ready for analysis.\")\n",
        "    else:\n",
        "        print(\"⚠️  Lower success rate. Review errors in output file.\")\n",
        "\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(f\"  1. Open: {output_file}\")\n",
        "    print(f\"  2. Verify: Check sample rows\")\n",
        "    print(f\"  3. Analyze: Use codes for stats, descriptions for context\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHa1-AuPqc5x",
        "outputId": "ee5266b6-3da7-47d8-b389-2f90be9b126d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking dependencies...\n",
            "Installing anthropic...\n",
            "\n",
            "================================================================================\n",
            "BEAD TRADE CODING - COMPLETE VERSION\n",
            "================================================================================\n",
            "Model: claude-haiku-4-5-20251001\n",
            "Input: Munashe_Cleaned.xlsx\n",
            "Rows: 1453\n",
            "All 13 fields with codes + descriptions\n",
            "\n",
            "Loading Munashe_Cleaned.xlsx...\n",
            "✓ Loaded 1453 rows, 23 columns\n",
            "\n",
            "================================================================================\n",
            "PROCESSING\n",
            "================================================================================\n",
            "Starting... Progress every 100 rows\n",
            "\n",
            "Row 100/1453 | Success: 98 | Errors: 2 | Cost: $0.17 | Rate: 11.5/min\n",
            "Row 200/1453 | Success: 198 | Errors: 2 | Cost: $0.33 | Rate: 12.0/min\n",
            "Row 300/1453 | Success: 298 | Errors: 2 | Cost: $0.50 | Rate: 11.8/min\n",
            "Row 400/1453 | Success: 396 | Errors: 4 | Cost: $0.67 | Rate: 11.8/min\n",
            "Row 500/1453 | Success: 495 | Errors: 5 | Cost: $0.88 | Rate: 11.7/min\n",
            "Row 600/1453 | Success: 595 | Errors: 5 | Cost: $1.04 | Rate: 11.8/min\n",
            "Row 700/1453 | Success: 695 | Errors: 5 | Cost: $1.22 | Rate: 11.9/min\n",
            "Row 800/1453 | Success: 795 | Errors: 5 | Cost: $1.44 | Rate: 11.5/min\n",
            "Row 900/1453 | Success: 895 | Errors: 5 | Cost: $1.61 | Rate: 11.5/min\n",
            "Row 1000/1453 | Success: 995 | Errors: 5 | Cost: $1.78 | Rate: 11.6/min\n",
            "Row 1100/1453 | Success: 1094 | Errors: 6 | Cost: $1.95 | Rate: 11.6/min\n",
            "Row 1200/1453 | Success: 1190 | Errors: 10 | Cost: $2.12 | Rate: 11.6/min\n",
            "Row 1300/1453 | Success: 1290 | Errors: 10 | Cost: $2.29 | Rate: 11.6/min\n",
            "Row 1400/1453 | Success: 1390 | Errors: 10 | Cost: $2.47 | Rate: 11.7/min\n",
            "\n",
            "================================================================================\n",
            "RESULTS\n",
            "================================================================================\n",
            "\n",
            "Processing Summary:\n",
            "  Total rows: 1453\n",
            "  Successfully coded: 1443\n",
            "  Errors: 10\n",
            "  Skipped (empty): 0\n",
            "  Success rate: 99.3%\n",
            "\n",
            "Token Usage:\n",
            "  Input: 2,981,000\n",
            "  Output: 804,853\n",
            "  Total: 3,785,853\n",
            "  Avg per row: 2606\n",
            "\n",
            "Cost:\n",
            "  Total: $2.58\n",
            "  Per row: $0.0018\n",
            "\n",
            "Time:\n",
            "  Duration: 124.7 minutes\n",
            "  Rate: 11.7 rows/min\n",
            "\n",
            "================================================================================\n",
            "SAVING OUTPUT\n",
            "================================================================================\n",
            "\n",
            "✓ Output saved: ./bead_output/bead_coded_complete.xlsx\n",
            "  Rows: 1453\n",
            "  Original columns: 23\n",
            "  New columns: 27\n",
            "  Total columns: 50\n",
            "\n",
            "Field Coverage (27 columns):\n",
            "  10_relationship_codes               | 1168 ( 80.9%)\n",
            "  10_relationship_description         | 1154 ( 80.0%)\n",
            "  11_units_description                |  687 ( 47.6%)\n",
            "  11_units_type                       |  456 ( 31.6%)\n",
            "  12_bead_ethnic_                     |  741 ( 51.4%)\n",
            "  13_nature_code                      | 1133 ( 78.5%)\n",
            "  13_nature_description               | 1133 ( 78.5%)\n",
            "  1_price_amount                      |  274 ( 19.0%)\n",
            "  1_price_currency                    |  578 ( 40.1%)\n",
            "  1_price_description                 |  615 ( 42.6%)\n",
            "  1_price_status                      | 1443 (100.0%)\n",
            "  2_size_code                         |  267 ( 18.5%)\n",
            "  2_size_description                  |  322 ( 22.3%)\n",
            "  3_colour_codes                      |  736 ( 51.0%)\n",
            "  3_colour_description                |  740 ( 51.3%)\n",
            "  4_location_codes                    | 1347 ( 93.3%)\n",
            "  4_location_names                    | 1347 ( 93.3%)\n",
            "  5_function_codes                    | 1315 ( 91.1%)\n",
            "  5_function_description              | 1315 ( 91.1%)\n",
            "  6_origin_of_bead                    |  406 ( 28.1%)\n",
            "  7_shape_codes                       |  316 ( 21.9%)\n",
            "  7_shape_description                 |  317 ( 22.0%)\n",
            "  8_type_codes                        | 1114 ( 77.2%)\n",
            "  8_type_description                  | 1142 ( 79.1%)\n",
            "  9_local_name_exists                 | 1443 (100.0%)\n",
            "  9_local_name_names                  |  182 ( 12.6%)\n",
            "  notes                               | 1443 (100.0%)\n",
            "\n",
            "✓ Report saved: ./bead_output/coding_report.txt\n",
            "\n",
            "================================================================================\n",
            "COMPLETE\n",
            "================================================================================\n",
            "✓ High success rate! Data ready for analysis.\n",
            "\n",
            "Next steps:\n",
            "  1. Open: ./bead_output/bead_coded_complete.xlsx\n",
            "  2. Verify: Check sample rows\n",
            "  3. Analyze: Use codes for stats, descriptions for context\n",
            "\n"
          ]
        }
      ]
    }
  ]
}