# -*- coding: utf-8 -*-
"""Copy of all_entries_research_focused.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gl7QuhOWtu7vIwazmEnBH-R0fonwRCa5
"""

# enhanced_bead_coding_notebook.py
"""
Enhanced Bead Trade Coding Notebook - Research Focused
======================================================

This notebook incorporates all the research-focused improvements:
- Enhanced exchange field logic (no vs xo)
- Detailed bead characteristic preservation
- Trading partner and route information capture
- Geographic context preservation
- Auto-save every 50 rows with resume capability
"""

import subprocess
import sys

# === INSTALL DEPENDENCIES IF NEEDED ===
def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

for pkg in ["anthropic", "openpyxl", "xlrd", "pandas"]:
    try:
        __import__(pkg if pkg != "openpyxl" else "openpyxl")
    except ImportError:
        install(pkg)

import pandas as pd
from anthropic import Anthropic
import json
import datetime
import os

# === GOOGLE DRIVE MOUNT ===
from google.colab import drive

drive.mount('/content/drive')
timestamp_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
GDRIVE_DIR = f"/content/drive/MyDrive/bead_annotation/research_focused_session_{timestamp_str}"
os.makedirs(GDRIVE_DIR, exist_ok=True)

print("üî¨ RESEARCH-FOCUSED BEAD TRADE CODING")
print("=" * 50)
print("üéØ Goal: Preserve ALL detail for trade route reconstruction")
print("üéØ Goal: Capture rich bead characteristics")
print("üéØ Goal: Preserve trading partner information")
print("=" * 50)

# === SETTINGS ===
EXCEL_PATH = "All_entries_beads_cleaned.xlsx"  # Changed back to All_entries file
ANTHROPIC_API_KEY = "your-anthropic-api-key-here"  # Replace with your actual API key
MODEL_NAME = "claude-3-haiku-20240307"  # Or use "claude-3-opus-20240229" for higher quality
LOG_FILE = "research_annotation_log.txt"
SAVE_EVERY = 50  # Auto-save frequency

# === ENHANCED RESEARCH-FOCUSED CODEBOOK ===
RESEARCH_CODEBOOK = """
ENHANCED BEAD TRADE CODING CODEBOOK - RESEARCH FOCUSED
======================================================

You are analyzing historical texts to reconstruct pre-colonial African trade networks and bead characteristics.
PRESERVE ALL DESCRIPTIVE DETAIL for research analysis.

### 4a. Exchange (code="4a_exchange") - ENHANCED
**Question**: Were beads actually exchanged in a specific transaction?

**Values**:
- `no`: No mention of specific exchange transaction (exchange did not take place, hearsay, descriptions, value statements, wearing, manufacturing, displays, general demand, ceremonial use, payment systems)
- `xo`: Exchange transaction explicitly described (beads traded for specific goods/services in a documented transaction)

**CRITICAL DECISION RULES**:
- Use `xo` ONLY when text explicitly describes beads being traded, bartered, sold, or exchanged for something else in a specific transaction
- Use `no` for ALL other bead mentions including:
  * Manufacturing/processing ("beads melt down", "fashioned from glass")
  * Value descriptions ("worth more than gold", "highly prized")
  * Personal adornment ("wore strings of beads", "around necks")
  * Market displays ("beads displayed in stalls", "exposed for sale")
  * General demand ("beads in great demand", "scarce items")
  * Ceremonial use ("offerings to gods", "ritual objects")
  * Payment systems ("teaspoonful per day", "wages in beads")
  * Physical descriptions ("bead-like eyes", metaphors)

**Examples of `no`**:
- "beads melt down into a uniform mass" ‚Üí Manufacturing
- "worth more than their weight in gold" ‚Üí Value description
- "wore strings of red beads around necks" ‚Üí Adornment
- "beads displayed in market calabashes" ‚Üí Display
- "beads in great demand" ‚Üí General demand
- "teaspoonful of beads per day" ‚Üí Payment system

**Examples of `xo`**:
- "A slave-boy could be purchased for fifty strings of beads"
- "traded twenty red beads for two goats"
- "exchanged glass beads for ivory tusks"
- "gave coral beads in return for cotton cloth"

### 4b. Beads Exchanged (code="4b_beads_exchanged") - RESEARCH CRITICAL
**Question**: What specific beads were exchanged, with complete physical and quantity details?

**ENHANCED FORMAT**: Extract EVERY detail for research analysis:
- **Physical characteristics**: Material (glass, coral, amber, porcelain), color (specific shades), size (with measurements), shape, weight, texture
- **Quantities**: Exact amounts, measurement units (strings, fundos, khetes), body measurements (arm's length, thumb to elbow)
- **Quality indicators**: Expensive, cheap, fine, coarse, prized, ceremonial, royal
- **Manufacturing details**: Origin, craftsmanship, local vs imported
- **Cultural context**: Purpose, status significance, ceremonial use

**Research-Quality Examples**:
- "A fondo of large white porcelain beads with blue eyes, each worth more than its weight in gold, manufactured in China"
- "Twenty strings of red Venetian glass beads, two fathoms in length, highly prized by Akan nobility"
- "Handful of tiny amber-colored tubular beads, arm's length measurement, used in royal ceremonies"

### 6. Bead Ethnic Group (code="6_bead_ethnic_group") - TRADE ROUTE ANALYSIS
**Question**: Which ethnic groups/traders were associated with beads, with complete context for trade route reconstruction?

**RESEARCH FORMAT**: Preserve ALL trade route information:
```
Primary Groups: [List specific ethnic groups with origins]
Trader Types: [Merchant categories, social status, specializations]
Geographic Origins: [Trading centers, departure points, territories]
Trade Network Context: [Relationships, hierarchies, intermediaries, route information]
Cultural Context: [Social status, cultural practices, historical background]
```

**Research-Quality Example**:
Input: "The booths are exclusively occupied by Mahommedan merchants, Hausas from Sokotu, Kanu and Kachina, Bornus from Kuka, Wong√°ras, principally from Kong, and Fulas from various towns of the interior."

Output: "Primary Groups: Hausas from Sokotu, Kanu and Kachina; Bornus from Kuka; Wong√°ras from Kong; Fulas from interior towns | Trader Types: Well-to-do Mahommedan merchants | Geographic Origins: Sokotu, Kuka, Kong, various interior towns | Trade Network Context: Hierarchical system with wealthy merchants using booth intermediaries"

### 8. Location Name (code="8_location_name") - MAPPING CRITICAL
**Question**: What is the complete location with all geographic context for mapping and trade route analysis?

**ENHANCED INSTRUCTIONS**: Include EVERY piece of location information:
- Primary place name with original spelling
- Geographic descriptors (near, situated, on coast, by river, in mountains)
- Directional context (north side, southern district, eastern territory)
- Regional information (kingdom, territory, district, chiefdom names)
- Distance/travel information if provided
- Modern equivalents if mentioned
- Strategic importance (port, crossroads, market center)

**Research-Quality Examples**:
- "Bont√∫ku (near Kumasi), major trading center in Akan territory, three days journey from coast"
- "Evyango, situated on the north side of the marsh district near Abomey, accessible by river during high water"
- "Salaga, the great market town in the northern territories, meeting point for Hausa and coastal traders"

### 10. Beads Observed (code="10_beads_observed") - COMPREHENSIVE DESCRIPTION
**Question**: What complete physical description of beads is provided for bead characteristic analysis?

**ENHANCED INSTRUCTIONS**: Extract EVERY physical detail:
- **Materials**: Glass, coral, amber, porcelain, ceramic, metal, bone, wood, crystal
- **Colors**: Specific shades (canary yellow, deep blue, bright red), patterns
- **Sizes**: Measurements, comparative sizes, proportions
- **Shapes**: Round, tubular, flat, faceted, cylindrical, oval, carved
- **Manufacturing**: Craftsmanship quality, origin indicators, techniques
- **Cultural significance**: Status indicators, ceremonial vs everyday use

### 12. Local Name (code="12_local_name") - CULTURAL ANALYSIS
**Question**: What are local/indigenous names with complete cultural context?

**ENHANCED FORMAT**: Capture full terminology:
- Indigenous/local name with original spelling
- Meaning or translation if provided
- Cultural significance (ceremonial, status, religious)
- Regional variations, associated practices
- Linguistic group if mentioned

**Examples**:
- "Damba (royal ceremonial beads used in Akan coronation rituals)"
- "Popoe beads (named after trading settlement, highly valued by coastal peoples)"
- "Khete (measurement unit for bead strings, equivalent to arm's length)"

### Additional Research Fields:
- **4c_exchanged_item**: What beads were exchanged FOR (preserve full context)
- **9_place_of_manufacture**: Complete manufacturing context (not just city names)
- **13_notes**: ALL additional research-relevant details

**CRITICAL RESEARCH PRINCIPLE**:
Preserve ALL descriptive detail. Rich, detailed information is more valuable than simplified extraction. Every detail about materials, colors, quantities, origins, and trading partners is potentially crucial for historical research. Apply this codebook to every row individually rather than drawing inferences from what you have read from other rows in the dataset.
"""

# === STEP 1: Load Excel File ===
print(f"\nüìÅ Loading Excel file: {EXCEL_PATH}")
try:
    df = pd.read_excel(EXCEL_PATH, engine="openpyxl")
    print(f"‚úÖ Successfully loaded Excel file: {len(df)} rows, {len(df.columns)} columns")
except Exception as e:
    print("‚ùå Failed to load Excel file as .xlsx. Trying alternatives...")
    try:
        df = pd.read_excel(EXCEL_PATH, engine="xlrd")
        print(f"‚úÖ Loaded as .xls: {len(df)} rows")
    except:
        try:
            df = pd.read_csv(EXCEL_PATH)
            print(f"‚úÖ Loaded as CSV: {len(df)} rows")
        except Exception as e3:
            print(f"‚ùå Could not load file: {e3}")
            raise

# Display sample data for verification - FIXED SECTION
print(f"\nüîç Sample data preview:")
text_columns = [col for col in df.columns if 'text' in col.lower() or 'highlight' in col.lower()]
if text_columns:
    text_col = text_columns[0]
    print(f"Text column found: {text_col}")

    # FIXED: Handle NaN/empty values properly
    # Find first non-null value in the text column
    non_null_texts = df[text_col].dropna()

    if len(non_null_texts) > 0:
        # Get the first non-null text
        first_valid_text = str(non_null_texts.iloc[0])
        sample_length = min(200, len(first_valid_text))
        sample = first_valid_text[:sample_length]
        if len(first_valid_text) > 200:
            sample += "..."
        print(f"Sample (from row {non_null_texts.index[0]}): {sample}")
        print(f"Total non-empty entries: {len(non_null_texts)}/{len(df)}")
    else:
        print("‚ö†Ô∏è No non-empty text entries found in the column!")
else:
    print("Available columns:", list(df.columns[:10]))
    print("‚ö†Ô∏è No text/highlight column found. Please check column names.")

# === STEP 2: Set Up Anthropic Client ===
client = Anthropic(api_key=ANTHROPIC_API_KEY)

# === STEP 3: Enhanced Prompt Template ===
def construct_research_prompt(entry_text):
    return f"""You are a historian and data specialist analyzing historical records of pre-colonial African bead trade. Your analysis will be used for trade route reconstruction and bead characteristic research.

{RESEARCH_CODEBOOK}

TASK:
Analyze the following historical excerpt and extract ALL relevant details. Preserve descriptive information for research value.

EXCERPT:
{entry_text}

IMPORTANT:
- Respond ONLY with valid JSON using double quotes
- Preserve ALL descriptive detail in your responses
- Include geographic context, bead characteristics, and trading partner information
- Use the enhanced field definitions above
- Omit keys only when there is absolutely no relevant information

JSON Response:"""

# === STEP 4: Resume Functionality ===
def get_resume_index():
    """Check for previous session and offer resume option."""
    index_path = os.path.join(GDRIVE_DIR, 'last_index.txt')
    try:
        if os.path.exists(index_path):
            with open(index_path, 'r') as f:
                last_index = int(f.read().strip())
            print(f"\nüîÑ Previous session detected!")
            print(f"Last processed row: {last_index}")
            resume = input(f"Resume from row {last_index + 1}? (y/n): ").strip().lower()
            if resume == 'y':
                return last_index + 1
    except:
        pass

    print("üîÅ Starting from beginning")
    return 0

# === STEP 5: Main Processing Loop with Research Focus ===
def process_entries_with_research_focus():
    """Process entries with research-focused enhancements."""

    total_input_tokens = 0
    total_output_tokens = 0
    responses = []
    start_index = get_resume_index()

    # Determine text column
    text_column = None
    possible_columns = ['text_page_gp', 'highlight', 'text', 'content']
    for col in possible_columns:
        if col in df.columns:
            text_column = col
            break

    if not text_column:
        print("‚ùå Could not find text column. Available columns:")
        print(list(df.columns))
        return responses, 0, 0  # Return empty results instead of None

    print(f"\nüî¨ STARTING RESEARCH-FOCUSED PROCESSING")
    print(f"Text column: {text_column}")
    print(f"Starting from row: {start_index}")
    print(f"Processing up to row: {min(2000, len(df))}")
    print(f"Auto-save every: {SAVE_EVERY} rows")

    log_path = os.path.join(GDRIVE_DIR, LOG_FILE)

    # Pre-fill responses list if resuming
    if start_index > 0:
        responses = [None] * start_index
        print(f"üìã Pre-filled {start_index} placeholder responses")

    with open(log_path, "a", encoding="utf-8") as log:
        log.write(f"\n=== Research-focused annotation started at {datetime.datetime.now().isoformat()} ===\n")
        log.write(f"Starting from row {start_index}\n")

        for i in range(start_index, min(2000, len(df))):
            try:
                row = df.iloc[i]
                entry_text = row.get(text_column)

                # FIXED: Check for NaN/None values properly
                if pd.isna(entry_text) or entry_text is None:
                    responses.append(None)
                    print(f"‚è≠Ô∏è  Row {i}: Empty text, skipping")
                    continue

                # Convert to string and strip
                entry_text = str(entry_text).strip()

                if not entry_text or entry_text == 'nan':
                    responses.append(None)
                    print(f"‚è≠Ô∏è  Row {i}: Empty text, skipping")
                    continue

                # Construct research-focused prompt
                prompt = construct_research_prompt(entry_text)
                timestamp = datetime.datetime.now().isoformat()

                print(f"\nüîç Processing row {i}/{min(2000, len(df))}")
                print(f"üìù Text sample: {entry_text[:100]}...")

                # Call Claude API
                response = client.messages.create(
                    model=MODEL_NAME,
                    max_tokens=2000,  # Increased for detailed responses
                    messages=[{"role": "user", "content": prompt}]
                )

                total_input_tokens += response.usage.input_tokens
                total_output_tokens += response.usage.output_tokens
                response_text = response.content[0].text.strip()

                print(f"ü§ñ Claude response preview: {response_text[:150]}...")

                # Validate JSON
                try:
                    parsed_json = json.loads(response_text)
                    responses.append(parsed_json)
                    log.write(f"[{timestamp}] Row {i}: Success - {len(parsed_json)} fields\n")
                    print(f"‚úÖ Row {i}: Successfully parsed JSON with {len(parsed_json)} fields")

                except json.JSONDecodeError as e:
                    log.write(f"[{timestamp}] Row {i}: JSON decode error - {str(e)}\n")
                    print(f"‚ùå Row {i}: JSON decode error, saving raw response")
                    responses.append({"raw_response": response_text, "error": "json_decode_error"})

                # Auto-save functionality
                if (i + 1) % SAVE_EVERY == 0:
                    save_intermediate_results(i, responses, total_input_tokens, total_output_tokens)

                # Cost tracking
                current_cost = total_input_tokens * 0.00025 + total_output_tokens * 0.00125
                if i % 10 == 0:  # Show cost every 10 rows
                    print(f"üí∞ Current cost: ${current_cost:.4f} | Tokens: {total_input_tokens}+{total_output_tokens}")

            except Exception as e:
                timestamp = datetime.datetime.now().isoformat()
                error_msg = f"[{timestamp}] Row {i}: Error - {str(e)}\n"
                log.write(error_msg)
                print(f"‚ùå {error_msg.strip()}")
                responses.append({"error": str(e), "row": i})

        log.write(f"=== Research-focused annotation completed at {datetime.datetime.now().isoformat()} ===\n")

    return responses, total_input_tokens, total_output_tokens

def save_intermediate_results(current_row, responses, input_tokens, output_tokens):
    """Save intermediate results with research metadata."""

    print(f"\nüíæ SAVING INTERMEDIATE RESULTS (Row {current_row})")

    # Save current index
    with open(os.path.join(GDRIVE_DIR, 'last_index.txt'), 'w') as f:
        f.write(str(current_row))

    # Create results dataframe
    df_partial = df.head(current_row + 1).copy()

    # Expand JSON responses into columns
    try:
        if responses:
            # Filter out None values for JSON normalization
            valid_responses = [r for r in responses if r is not None]
            if valid_responses:
                output_df = pd.json_normalize(valid_responses)

                # Ensure response list matches df length
                response_series = pd.Series(responses[:len(df_partial)])
                df_partial["raw_llm_response"] = response_series

                # Add expanded columns where possible
                for col in output_df.columns:
                    if col not in df_partial.columns:
                        col_data = [resp.get(col) if isinstance(resp, dict) else None for resp in responses[:len(df_partial)]]
                        df_partial[col] = col_data
    except Exception as e:
        print(f"‚ö†Ô∏è  JSON expansion error: {e}, saving raw responses only")
        df_partial["raw_llm_response"] = pd.Series(responses[:len(df_partial)])

    # Save intermediate file
    intermediate_path = os.path.join(GDRIVE_DIR, f"research_intermediate_row_{current_row}.xlsx")
    df_partial.to_excel(intermediate_path, index=False)

    # Save cost summary
    cost = input_tokens * 0.00025 + output_tokens * 0.00125
    cost_summary = f"""Research-Focused Bead Coding - Intermediate Save
Row: {current_row}
Input tokens: {input_tokens:,}
Output tokens: {output_tokens:,}
Estimated cost: ${cost:.4f}
Timestamp: {datetime.datetime.now().isoformat()}
"""

    with open(os.path.join(GDRIVE_DIR, 'cost_summary.txt'), 'w') as f:
        f.write(cost_summary)

    print(f"‚úÖ Saved to: research_intermediate_row_{current_row}.xlsx")
    print(f"üí∞ Cost so far: ${cost:.4f}")

# === STEP 6: Execute Processing ===
print(f"\nüöÄ Starting research-focused processing...")
responses, total_input_tokens, total_output_tokens = process_entries_with_research_focus()

# === STEP 7: Final Results ===
print(f"\nüéâ PROCESSING COMPLETE!")
print("=" * 30)

# Only create final results if we have responses
if responses:
    # Create final results
    final_df = df.head(len(responses)).copy()

    # Expand JSON into columns
    try:
        valid_responses = [r for r in responses if r is not None and isinstance(r, dict)]
        if valid_responses:
            expanded_df = pd.json_normalize(responses)

            # Add all expanded columns
            for col in expanded_df.columns:
                if col not in final_df.columns:
                    col_data = [resp.get(col) if isinstance(resp, dict) else None for resp in responses]
                    final_df[col] = col_data[:len(final_df)]
    except Exception as e:
        print(f"‚ö†Ô∏è  Final JSON expansion error: {e}")

    # Always include raw responses
    final_df["raw_llm_response"] = pd.Series(responses[:len(final_df)])

    # Save final results
    final_path = os.path.join(GDRIVE_DIR, "research_focused_bead_coding_results.xlsx")
    final_df.to_excel(final_path, index=False)

    # Create summary report
    final_cost = total_input_tokens * 0.00025 + total_output_tokens * 0.00125
    summary_report = f"""
RESEARCH-FOCUSED BEAD CODING COMPLETE
====================================

üìä PROCESSING SUMMARY:
- Total entries processed: {len(responses)}
- Successful JSON responses: {len([r for r in responses if isinstance(r, dict)])}
- Input tokens used: {total_input_tokens:,}
- Output tokens used: {total_output_tokens:,}
- Total cost: ${final_cost:.4f}

üî¨ RESEARCH ENHANCEMENTS APPLIED:
‚úÖ Enhanced exchange field logic (no vs xo)
‚úÖ Detailed bead characteristic preservation
‚úÖ Trading partner and route information capture
‚úÖ Geographic context preservation
‚úÖ Cultural and historical context maintained

üìÅ OUTPUT FILES:
- Main results: research_focused_bead_coding_results.xlsx
- Session logs: {LOG_FILE}
- Cost summary: cost_summary.txt

üéØ RESEARCH READINESS:
‚úÖ Data ready for pre-colonial African trade route reconstruction
‚úÖ Data ready for bead characteristic analysis
‚úÖ Data ready for economic network mapping
‚úÖ Data ready for cultural exchange studies

Generated: {datetime.datetime.now().isoformat()}
"""

    print(summary_report)

    # Save summary
    with open(os.path.join(GDRIVE_DIR, 'final_summary.txt'), 'w') as f:
        f.write(summary_report)

    # Reset index for next run
    with open(os.path.join(GDRIVE_DIR, 'last_index.txt'), 'w') as f:
        f.write('0')

    print(f"\nüéâ ALL FILES SAVED TO: {GDRIVE_DIR}")
    print("üî¨ Research-focused bead coding complete!")
else:
    print("\n‚ö†Ô∏è No entries were processed. Please check your data and try again.")