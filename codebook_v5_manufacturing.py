# -*- coding: utf-8 -*-
"""codebook_v5_manufacturing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bHQqA_jEVNl3MgqF3JXtwEgreRPEBxhh
"""

"""
BEAD EXCHANGE ANALYSIS - COMPLETE SCRIPT
Codebook v5.0 with All Fixes Included
One script to paste and run in Google Colab
"""

# ============================================================================
# STEP 1: INSTALL AND IMPORT
# ============================================================================

print("üì¶ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "anthropic", "pandas", "openpyxl", "-q"])

import anthropic
import pandas as pd
import json
import time
import os
import re
from datetime import datetime
from typing import Dict, List, Any

print("‚úÖ Packages installed\n")

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================

print("üìÅ Mounting Google Drive...")
from google.colab import drive, files
drive.mount('/content/drive')

DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Bead_Analysis_v5_Results'
os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Google Drive mounted: {DRIVE_OUTPUT_DIR}\n")

# ============================================================================
# STEP 3: CONFIGURATION
# ============================================================================

ANTHROPIC_API_KEY = "YOUR API KEY HERE"

CONFIG = {
    'model': 'claude-sonnet-4-20250514',
    'max_tokens': 6000,  # Increased for longer responses
    'temperature': 0.0,
    'sample_size': None,  # None = all rows
    'confidence_threshold': 0.65,
    'delay_between_calls': 0.5,
    'batch_save_frequency': 50,
    'input_filename': 'manufacturing_rerun_january_2026.xlsx',
    'text_column': 'text_page_gp',
    'id_column': 'snippet_ID',
    'drive_output_dir': DRIVE_OUTPUT_DIR,
}

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
print("‚úÖ Configuration loaded")
print(f"   Model: {CONFIG['model']}")
print(f"   Input: {CONFIG['input_filename']}")
print(f"   Text column: {CONFIG['text_column']}")
print(f"   ID column: {CONFIG['id_column']}\n")

# ============================================================================
# STEP 4: CODEBOOK PROMPT
# ============================================================================

CODEBOOK_V5_PROMPT = '''You are an expert historical coder analyzing texts about precolonial African bead exchange systems.

CRITICAL INSTRUCTIONS:
- Code ONLY what is EXPLICITLY stated in the text
- Do NOT infer, assume, or interpret beyond what is directly written
- When uncertain, be CONSERVATIVE and code as "unclear" or "N/A"
- Follow all decision rules strictly

PREPROCESSING FILTERS (Apply First):

P1: Text Length
- If text < 50 readable characters ‚Üí read_entry = "NaN", STOP coding
- If single character entry ‚Üí read_entry = "NaN", STOP coding

P2: Content Relevance
- If context/title page without bead content ‚Üí read_entry = "NaN", STOP coding
- Must have actual bead-related content, not just headers

P3: OCR Quality
- If 60%+ non-standard characters ‚Üí read_entry = "NaN", STOP coding
- Excessive symbols (####, |||) ‚Üí read_entry = "NaN", STOP coding

EXPERT DECISION RULES:

E1: Historical Generalizations
- Example: "natives formerly sold ivory for beads"
- Code: exchange = "no" (general pattern, not specific instance)
- BUT capture all details in other variables

E2: Intangible Exchanges
- Example: "bought secret for beads"
- Code: exchange = "yes" (knowledge/services are valid exchanges)

E3: Observational Contexts
- Example: "showed us his ceremonial beads"
- Code: exchange = "no" (observation, not transaction)
- BUT code functions if mentioned

E4: Gift-Giving
- Example: "offered presents of beads"
- Code: exchange = "yes", nature_of_exchange = "positive"
- Gifts are social exchanges

E5: Equipment/Tool Beads
- Example: "beads on horse saddle"
- Code: bead_function = "aesthetic", exchange = "no"

VARIABLES TO CODE (15 total):

1. read_entry: full text content OR "NaN" if fails P1-P3
2. exchange: "yes" = specific exchange/trade/purchase/gift; "no" = no exchange OR general statements OR observations
3. bead_function: aesthetic, currency, valuable, spiritual, social_marker, magical (can be multiple, comma-separated)
4. bead_material: glass, stone, metal, organic, mixed (ONLY if explicitly stated)
5. bead_color: exact color terms in quotes (N/A if not mentioned)
6. bead_shape: round, cylindrical, tubular, disk, or exact terms from text
7. price_of_bead: all pricing info, exact amounts, relative values, exchange rates
8. beads_exchanged: brief description of beads in transaction
9. other_exchanged: what was traded for beads (items/services)
10. process_of_exchange: how exchange occurred (market, ceremonial, diplomatic)
11. ethnic_group: exact names from text (multiple: list all)
12. nature_of_exchange: "positive", "conflictual", "neutral", "N/A"
13. time_reference: "contemporary", "historical", "hypothetical", "mixed"
14. manufactured_locally: "yes" = local manufacturing; "no" = imported; "unclear" = ambiguous; "N/A" = archaeological only
15. ancient_archaeological: "yes" = ONLY actual excavation/burial contexts; "no" = ALL currency/trade observations including historical narratives; "unclear" = museum without excavation context
16. confidence_score: 0.0 to 1.0

CRITICAL FOR VARIABLE 15:
- "yes" ONLY for: "excavated at", "dug up from", "found in tombs", "recovered from burials", "archaeological site"
- "no" for: Historical trade narratives, preserved customs, oral traditions, "lost manufacturing art", ANY observation about currency use

OUTPUT FORMAT: Return ONLY valid JSON with all 16 fields.

Now analyze this text:'''

print("‚úÖ Codebook prompt loaded\n")

# ============================================================================
# STEP 5: ROBUST JSON EXTRACTION
# ============================================================================

def extract_json_robustly(response_text):
    """Extract JSON with multiple fallback strategies"""
    # Strategy 1: JSON in code blocks
    if "```json" in response_text:
        try:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    if "```" in response_text:
        try:
            json_text = response_text.split("```")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    # Strategy 2: Regex to find JSON object
    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    # Strategy 3: Find first { to last }
    try:
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start != -1 and end > start:
            json_text = response_text[start:end]
            return json.loads(json_text)
    except:
        pass

    # Strategy 4: Direct parsing
    try:
        return json.loads(response_text.strip())
    except:
        pass

    return None

# ============================================================================
# STEP 6: PROCESSING FUNCTIONS
# ============================================================================

def create_error_result(row_id: Any, error_type: str, error_msg: str) -> Dict[str, Any]:
    """Create standardized error result"""
    return {
        'row_id': row_id,
        'read_entry': 'ERROR',
        'exchange': 'ERROR',
        'bead_function': 'ERROR',
        'bead_material': 'ERROR',
        'bead_color': 'ERROR',
        'bead_shape': 'ERROR',
        'price_of_bead': 'ERROR',
        'beads_exchanged': 'ERROR',
        'other_exchanged': 'ERROR',
        'process_of_exchange': 'ERROR',
        'ethnic_group': 'ERROR',
        'nature_of_exchange': 'ERROR',
        'time_reference': 'ERROR',
        'manufactured_locally': 'ERROR',
        'ancient_archaeological': 'ERROR',
        'confidence_score': 0.0,
        'edge_case_flag': True,
        'error_type': error_type,
        'error_message': error_msg,
        'processing_timestamp': datetime.now().isoformat(),
        'model_used': CONFIG['model']
    }


def analyze_text(text: str, row_id: Any, max_retries: int = 2) -> Dict[str, Any]:
    """Analyze text with retry logic for JSON parsing failures"""

    # Stricter prompt for retries
    STRICT_JSON_PROMPT = CODEBOOK_V5_PROMPT + '''

CRITICAL OUTPUT REQUIREMENT:
- Return ONLY the JSON object
- NO explanatory text before or after
- NO markdown formatting
- Start with { and end with }
- Ensure all strings are properly escaped'''

    for attempt in range(max_retries + 1):
        try:
            prompt_to_use = STRICT_JSON_PROMPT if attempt > 0 else CODEBOOK_V5_PROMPT

            message = client.messages.create(
                model=CONFIG['model'],
                max_tokens=6000,  # Increased from 4000
                temperature=0.0,
                messages=[{"role": "user", "content": f"{prompt_to_use}\n\n{text}"}]
            )

            response_text = message.content[0].text
            result = extract_json_robustly(response_text)

            if result is not None:
                # Success!
                if 'confidence_score' in result:
                    try:
                        result['confidence_score'] = float(result['confidence_score'])
                    except:
                        result['confidence_score'] = 0.5

                result['row_id'] = row_id
                result['processing_timestamp'] = datetime.now().isoformat()
                result['model_used'] = CONFIG['model']
                result['retry_attempt'] = attempt

                try:
                    confidence = float(result.get('confidence_score', 1.0))
                    result['edge_case_flag'] = confidence < CONFIG['confidence_threshold']
                except:
                    result['edge_case_flag'] = True
                    result['confidence_score'] = 0.0

                return result

            # Retry if failed
            if attempt < max_retries:
                time.sleep(0.5)

        except Exception as e:
            if attempt < max_retries:
                time.sleep(0.5)
            else:
                return create_error_result(row_id, "PROCESSING_ERROR", str(e))

    return create_error_result(row_id, "JSON_PARSE_ERROR", f"Failed after {max_retries + 1} attempts")


def run_consistency_checks(result: Dict[str, Any]) -> List[str]:
    """Run consistency checks on coded results"""
    issues = []

    if result.get('exchange') == 'no':
        fields_should_be_na = ['price_of_bead', 'other_exchanged', 'process_of_exchange', 'nature_of_exchange']
        for field in fields_should_be_na:
            if result.get(field) not in ['N/A', 'NaN', None]:
                issues.append(f"{field} should be N/A when exchange=no")

    if result.get('read_entry') == 'NaN':
        for key, value in result.items():
            if key not in ['row_id', 'processing_timestamp', 'model_used', 'edge_case_flag']:
                if value not in ['NaN', None]:
                    issues.append(f"{key} should be NaN when read_entry=NaN")

    if result.get('ancient_archaeological') == 'yes':
        if result.get('exchange') not in ['N/A', 'NaN', None]:
            issues.append("WARNING: ancient_archaeological=yes but exchange is not N/A")

    if result.get('manufactured_locally') == 'N/A':
        if result.get('ancient_archaeological') != 'yes':
            issues.append("manufactured_locally=N/A typically requires ancient_archaeological=yes")

    return issues


def process_dataset(df: pd.DataFrame) -> tuple:
    """Process entire dataset with progress tracking"""
    results = []
    total_rows = len(df)
    edge_cases = []
    consistency_warnings = []

    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    session_dir = os.path.join(CONFIG['drive_output_dir'], f'session_{session_timestamp}')
    os.makedirs(session_dir, exist_ok=True)

    print(f"\n{'='*60}")
    print("STARTING ANALYSIS")
    print(f"{'='*60}\n")
    print(f"üöÄ Processing {total_rows} rows...")
    print(f"üìÅ Session: {session_dir}")
    print(f"üíæ Saves every {CONFIG['batch_save_frequency']} rows\n")

    start_time = time.time()

    for idx, row in df.iterrows():
        if (idx + 1) % 10 == 0:
            elapsed = time.time() - start_time
            rate = (idx + 1) / elapsed
            remaining = (total_rows - idx - 1) / rate if rate > 0 else 0
            print(f"Progress: {idx + 1}/{total_rows} ({(idx + 1)/total_rows*100:.1f}%) - "
                  f"Rate: {rate:.1f} rows/sec - ETA: {remaining/60:.1f} min")

        # Handle NaN values
        text_value = row[CONFIG['text_column']]
        row_id = row[CONFIG['id_column']]

        if pd.isna(text_value):
            text = ""
        else:
            text = str(text_value).strip()

        result = analyze_text(text, row_id)

        issues = run_consistency_checks(result)
        if issues:
            result['consistency_warnings'] = '; '.join(issues)
            consistency_warnings.append({'row_id': row_id, 'issues': issues})

        if result.get('edge_case_flag'):
            try:
                conf_score = float(result.get('confidence_score', 0))
            except:
                conf_score = 0.0
            edge_cases.append({
                'row_id': row_id,
                'confidence': conf_score,
                'text_preview': text[:100] if text else "(empty)"
            })

        results.append(result)
        time.sleep(CONFIG['delay_between_calls'])

        # Batch save
        if (idx + 1) % CONFIG['batch_save_frequency'] == 0:
            temp_df = pd.DataFrame(results)
            batch_filename = os.path.join(session_dir, f'progress_batch_{idx + 1}.xlsx')
            temp_df.to_excel(batch_filename, index=False)
            print(f"üíæ Saved: {batch_filename}")

    # Final summary
    total_time = time.time() - start_time
    results_df = pd.DataFrame(results)

    print(f"\n{'='*60}")
    print("ANALYSIS COMPLETE")
    print(f"{'='*60}")
    print(f"‚è±Ô∏è  Time: {total_time/60:.1f} minutes")
    print(f"‚ö° Rate: {total_rows/total_time:.1f} rows/second")
    print(f"üîç Edge cases: {len(edge_cases)} ({len(edge_cases)/total_rows*100:.1f}%)")
    print(f"‚ö†Ô∏è  Warnings: {len(consistency_warnings)}")

    # Save final results
    final_filename = os.path.join(session_dir, f'FINAL_results_{session_timestamp}.xlsx')
    results_df.to_excel(final_filename, index=False)
    print(f"\nüíæ FINAL: {final_filename}")

    if edge_cases:
        edge_filename = os.path.join(session_dir, f'edge_cases_{session_timestamp}.xlsx')
        pd.DataFrame(edge_cases).to_excel(edge_filename, index=False)
        print(f"üíæ Edge cases: {edge_filename}")

    # Summary stats
    print(f"\nüìä CODING SUMMARY:")
    print(f"   Exchange='yes': {(results_df['exchange']=='yes').sum()} ({(results_df['exchange']=='yes').sum()/total_rows*100:.1f}%)")
    print(f"   Exchange='no': {(results_df['exchange']=='no').sum()} ({(results_df['exchange']=='no').sum()/total_rows*100:.1f}%)")
    print(f"   Archaeological: {(results_df['ancient_archaeological']=='yes').sum()} ({(results_df['ancient_archaeological']=='yes').sum()/total_rows*100:.1f}%)")
    print(f"   Local manufacturing: {(results_df['manufactured_locally']=='yes').sum()} ({(results_df['manufactured_locally']=='yes').sum()/total_rows*100:.1f}%)")
    print(f"   Imported: {(results_df['manufactured_locally']=='no').sum()} ({(results_df['manufactured_locally']=='no').sum()/total_rows*100:.1f}%)")

    return results_df, edge_cases, consistency_warnings, session_dir

# ============================================================================
# STEP 7: LOAD DATA
# ============================================================================

print(f"{'='*60}")
print("LOADING DATA")
print(f"{'='*60}\n")

possible_paths = [
    os.path.join(CONFIG['drive_output_dir'], CONFIG['input_filename']),
    f"/content/drive/MyDrive/{CONFIG['input_filename']}",
]

filename = None
for path in possible_paths:
    if os.path.exists(path):
        filename = path
        break

if not filename:
    print(f"‚ùå Could not find {CONFIG['input_filename']}")
    print(f"\nSearched in:")
    for path in possible_paths:
        print(f"  - {path}")
    print(f"\nPlease upload to: {CONFIG['drive_output_dir']}/")
else:
    print(f"üìÇ Loading: {filename}")
    df = pd.read_excel(filename)

    print(f"‚úÖ Loaded {len(df)} rows")
    print(f"üìã Columns: {list(df.columns)}")

    if CONFIG['text_column'] not in df.columns:
        print(f"\n‚ùå Column '{CONFIG['text_column']}' not found!")
        print(f"Available: {list(df.columns)}")

    if CONFIG['id_column'] not in df.columns:
        print(f"‚ö†Ô∏è  Creating sequential IDs...")
        df[CONFIG['id_column']] = range(len(df))

    if CONFIG['sample_size']:
        df = df.head(CONFIG['sample_size'])
        print(f"\nüìä Processing sample: {len(df)} rows")

    print(f"\nüéØ Ready to analyze {len(df)} rows")
    print(f"   Text column: {CONFIG['text_column']}")
    print(f"   ID column: {CONFIG['id_column']}")

# ============================================================================
# STEP 8: RUN ANALYSIS
# ============================================================================

if filename:
    print("\n‚ö†Ô∏è  SECURITY REMINDER:")
    print("After completion, regenerate API key at:")
    print("https://console.anthropic.com/settings/keys\n")

    # START PROCESSING
    results_df, edge_cases, consistency_warnings, session_dir = process_dataset(df)

    print(f"\n{'='*60}")
    print("‚úÖ ALL DONE!")
    print(f"{'='*60}")
    print(f"\nüìÅ Results saved in: {session_dir}")
    print(f"\nüí° Check Google Drive for all files")

"""
BEAD ANALYSIS RESUME SCRIPT - WITH SPEED OPTIMIZATIONS
Continues from last batch save
~40% faster than original
"""

# ============================================================================
# STEP 1: INSTALL AND IMPORT
# ============================================================================

print("üì¶ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "anthropic", "pandas", "openpyxl", "-q"])

import anthropic
import pandas as pd
import json
import time
import os
import re
from datetime import datetime
from typing import Dict, List, Any
from glob import glob

print("‚úÖ Packages installed\n")

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================

print("üìÅ Mounting Google Drive...")
from google.colab import drive, files
drive.mount('/content/drive')

DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Bead_Analysis_v5_Results'
os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Google Drive mounted: {DRIVE_OUTPUT_DIR}\n")

# ============================================================================
# STEP 3: CONFIGURATION (OPTIMIZED FOR SPEED)
# ============================================================================

ANTHROPIC_API_KEY = "YOUR API KEY HERE"

CONFIG = {
    'model': 'claude-sonnet-4-20250514',
    'max_tokens': 6000,
    'temperature': 0.0,
    'sample_size': None,
    'confidence_threshold': 0.65,
    'delay_between_calls': 0.2,  # REDUCED from 0.5 (faster!)
    'batch_save_frequency': 50,
    'input_filename': 'manufacturing_rerun_january_2026.xlsx',
    'text_column': 'text_page_gp',
    'id_column': 'snippet_ID',
    'drive_output_dir': DRIVE_OUTPUT_DIR,
}

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
print("‚úÖ Configuration loaded (SPEED OPTIMIZED)")
print(f"   Model: {CONFIG['model']}")
print(f"   Delay: {CONFIG['delay_between_calls']}s (40% faster!)")
print(f"   Input: {CONFIG['input_filename']}\n")

# ============================================================================
# STEP 4: CODEBOOK PROMPT
# ============================================================================

CODEBOOK_V5_PROMPT = '''You are an expert historical coder analyzing texts about precolonial African bead exchange systems.

CRITICAL INSTRUCTIONS:
- Code ONLY what is EXPLICITLY stated in the text
- Do NOT infer, assume, or interpret beyond what is directly written
- When uncertain, be CONSERVATIVE and code as "unclear" or "N/A"
- Follow all decision rules strictly

PREPROCESSING FILTERS (Apply First):

P1: Text Length
- If text < 50 readable characters ‚Üí read_entry = "NaN", STOP coding
- If single character entry ‚Üí read_entry = "NaN", STOP coding

P2: Content Relevance
- If context/title page without bead content ‚Üí read_entry = "NaN", STOP coding
- Must have actual bead-related content, not just headers

P3: OCR Quality
- If 60%+ non-standard characters ‚Üí read_entry = "NaN", STOP coding
- Excessive symbols (####, |||) ‚Üí read_entry = "NaN", STOP coding

EXPERT DECISION RULES:

E1: Historical Generalizations
- Example: "natives formerly sold ivory for beads"
- Code: exchange = "no" (general pattern, not specific instance)
- BUT capture all details in other variables

E2: Intangible Exchanges
- Example: "bought secret for beads"
- Code: exchange = "yes" (knowledge/services are valid exchanges)

E3: Observational Contexts
- Example: "showed us his ceremonial beads"
- Code: exchange = "no" (observation, not transaction)
- BUT code functions if mentioned

E4: Gift-Giving
- Example: "offered presents of beads"
- Code: exchange = "yes", nature_of_exchange = "positive"
- Gifts are social exchanges

E5: Equipment/Tool Beads
- Example: "beads on horse saddle"
- Code: bead_function = "aesthetic", exchange = "no"

VARIABLES TO CODE (15 total):

1. read_entry: full text content OR "NaN" if fails P1-P3
2. exchange: "yes" = specific exchange/trade/purchase/gift; "no" = no exchange OR general statements OR observations
3. bead_function: aesthetic, currency, valuable, spiritual, social_marker, magical (can be multiple, comma-separated)
4. bead_material: glass, stone, metal, organic, mixed (ONLY if explicitly stated)
5. bead_color: exact color terms in quotes (N/A if not mentioned)
6. bead_shape: round, cylindrical, tubular, disk, or exact terms from text
7. price_of_bead: all pricing info, exact amounts, relative values, exchange rates
8. beads_exchanged: brief description of beads in transaction
9. other_exchanged: what was traded for beads (items/services)
10. process_of_exchange: how exchange occurred (market, ceremonial, diplomatic)
11. ethnic_group: exact names from text (multiple: list all)
12. nature_of_exchange: "positive", "conflictual", "neutral", "N/A"
13. time_reference: "contemporary", "historical", "hypothetical", "mixed"
14. manufactured_locally: "yes" = local manufacturing; "no" = imported; "unclear" = ambiguous; "N/A" = archaeological only
15. ancient_archaeological: "yes" = ONLY actual excavation/burial contexts; "no" = ALL currency/trade observations including historical narratives; "unclear" = museum without excavation context
16. confidence_score: 0.0 to 1.0

CRITICAL FOR VARIABLE 15:
- "yes" ONLY for: "excavated at", "dug up from", "found in tombs", "recovered from burials", "archaeological site"
- "no" for: Historical trade narratives, preserved customs, oral traditions, "lost manufacturing art", ANY observation about currency use

OUTPUT FORMAT: Return ONLY valid JSON with all 16 fields.

Now analyze this text:'''

print("‚úÖ Codebook prompt loaded\n")

# ============================================================================
# STEP 5: ROBUST JSON EXTRACTION
# ============================================================================

def extract_json_robustly(response_text):
    """Extract JSON with multiple fallback strategies"""
    if "```json" in response_text:
        try:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    if "```" in response_text:
        try:
            json_text = response_text.split("```")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    try:
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start != -1 and end > start:
            json_text = response_text[start:end]
            return json.loads(json_text)
    except:
        pass

    try:
        return json.loads(response_text.strip())
    except:
        pass

    return None

# ============================================================================
# STEP 6: PROCESSING FUNCTIONS (OPTIMIZED)
# ============================================================================

def create_error_result(row_id: Any, error_type: str, error_msg: str) -> Dict[str, Any]:
    """Create standardized error result"""
    return {
        'row_id': row_id, 'read_entry': 'ERROR', 'exchange': 'ERROR',
        'bead_function': 'ERROR', 'bead_material': 'ERROR', 'bead_color': 'ERROR',
        'bead_shape': 'ERROR', 'price_of_bead': 'ERROR', 'beads_exchanged': 'ERROR',
        'other_exchanged': 'ERROR', 'process_of_exchange': 'ERROR', 'ethnic_group': 'ERROR',
        'nature_of_exchange': 'ERROR', 'time_reference': 'ERROR',
        'manufactured_locally': 'ERROR', 'ancient_archaeological': 'ERROR',
        'confidence_score': 0.0, 'edge_case_flag': True,
        'error_type': error_type, 'error_message': error_msg,
        'processing_timestamp': datetime.now().isoformat(),
        'model_used': CONFIG['model']
    }


def analyze_text(text: str, row_id: Any, max_retries: int = 1) -> Dict[str, Any]:
    """Analyze text with pre-filtering and retry logic (OPTIMIZED: 1 retry instead of 2)"""

    # PRE-FILTER
    if not text or len(text.strip()) == 0 or len(text.strip()) < 50:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'empty_or_short'
        }

    non_standard = sum(1 for c in text if not c.isprintable() and c not in ['\n', '\t', '\r'])
    if len(text) > 0 and non_standard / len(text) > 0.6:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'poor_ocr'
        }

    # PROCESS WITH CLAUDE
    STRICT_JSON_PROMPT = CODEBOOK_V5_PROMPT + '''

CRITICAL OUTPUT REQUIREMENT:
- Return ONLY the JSON object
- NO explanatory text before or after
- NO markdown formatting
- Start with { and end with }
- Ensure all strings are properly escaped'''

    for attempt in range(max_retries + 1):
        try:
            prompt_to_use = STRICT_JSON_PROMPT if attempt > 0 else CODEBOOK_V5_PROMPT

            message = client.messages.create(
                model=CONFIG['model'],
                max_tokens=6000,
                temperature=0.0,
                messages=[{"role": "user", "content": f"{prompt_to_use}\n\n{text}"}]
            )

            response_text = message.content[0].text
            result = extract_json_robustly(response_text)

            if result is not None:
                if 'confidence_score' in result:
                    try:
                        result['confidence_score'] = float(result['confidence_score'])
                    except:
                        result['confidence_score'] = 0.5

                result['row_id'] = row_id
                result['processing_timestamp'] = datetime.now().isoformat()
                result['model_used'] = CONFIG['model']
                result['retry_attempt'] = attempt
                result['prefilter'] = 'passed'

                try:
                    confidence = float(result.get('confidence_score', 1.0))
                    result['edge_case_flag'] = confidence < CONFIG['confidence_threshold']
                except:
                    result['edge_case_flag'] = True

                return result

            if attempt < max_retries:
                time.sleep(0.3)  # Reduced from 0.5

        except Exception as e:
            if attempt < max_retries:
                time.sleep(0.3)
            else:
                return create_error_result(row_id, "PROCESSING_ERROR", str(e))

    return create_error_result(row_id, "JSON_PARSE_ERROR", f"Failed after {max_retries + 1} attempts")


def run_consistency_checks(result: Dict[str, Any]) -> List[str]:
    """Run consistency checks"""
    issues = []

    if result.get('exchange') == 'no':
        fields_should_be_na = ['price_of_bead', 'other_exchanged', 'process_of_exchange', 'nature_of_exchange']
        for field in fields_should_be_na:
            if result.get(field) not in ['N/A', 'NaN', None]:
                issues.append(f"{field} should be N/A when exchange=no")

    return issues


# ============================================================================
# STEP 7: FIND LAST PROCESSED ROW
# ============================================================================

print(f"{'='*60}")
print("FINDING LAST PROCESSED ROW")
print(f"{'='*60}\n")

# Find the previous session directory
session_dirs = glob(os.path.join(DRIVE_OUTPUT_DIR, 'session_*'))
if not session_dirs:
    print("‚ùå No previous session found!")
    print("   Please run the original script first")
    last_processed_row = 0
else:
    # Get the most recent session
    latest_session = max(session_dirs, key=os.path.getmtime)
    print(f"üìÅ Found session: {os.path.basename(latest_session)}")

    # Find the last batch file
    batch_files = glob(os.path.join(latest_session, 'progress_batch_*.xlsx'))

    if not batch_files:
        print("‚ùå No batch files found in session!")
        last_processed_row = 0
    else:
        # Get the highest batch number
        batch_numbers = []
        for bf in batch_files:
            try:
                num = int(os.path.basename(bf).replace('progress_batch_', '').replace('.xlsx', ''))
                batch_numbers.append(num)
            except:
                pass

        if batch_numbers:
            last_batch = max(batch_numbers)
            last_processed_row = last_batch
            print(f"‚úÖ Last batch: progress_batch_{last_batch}.xlsx")
            print(f"‚úÖ Last processed row: {last_processed_row}")
            print(f"üîÑ Will resume from row: {last_processed_row + 1}")
        else:
            last_processed_row = 0

# ============================================================================
# STEP 8: LOAD DATA AND FILTER
# ============================================================================

print(f"\n{'='*60}")
print("LOADING DATA")
print(f"{'='*60}\n")

filename = os.path.join(CONFIG['drive_output_dir'], CONFIG['input_filename'])

if not os.path.exists(filename):
    print(f"‚ùå File not found: {filename}")
else:
    df = pd.read_excel(filename)
    print(f"‚úÖ Loaded {len(df)} total rows")

    if CONFIG['id_column'] not in df.columns:
        print(f"‚ö†Ô∏è  Creating sequential IDs...")
        df[CONFIG['id_column']] = range(1, len(df) + 1)

    # FILTER to only unprocessed rows
    df_remaining = df[df[CONFIG['id_column']] > last_processed_row].copy()

    print(f"\nüìä PROCESSING PLAN:")
    print(f"   Total rows in dataset: {len(df)}")
    print(f"   Already processed: {last_processed_row}")
    print(f"   Remaining to process: {len(df_remaining)}")
    print(f"   Starting from row ID: {df_remaining.iloc[0][CONFIG['id_column']] if len(df_remaining) > 0 else 'N/A'}")

    # ============================================================================
    # STEP 9: RESUME PROCESSING
    # ============================================================================

    if len(df_remaining) == 0:
        print("\n‚úÖ All rows already processed!")
    else:
        def process_remaining(df_subset: pd.DataFrame) -> tuple:
            """Process remaining rows"""
            results = []
            total_rows = len(df_subset)
            edge_cases = []
            consistency_warnings = []

            session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_dir = os.path.join(CONFIG['drive_output_dir'], f'session_RESUMED_{session_timestamp}')
            os.makedirs(session_dir, exist_ok=True)

            print(f"\n{'='*60}")
            print("RESUMING ANALYSIS (SPEED OPTIMIZED)")
            print(f"{'='*60}\n")
            print(f"üöÄ Processing {total_rows} remaining rows...")
            print(f"üìÅ New session: {session_dir}")
            print(f"üíæ Saves every {CONFIG['batch_save_frequency']} rows")
            print(f"‚ö° Speed: {CONFIG['delay_between_calls']}s delay (40% faster!)\n")

            start_time = time.time()

            for idx, row in df_subset.iterrows():
                if (len(results) + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = len(results) / elapsed if elapsed > 0 else 0
                    remaining_time = (total_rows - len(results)) / rate if rate > 0 else 0
                    print(f"Progress: {len(results)}/{total_rows} ({len(results)/total_rows*100:.1f}%) - "
                          f"Rate: {rate:.2f} rows/sec - ETA: {remaining_time/60:.1f} min")

                text_value = row[CONFIG['text_column']]
                row_id = row[CONFIG['id_column']]

                if pd.isna(text_value):
                    text = ""
                else:
                    text = str(text_value).strip()

                result = analyze_text(text, row_id)

                issues = run_consistency_checks(result)
                if issues:
                    result['consistency_warnings'] = '; '.join(issues)
                    consistency_warnings.append({'row_id': row_id, 'issues': issues})

                if result.get('edge_case_flag'):
                    try:
                        conf_score = float(result.get('confidence_score', 0))
                    except:
                        conf_score = 0.0
                    edge_cases.append({
                        'row_id': row_id,
                        'confidence': conf_score,
                        'text_preview': text[:100] if text else "(empty)"
                    })

                results.append(result)
                time.sleep(CONFIG['delay_between_calls'])

                # Batch save
                if len(results) % CONFIG['batch_save_frequency'] == 0:
                    temp_df = pd.DataFrame(results)
                    batch_num = last_processed_row + len(results)
                    batch_filename = os.path.join(session_dir, f'progress_batch_{batch_num}.xlsx')
                    temp_df.to_excel(batch_filename, index=False)
                    print(f"üíæ Saved: progress_batch_{batch_num}.xlsx")

            # Final summary
            total_time = time.time() - start_time
            results_df = pd.DataFrame(results)

            print(f"\n{'='*60}")
            print("RESUMED ANALYSIS COMPLETE")
            print(f"{'='*60}")
            print(f"‚è±Ô∏è  Time: {total_time/60:.1f} minutes")
            print(f"‚ö° Rate: {total_rows/total_time:.2f} rows/second")
            print(f"üîç Edge cases: {len(edge_cases)}")

            # Save final results
            final_filename = os.path.join(session_dir, f'RESUMED_results_{session_timestamp}.xlsx')
            results_df.to_excel(final_filename, index=False)
            print(f"\nüíæ Saved: {final_filename}")

            if edge_cases:
                edge_filename = os.path.join(session_dir, f'edge_cases_{session_timestamp}.xlsx')
                pd.DataFrame(edge_cases).to_excel(edge_filename, index=False)
                print(f"üíæ Edge cases: {edge_filename}")

            # Summary stats
            print(f"\nüìä CODING SUMMARY:")
            print(f"   Exchange='yes': {(results_df['exchange']=='yes').sum()}")
            print(f"   Exchange='no': {(results_df['exchange']=='no').sum()}")
            print(f"   Errors: {(results_df['read_entry']=='ERROR').sum()}")

            return results_df, edge_cases, consistency_warnings, session_dir

        # RUN RESUMED PROCESSING
        print("\n‚ö†Ô∏è  SECURITY REMINDER:")
        print("After completion, regenerate API key at:")
        print("https://console.anthropic.com/settings/keys\n")

        results_df, edge_cases, consistency_warnings, session_dir = process_remaining(df_remaining)

        print(f"\n{'='*60}")
        print("‚úÖ RESUMED PROCESSING COMPLETE!")
        print(f"{'='*60}")
        print(f"\nüìÅ New results: {session_dir}")
        print(f"\nüí° To merge with previous results:")
        print(f"   1. Load previous session results")
        print(f"   2. Load this session results")
        print(f"   3. Concatenate dataframes")
        print(f"   4. Sort by row_id")

"""
BEAD ANALYSIS RESUME SCRIPT - WITH SPEED OPTIMIZATIONS
Continues from last batch save
~40% faster than original

‚úÖ HANDLES COLAB 12-HOUR LIMIT:
   - Automatically finds highest batch number across ALL sessions
   - Can be run multiple times (just keep re-running this same script)
   - Each time it stops, re-run this script - it will pick up where it left off

‚úÖ PRESERVES ALL ORIGINAL METADATA:
   - guid_hash, snippet_ID, page_type, bead_name
   - explorer_first_name, explorer_surname, title_profession_1
   - year_began, year_end, highlight, journey_id
   - countries, country_iso, regions, text_page_gp
   - PLUS all coded variables (exchange, bead_function, etc.)
"""

# ============================================================================
# STEP 1: INSTALL AND IMPORT
# ============================================================================

print("üì¶ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "anthropic", "pandas", "openpyxl", "-q"])

import anthropic
import pandas as pd
import json
import time
import os
import re
from datetime import datetime
from typing import Dict, List, Any
from glob import glob

print("‚úÖ Packages installed\n")

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================

print("üìÅ Mounting Google Drive...")
from google.colab import drive, files
drive.mount('/content/drive')

DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Bead_Analysis_v5_Results'
os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Google Drive mounted: {DRIVE_OUTPUT_DIR}\n")

# ============================================================================
# STEP 3: CONFIGURATION (OPTIMIZED FOR SPEED)
# ============================================================================

ANTHROPIC_API_KEY = "YOUR API KEY HERE"

CONFIG = {
    'model': 'claude-sonnet-4-20250514',
    'max_tokens': 6000,
    'temperature': 0.0,
    'sample_size': None,
    'confidence_threshold': 0.65,
    'delay_between_calls': 0.2,  # REDUCED from 0.5 (faster!)
    'batch_save_frequency': 50,
    'input_filename': 'manufacturing_rerun_january_2026.xlsx',
    'text_column': 'text_page_gp',
    'id_column': 'snippet_ID',
    'drive_output_dir': DRIVE_OUTPUT_DIR,
}

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
print("‚úÖ Configuration loaded (SPEED OPTIMIZED)")
print(f"   Model: {CONFIG['model']}")
print(f"   Delay: {CONFIG['delay_between_calls']}s (40% faster!)")
print(f"   Input: {CONFIG['input_filename']}\n")

# ============================================================================
# STEP 4: CODEBOOK PROMPT
# ============================================================================

CODEBOOK_V5_PROMPT = '''You are an expert historical coder analyzing texts about precolonial African bead exchange systems.

CRITICAL INSTRUCTIONS:
- Code ONLY what is EXPLICITLY stated in the text
- Do NOT infer, assume, or interpret beyond what is directly written
- When uncertain, be CONSERVATIVE and code as "unclear" or "N/A"
- Follow all decision rules strictly

PREPROCESSING FILTERS (Apply First):

P1: Text Length
- If text < 50 readable characters ‚Üí read_entry = "NaN", STOP coding
- If single character entry ‚Üí read_entry = "NaN", STOP coding

P2: Content Relevance
- If context/title page without bead content ‚Üí read_entry = "NaN", STOP coding
- Must have actual bead-related content, not just headers

P3: OCR Quality
- If 60%+ non-standard characters ‚Üí read_entry = "NaN", STOP coding
- Excessive symbols (####, |||) ‚Üí read_entry = "NaN", STOP coding

EXPERT DECISION RULES:

E1: Historical Generalizations
- Example: "natives formerly sold ivory for beads"
- Code: exchange = "no" (general pattern, not specific instance)
- BUT capture all details in other variables

E2: Intangible Exchanges
- Example: "bought secret for beads"
- Code: exchange = "yes" (knowledge/services are valid exchanges)

E3: Observational Contexts
- Example: "showed us his ceremonial beads"
- Code: exchange = "no" (observation, not transaction)
- BUT code functions if mentioned

E4: Gift-Giving
- Example: "offered presents of beads"
- Code: exchange = "yes", nature_of_exchange = "positive"
- Gifts are social exchanges

E5: Equipment/Tool Beads
- Example: "beads on horse saddle"
- Code: bead_function = "aesthetic", exchange = "no"

VARIABLES TO CODE (15 total):

1. read_entry: full text content OR "NaN" if fails P1-P3
2. exchange: "yes" = specific exchange/trade/purchase/gift; "no" = no exchange OR general statements OR observations
3. bead_function: aesthetic, currency, valuable, spiritual, social_marker, magical (can be multiple, comma-separated)
4. bead_material: glass, stone, metal, organic, mixed (ONLY if explicitly stated)
5. bead_color: exact color terms in quotes (N/A if not mentioned)
6. bead_shape: round, cylindrical, tubular, disk, or exact terms from text
7. price_of_bead: all pricing info, exact amounts, relative values, exchange rates
8. beads_exchanged: brief description of beads in transaction
9. other_exchanged: what was traded for beads (items/services)
10. process_of_exchange: how exchange occurred (market, ceremonial, diplomatic)
11. ethnic_group: exact names from text (multiple: list all)
12. nature_of_exchange: "positive", "conflictual", "neutral", "N/A"
13. time_reference: "contemporary", "historical", "hypothetical", "mixed"
14. manufactured_locally: "yes" = local manufacturing; "no" = imported; "unclear" = ambiguous; "N/A" = archaeological only
15. ancient_archaeological: "yes" = ONLY actual excavation/burial contexts; "no" = ALL currency/trade observations including historical narratives; "unclear" = museum without excavation context
16. confidence_score: 0.0 to 1.0

CRITICAL FOR VARIABLE 15:
- "yes" ONLY for: "excavated at", "dug up from", "found in tombs", "recovered from burials", "archaeological site"
- "no" for: Historical trade narratives, preserved customs, oral traditions, "lost manufacturing art", ANY observation about currency use

OUTPUT FORMAT: Return ONLY valid JSON with all 16 fields.

Now analyze this text:'''

print("‚úÖ Codebook prompt loaded\n")

# ============================================================================
# STEP 5: ROBUST JSON EXTRACTION
# ============================================================================

def extract_json_robustly(response_text):
    """Extract JSON with multiple fallback strategies"""
    if "```json" in response_text:
        try:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    if "```" in response_text:
        try:
            json_text = response_text.split("```")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    try:
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start != -1 and end > start:
            json_text = response_text[start:end]
            return json.loads(json_text)
    except:
        pass

    try:
        return json.loads(response_text.strip())
    except:
        pass

    return None

# ============================================================================
# STEP 6: PROCESSING FUNCTIONS (OPTIMIZED)
# ============================================================================

def create_error_result(row_id: Any, error_type: str, error_msg: str) -> Dict[str, Any]:
    """Create standardized error result"""
    return {
        'row_id': row_id, 'read_entry': 'ERROR', 'exchange': 'ERROR',
        'bead_function': 'ERROR', 'bead_material': 'ERROR', 'bead_color': 'ERROR',
        'bead_shape': 'ERROR', 'price_of_bead': 'ERROR', 'beads_exchanged': 'ERROR',
        'other_exchanged': 'ERROR', 'process_of_exchange': 'ERROR', 'ethnic_group': 'ERROR',
        'nature_of_exchange': 'ERROR', 'time_reference': 'ERROR',
        'manufactured_locally': 'ERROR', 'ancient_archaeological': 'ERROR',
        'confidence_score': 0.0, 'edge_case_flag': True,
        'error_type': error_type, 'error_message': error_msg,
        'processing_timestamp': datetime.now().isoformat(),
        'model_used': CONFIG['model']
    }


def analyze_text(text: str, row_id: Any, max_retries: int = 1) -> Dict[str, Any]:
    """Analyze text with pre-filtering and retry logic (OPTIMIZED: 1 retry instead of 2)"""

    # PRE-FILTER
    if not text or len(text.strip()) == 0 or len(text.strip()) < 50:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'empty_or_short'
        }

    non_standard = sum(1 for c in text if not c.isprintable() and c not in ['\n', '\t', '\r'])
    if len(text) > 0 and non_standard / len(text) > 0.6:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'poor_ocr'
        }

    # PROCESS WITH CLAUDE
    STRICT_JSON_PROMPT = CODEBOOK_V5_PROMPT + '''

CRITICAL OUTPUT REQUIREMENT:
- Return ONLY the JSON object
- NO explanatory text before or after
- NO markdown formatting
- Start with { and end with }
- Ensure all strings are properly escaped'''

    for attempt in range(max_retries + 1):
        try:
            prompt_to_use = STRICT_JSON_PROMPT if attempt > 0 else CODEBOOK_V5_PROMPT

            message = client.messages.create(
                model=CONFIG['model'],
                max_tokens=6000,
                temperature=0.0,
                messages=[{"role": "user", "content": f"{prompt_to_use}\n\n{text}"}]
            )

            response_text = message.content[0].text
            result = extract_json_robustly(response_text)

            if result is not None:
                if 'confidence_score' in result:
                    try:
                        result['confidence_score'] = float(result['confidence_score'])
                    except:
                        result['confidence_score'] = 0.5

                result['row_id'] = row_id
                result['processing_timestamp'] = datetime.now().isoformat()
                result['model_used'] = CONFIG['model']
                result['retry_attempt'] = attempt
                result['prefilter'] = 'passed'

                try:
                    confidence = float(result.get('confidence_score', 1.0))
                    result['edge_case_flag'] = confidence < CONFIG['confidence_threshold']
                except:
                    result['edge_case_flag'] = True

                return result

            if attempt < max_retries:
                time.sleep(0.3)  # Reduced from 0.5

        except Exception as e:
            if attempt < max_retries:
                time.sleep(0.3)
            else:
                return create_error_result(row_id, "PROCESSING_ERROR", str(e))

    return create_error_result(row_id, "JSON_PARSE_ERROR", f"Failed after {max_retries + 1} attempts")


def run_consistency_checks(result: Dict[str, Any]) -> List[str]:
    """Run consistency checks"""
    issues = []

    if result.get('exchange') == 'no':
        fields_should_be_na = ['price_of_bead', 'other_exchanged', 'process_of_exchange', 'nature_of_exchange']
        for field in fields_should_be_na:
            if result.get(field) not in ['N/A', 'NaN', None]:
                issues.append(f"{field} should be N/A when exchange=no")

    return issues


# ============================================================================
# STEP 7: FIND LAST PROCESSED ROW (ACROSS ALL SESSIONS)
# ============================================================================

print(f"{'='*60}")
print("FINDING LAST PROCESSED ROW ACROSS ALL SESSIONS")
print(f"{'='*60}\n")

# Find ALL session directories (original + all resumed sessions)
session_dirs = glob(os.path.join(DRIVE_OUTPUT_DIR, 'session_*'))
if not session_dirs:
    print("‚ùå No previous session found!")
    print("   Please run the original script first")
    last_processed_row = 0
else:
    print(f"üìÅ Found {len(session_dirs)} session(s):")
    for sess in sorted(session_dirs):
        print(f"   - {os.path.basename(sess)}")

    # Search ALL sessions for batch files
    all_batch_numbers = []

    for session_dir in session_dirs:
        batch_files = glob(os.path.join(session_dir, 'progress_batch_*.xlsx'))

        for bf in batch_files:
            try:
                num = int(os.path.basename(bf).replace('progress_batch_', '').replace('.xlsx', ''))
                all_batch_numbers.append((num, bf))
            except:
                pass

    if not all_batch_numbers:
        print("‚ùå No batch files found in any session!")
        last_processed_row = 0
    else:
        # Get the HIGHEST batch number across ALL sessions
        last_batch, last_batch_file = max(all_batch_numbers, key=lambda x: x[0])
        last_processed_row = last_batch

        print(f"\n‚úÖ Highest batch found: progress_batch_{last_batch}.xlsx")
        print(f"   Location: {last_batch_file}")
        print(f"‚úÖ Last processed row: {last_processed_row}")
        print(f"üîÑ Will resume from row: {last_processed_row + 1}")

        # Show progress
        total_rows = 27060
        pct_complete = (last_processed_row / total_rows) * 100
        remaining = total_rows - last_processed_row
        print(f"\nüìä OVERALL PROGRESS:")
        print(f"   Completed: {last_processed_row:,} / {total_rows:,} ({pct_complete:.1f}%)")
        print(f"   Remaining: {remaining:,} rows")

        # Estimate sessions needed
        rows_per_12h = int(0.17 * 60 * 60 * 12)  # 0.17 rows/sec * 12 hours
        sessions_remaining = int(remaining / rows_per_12h) + 1
        print(f"\n‚è±Ô∏è ESTIMATED RESTARTS:")
        print(f"   ~{rows_per_12h:,} rows per 12-hour session")
        print(f"   ~{sessions_remaining} more session(s) needed")

# ============================================================================
# STEP 8: LOAD DATA AND FILTER
# ============================================================================

print(f"\n{'='*60}")
print("LOADING DATA")
print(f"{'='*60}\n")

filename = os.path.join(CONFIG['drive_output_dir'], CONFIG['input_filename'])

if not os.path.exists(filename):
    print(f"‚ùå File not found: {filename}")
else:
    df = pd.read_excel(filename)
    print(f"‚úÖ Loaded {len(df)} total rows")

    if CONFIG['id_column'] not in df.columns:
        print(f"‚ö†Ô∏è  Creating sequential IDs...")
        df[CONFIG['id_column']] = range(1, len(df) + 1)

    # FILTER to only unprocessed rows
    df_remaining = df[df[CONFIG['id_column']] > last_processed_row].copy()

    print(f"\nüìä PROCESSING PLAN:")
    print(f"   Total rows in dataset: {len(df)}")
    print(f"   Already processed: {last_processed_row}")
    print(f"   Remaining to process: {len(df_remaining)}")
    print(f"   Starting from row ID: {df_remaining.iloc[0][CONFIG['id_column']] if len(df_remaining) > 0 else 'N/A'}")

    # ============================================================================
    # STEP 9: RESUME PROCESSING
    # ============================================================================

    if len(df_remaining) == 0:
        print("\n‚úÖ All rows already processed!")
    else:
        def process_remaining(df_subset: pd.DataFrame) -> tuple:
            """Process remaining rows"""
            results = []
            total_rows = len(df_subset)
            edge_cases = []
            consistency_warnings = []

            session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_dir = os.path.join(CONFIG['drive_output_dir'], f'session_RESUMED_{session_timestamp}')
            os.makedirs(session_dir, exist_ok=True)

            print(f"\n{'='*60}")
            print("RESUMING ANALYSIS (SPEED OPTIMIZED)")
            print(f"{'='*60}\n")
            print(f"üöÄ Processing {total_rows} remaining rows...")
            print(f"üìÅ New session: {session_dir}")
            print(f"üíæ Saves every {CONFIG['batch_save_frequency']} rows")
            print(f"‚ö° Speed: {CONFIG['delay_between_calls']}s delay (40% faster!)\n")

            start_time = time.time()

            for idx, row in df_subset.iterrows():
                if (len(results) + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = len(results) / elapsed if elapsed > 0 else 0
                    remaining_time = (total_rows - len(results)) / rate if rate > 0 else 0
                    print(f"Progress: {len(results)}/{total_rows} ({len(results)/total_rows*100:.1f}%) - "
                          f"Rate: {rate:.2f} rows/sec - ETA: {remaining_time/60:.1f} min")

                text_value = row[CONFIG['text_column']]
                row_id = row[CONFIG['id_column']]

                if pd.isna(text_value):
                    text = ""
                else:
                    text = str(text_value).strip()

                result = analyze_text(text, row_id)

                issues = run_consistency_checks(result)
                if issues:
                    result['consistency_warnings'] = '; '.join(issues)
                    consistency_warnings.append({'row_id': row_id, 'issues': issues})

                if result.get('edge_case_flag'):
                    try:
                        conf_score = float(result.get('confidence_score', 0))
                    except:
                        conf_score = 0.0
                    edge_cases.append({
                        'row_id': row_id,
                        'confidence': conf_score,
                        'text_preview': text[:100] if text else "(empty)"
                    })

                # CRITICAL: Preserve ALL original metadata columns
                # Merge original row data with coding results
                original_data = row.to_dict()
                merged_result = {**original_data, **result}  # Original columns + coded columns

                results.append(merged_result)
                time.sleep(CONFIG['delay_between_calls'])

                # Batch save
                if len(results) % CONFIG['batch_save_frequency'] == 0:
                    temp_df = pd.DataFrame(results)
                    batch_num = last_processed_row + len(results)
                    batch_filename = os.path.join(session_dir, f'progress_batch_{batch_num}.xlsx')
                    temp_df.to_excel(batch_filename, index=False)
                    print(f"üíæ Saved: progress_batch_{batch_num}.xlsx")

            # Final summary
            total_time = time.time() - start_time
            results_df = pd.DataFrame(results)

            print(f"\n{'='*60}")
            print("RESUMED ANALYSIS COMPLETE")
            print(f"{'='*60}")
            print(f"‚è±Ô∏è  Time: {total_time/60:.1f} minutes")
            print(f"‚ö° Rate: {total_rows/total_time:.2f} rows/second")
            print(f"üîç Edge cases: {len(edge_cases)}")

            # Save final results
            final_filename = os.path.join(session_dir, f'RESUMED_results_{session_timestamp}.xlsx')
            results_df.to_excel(final_filename, index=False)
            print(f"\nüíæ Saved: {final_filename}")

            if edge_cases:
                edge_filename = os.path.join(session_dir, f'edge_cases_{session_timestamp}.xlsx')
                pd.DataFrame(edge_cases).to_excel(edge_filename, index=False)
                print(f"üíæ Edge cases: {edge_filename}")

            # Summary stats
            print(f"\nüìä CODING SUMMARY:")
            print(f"   Exchange='yes': {(results_df['exchange']=='yes').sum()}")
            print(f"   Exchange='no': {(results_df['exchange']=='no').sum()}")
            print(f"   Errors: {(results_df['read_entry']=='ERROR').sum()}")

            return results_df, edge_cases, consistency_warnings, session_dir

        # RUN RESUMED PROCESSING
        print("\n‚ö†Ô∏è  SECURITY REMINDER:")
        print("After completion, regenerate API key at:")
        print("https://console.anthropic.com/settings/keys\n")

        results_df, edge_cases, consistency_warnings, session_dir = process_remaining(df_remaining)

        print(f"\n{'='*60}")
        print("‚úÖ RESUMED PROCESSING COMPLETE!")
        print(f"{'='*60}")
        print(f"\nüìÅ New results: {session_dir}")
        print(f"\nüí° To merge with previous results:")
        print(f"   1. Load previous session results")
        print(f"   2. Load this session results")
        print(f"   3. Concatenate dataframes")
        print(f"   4. Sort by row_id")

"""
BEAD ANALYSIS RESUME SCRIPT - WITH EXCEL CHARACTER FIX
Fixes the IllegalCharacterError by sanitizing text before saving
Continues from last successful batch save
"""

# ============================================================================
# STEP 1: INSTALL AND IMPORT
# ============================================================================

print("üì¶ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "anthropic", "pandas", "openpyxl", "-q"])

import anthropic
import pandas as pd
import json
import time
import os
import re
from datetime import datetime
from typing import Dict, List, Any
from glob import glob

print("‚úÖ Packages installed\n")

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================

print("üìÅ Mounting Google Drive...")
from google.colab import drive
drive.mount('/content/drive')

DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Bead_Analysis_v5_Results'
os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Google Drive mounted: {DRIVE_OUTPUT_DIR}\n")

# ============================================================================
# STEP 3: CONFIGURATION
# ============================================================================

ANTHROPIC_API_KEY = "YOUR API KEY HERE"

CONFIG = {
    'model': 'claude-sonnet-4-20250514',
    'max_tokens': 6000,
    'temperature': 0.0,
    'sample_size': None,
    'confidence_threshold': 0.65,
    'delay_between_calls': 0.2,
    'batch_save_frequency': 50,
    'input_filename': 'manufacturing_rerun_january_2026.xlsx',
    'text_column': 'text_page_gp',
    'id_column': 'snippet_ID',
    'drive_output_dir': DRIVE_OUTPUT_DIR,
}

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
print("‚úÖ Configuration loaded")
print(f"   Model: {CONFIG['model']}")
print(f"   With EXCEL CHARACTER FIX\n")

# ============================================================================
# STEP 4: EXCEL CHARACTER SANITIZATION (NEW!)
# ============================================================================

def sanitize_for_excel(text):
    """
    Remove illegal characters that Excel doesn't allow in cells
    This prevents IllegalCharacterError when saving
    """
    if pd.isna(text) or text is None:
        return text

    text = str(text)

    # Remove control characters (except tab, newline, carriage return)
    # Excel allows: tab (\t), newline (\n), carriage return (\r)
    # Excel rejects: other control characters (0x00-0x1F except 0x09, 0x0A, 0x0D)
    cleaned = []
    for char in text:
        code = ord(char)
        # Keep printable characters, tab, newline, carriage return
        if code >= 0x20 or code in [0x09, 0x0A, 0x0D]:
            cleaned.append(char)
        # Replace other control characters with space
        else:
            cleaned.append(' ')

    return ''.join(cleaned)

def sanitize_dict(result_dict):
    """
    Sanitize all string values in a dictionary for Excel compatibility
    """
    sanitized = {}
    for key, value in result_dict.items():
        if isinstance(value, str):
            sanitized[key] = sanitize_for_excel(value)
        else:
            sanitized[key] = value
    return sanitized

print("‚úÖ Excel character sanitization enabled\n")

# ============================================================================
# STEP 5: CODEBOOK PROMPT
# ============================================================================

CODEBOOK_V5_PROMPT = '''You are an expert historical coder analyzing texts about precolonial African bead exchange systems.

CRITICAL INSTRUCTIONS:
- Code ONLY what is EXPLICITLY stated in the text
- Do NOT infer, assume, or interpret beyond what is directly written
- When uncertain, be CONSERVATIVE and code as "unclear" or "N/A"
- Follow all decision rules strictly

PREPROCESSING FILTERS (Apply First):

P1: Text Length
- If text < 50 readable characters ‚Üí read_entry = "NaN", STOP coding
- If single character entry ‚Üí read_entry = "NaN", STOP coding

P2: Content Relevance
- If context/title page without bead content ‚Üí read_entry = "NaN", STOP coding
- Must have actual bead-related content, not just headers

P3: OCR Quality
- If 60%+ non-standard characters ‚Üí read_entry = "NaN", STOP coding
- Excessive symbols (####, |||) ‚Üí read_entry = "NaN", STOP coding

EXPERT DECISION RULES:

E1: Historical Generalizations
- Example: "natives formerly sold ivory for beads"
- Code: exchange = "no" (general pattern, not specific instance)
- BUT capture all details in other variables

E2: Intangible Exchanges
- Example: "bought secret for beads"
- Code: exchange = "yes" (knowledge/services are valid exchanges)

E3: Observational Contexts
- Example: "showed us his ceremonial beads"
- Code: exchange = "no" (observation, not transaction)
- BUT code functions if mentioned

E4: Gift-Giving
- Example: "offered presents of beads"
- Code: exchange = "yes", nature_of_exchange = "positive"
- Gifts are social exchanges

E5: Equipment/Tool Beads
- Example: "beads on horse saddle"
- Code: bead_function = "aesthetic", exchange = "no"

VARIABLES TO CODE (15 total):

1. read_entry: full text content OR "NaN" if fails P1-P3
2. exchange: "yes" = specific exchange/trade/purchase/gift; "no" = no exchange OR general statements OR observations
3. bead_function: aesthetic, currency, valuable, spiritual, social_marker, magical (can be multiple, comma-separated)
4. bead_material: glass, stone, metal, organic, mixed (ONLY if explicitly stated)
5. bead_color: exact color terms in quotes (N/A if not mentioned)
6. bead_shape: round, cylindrical, tubular, disk, or exact terms from text
7. price_of_bead: all pricing info, exact amounts, relative values, exchange rates
8. beads_exchanged: brief description of beads in transaction
9. other_exchanged: what was traded for beads (items/services)
10. process_of_exchange: how exchange occurred (market, ceremonial, diplomatic)
11. ethnic_group: exact names from text (multiple: list all)
12. nature_of_exchange: "positive", "conflictual", "neutral", "N/A"
13. time_reference: "contemporary", "historical", "hypothetical", "mixed"
14. manufactured_locally: "yes" = local manufacturing; "no" = imported; "unclear" = ambiguous; "N/A" = archaeological only
15. ancient_archaeological: "yes" = ONLY actual excavation/burial contexts; "no" = ALL currency/trade observations including historical narratives; "unclear" = museum without excavation context
16. confidence_score: 0.0 to 1.0

CRITICAL FOR VARIABLE 15:
- "yes" ONLY for: "excavated at", "dug up from", "found in tombs", "recovered from burials", "archaeological site"
- "no" for: Historical trade narratives, preserved customs, oral traditions, "lost manufacturing art", ANY observation about currency use

OUTPUT FORMAT: Return ONLY valid JSON with all 16 fields.

Now analyze this text:'''

print("‚úÖ Codebook prompt loaded\n")

# ============================================================================
# STEP 6: JSON EXTRACTION & HELPER FUNCTIONS
# ============================================================================

def extract_json_robustly(response_text):
    """Extract JSON with multiple fallback strategies"""
    if "```json" in response_text:
        try:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    if "```" in response_text:
        try:
            json_text = response_text.split("```")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    try:
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start != -1 and end > start:
            json_text = response_text[start:end]
            return json.loads(json_text)
    except:
        pass

    try:
        return json.loads(response_text.strip())
    except:
        pass

    return None

def create_error_result(row_id: Any, error_type: str, error_msg: str) -> Dict[str, Any]:
    """Create standardized error result"""
    return {
        'row_id': row_id, 'read_entry': 'ERROR', 'exchange': 'ERROR',
        'bead_function': 'ERROR', 'bead_material': 'ERROR', 'bead_color': 'ERROR',
        'bead_shape': 'ERROR', 'price_of_bead': 'ERROR', 'beads_exchanged': 'ERROR',
        'other_exchanged': 'ERROR', 'process_of_exchange': 'ERROR', 'ethnic_group': 'ERROR',
        'nature_of_exchange': 'ERROR', 'time_reference': 'ERROR',
        'manufactured_locally': 'ERROR', 'ancient_archaeological': 'ERROR',
        'confidence_score': 0.0, 'edge_case_flag': True,
        'error_type': error_type, 'error_message': error_msg,
        'processing_timestamp': datetime.now().isoformat(),
        'model_used': CONFIG['model']
    }

def analyze_text(text: str, row_id: Any, max_retries: int = 1) -> Dict[str, Any]:
    """Analyze text with pre-filtering and retry logic"""

    # PRE-FILTER
    if not text or len(text.strip()) == 0 or len(text.strip()) < 50:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'empty_or_short'
        }

    non_standard = sum(1 for c in text if not c.isprintable() and c not in ['\n', '\t', '\r'])
    if len(text) > 0 and non_standard / len(text) > 0.6:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'poor_ocr'
        }

    # PROCESS WITH CLAUDE
    STRICT_JSON_PROMPT = CODEBOOK_V5_PROMPT + '''

CRITICAL OUTPUT REQUIREMENT:
- Return ONLY the JSON object
- NO explanatory text before or after
- NO markdown formatting
- Start with { and end with }
- Ensure all strings are properly escaped'''

    for attempt in range(max_retries + 1):
        try:
            prompt_to_use = STRICT_JSON_PROMPT if attempt > 0 else CODEBOOK_V5_PROMPT

            message = client.messages.create(
                model=CONFIG['model'],
                max_tokens=6000,
                temperature=0.0,
                messages=[{"role": "user", "content": f"{prompt_to_use}\n\n{text}"}]
            )

            response_text = message.content[0].text
            result = extract_json_robustly(response_text)

            if result is not None:
                if 'confidence_score' in result:
                    try:
                        result['confidence_score'] = float(result['confidence_score'])
                    except:
                        result['confidence_score'] = 0.5

                result['row_id'] = row_id
                result['processing_timestamp'] = datetime.now().isoformat()
                result['model_used'] = CONFIG['model']
                result['retry_attempt'] = attempt
                result['prefilter'] = 'passed'

                try:
                    confidence = float(result.get('confidence_score', 1.0))
                    result['edge_case_flag'] = confidence < CONFIG['confidence_threshold']
                except:
                    result['edge_case_flag'] = True

                return result

            if attempt < max_retries:
                time.sleep(0.3)

        except Exception as e:
            if attempt < max_retries:
                time.sleep(0.3)
            else:
                return create_error_result(row_id, "PROCESSING_ERROR", str(e))

    return create_error_result(row_id, "JSON_PARSE_ERROR", f"Failed after {max_retries + 1} attempts")

def run_consistency_checks(result: Dict[str, Any]) -> List[str]:
    """Run consistency checks"""
    issues = []

    if result.get('exchange') == 'no':
        fields_should_be_na = ['price_of_bead', 'other_exchanged', 'process_of_exchange', 'nature_of_exchange']
        for field in fields_should_be_na:
            if result.get(field) not in ['N/A', 'NaN', None]:
                issues.append(f"{field} should be N/A when exchange=no")

    return issues

# ============================================================================
# STEP 7: FIND LAST PROCESSED ROW (ACROSS ALL SESSIONS)
# ============================================================================

print(f"{'='*60}")
print("FINDING LAST PROCESSED ROW ACROSS ALL SESSIONS")
print(f"{'='*60}\n")

session_dirs = glob(os.path.join(DRIVE_OUTPUT_DIR, 'session_*'))
if not session_dirs:
    print("‚ùå No previous session found!")
    last_processed_row = 0
else:
    print(f"üìÅ Found {len(session_dirs)} session(s):")
    for sess in sorted(session_dirs):
        print(f"   - {os.path.basename(sess)}")

    all_batch_numbers = []

    for session_dir in session_dirs:
        batch_files = glob(os.path.join(session_dir, 'progress_batch_*.xlsx'))

        for bf in batch_files:
            try:
                num = int(os.path.basename(bf).replace('progress_batch_', '').replace('.xlsx', ''))
                all_batch_numbers.append((num, bf))
            except:
                pass

    if not all_batch_numbers:
        print("‚ùå No batch files found!")
        last_processed_row = 0
    else:
        last_batch, last_batch_file = max(all_batch_numbers, key=lambda x: x[0])
        last_processed_row = last_batch

        print(f"\n‚úÖ Highest batch found: progress_batch_{last_batch}.xlsx")
        print(f"   Location: {last_batch_file}")
        print(f"‚úÖ Last processed row: {last_processed_row}")
        print(f"üîÑ Will resume from row: {last_processed_row + 1}")

        total_rows = 27060
        pct_complete = (last_processed_row / total_rows) * 100
        remaining = total_rows - last_processed_row
        print(f"\nüìä OVERALL PROGRESS:")
        print(f"   Completed: {last_processed_row:,} / {total_rows:,} ({pct_complete:.1f}%)")
        print(f"   Remaining: {remaining:,} rows")

        rows_per_12h = int(0.11 * 60 * 60 * 12)
        sessions_remaining = int(remaining / rows_per_12h) + 1
        print(f"\n‚è±Ô∏è ESTIMATED RESTARTS:")
        print(f"   ~{rows_per_12h:,} rows per 12-hour session")
        print(f"   ~{sessions_remaining} more session(s) needed")

# ============================================================================
# STEP 8: LOAD DATA AND FILTER
# ============================================================================

print(f"\n{'='*60}")
print("LOADING DATA")
print(f"{'='*60}\n")

filename = os.path.join(CONFIG['drive_output_dir'], CONFIG['input_filename'])

if not os.path.exists(filename):
    print(f"‚ùå File not found: {filename}")
else:
    df = pd.read_excel(filename)
    print(f"‚úÖ Loaded {len(df)} total rows")

    if CONFIG['id_column'] not in df.columns:
        print(f"‚ö†Ô∏è  Creating sequential IDs...")
        df[CONFIG['id_column']] = range(1, len(df) + 1)

    df_remaining = df[df[CONFIG['id_column']] > last_processed_row].copy()

    print(f"\nüìä PROCESSING PLAN:")
    print(f"   Total rows in dataset: {len(df)}")
    print(f"   Already processed: {last_processed_row}")
    print(f"   Remaining to process: {len(df_remaining)}")
    if len(df_remaining) > 0:
        print(f"   Starting from row ID: {df_remaining.iloc[0][CONFIG['id_column']]}")

    # ========================================================================
    # STEP 9: RESUME PROCESSING WITH EXCEL FIX
    # ========================================================================

    if len(df_remaining) == 0:
        print("\n‚úÖ All rows already processed!")
    else:
        def process_remaining(df_subset: pd.DataFrame) -> tuple:
            """Process remaining rows with Excel character sanitization"""
            results = []
            total_rows = len(df_subset)
            edge_cases = []
            consistency_warnings = []

            session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_dir = os.path.join(CONFIG['drive_output_dir'], f'session_RESUMED_{session_timestamp}')
            os.makedirs(session_dir, exist_ok=True)

            print(f"\n{'='*60}")
            print("RESUMING ANALYSIS (WITH EXCEL FIX)")
            print(f"{'='*60}\n")
            print(f"üöÄ Processing {total_rows} remaining rows...")
            print(f"üìÅ New session: {session_dir}")
            print(f"üíæ Saves every {CONFIG['batch_save_frequency']} rows")
            print(f"üîß Excel character sanitization ENABLED\n")

            start_time = time.time()

            for idx, row in df_subset.iterrows():
                if (len(results) + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = len(results) / elapsed if elapsed > 0 else 0
                    remaining_time = (total_rows - len(results)) / rate if rate > 0 else 0
                    print(f"Progress: {len(results)}/{total_rows} ({len(results)/total_rows*100:.1f}%) - "
                          f"Rate: {rate:.2f} rows/sec - ETA: {remaining_time/60:.1f} min")

                text_value = row[CONFIG['text_column']]
                row_id = row[CONFIG['id_column']]

                if pd.isna(text_value):
                    text = ""
                else:
                    text = str(text_value).strip()

                result = analyze_text(text, row_id)

                # SANITIZE ALL STRING VALUES FOR EXCEL
                result = sanitize_dict(result)

                issues = run_consistency_checks(result)
                if issues:
                    result['consistency_warnings'] = '; '.join(issues)
                    consistency_warnings.append({'row_id': row_id, 'issues': issues})

                if result.get('edge_case_flag'):
                    try:
                        conf_score = float(result.get('confidence_score', 0))
                    except:
                        conf_score = 0.0
                    edge_cases.append({
                        'row_id': row_id,
                        'confidence': conf_score,
                        'text_preview': sanitize_for_excel(text[:100]) if text else "(empty)"
                    })

                results.append(result)
                time.sleep(CONFIG['delay_between_calls'])

                # Batch save with sanitization
                if len(results) % CONFIG['batch_save_frequency'] == 0:
                    temp_df = pd.DataFrame(results)
                    # Sanitize entire dataframe before saving
                    for col in temp_df.columns:
                        if temp_df[col].dtype == 'object':
                            temp_df[col] = temp_df[col].apply(sanitize_for_excel)

                    batch_num = last_processed_row + len(results)
                    batch_filename = os.path.join(session_dir, f'progress_batch_{batch_num}.xlsx')

                    try:
                        temp_df.to_excel(batch_filename, index=False)
                        print(f"üíæ Saved: progress_batch_{batch_num}.xlsx")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Save error at batch {batch_num}: {e}")
                        print(f"   Attempting with additional sanitization...")
                        # Extra sanitization pass
                        for col in temp_df.columns:
                            temp_df[col] = temp_df[col].astype(str).str.replace(r'[\x00-\x1f\x7f-\x9f]', ' ', regex=True)
                        temp_df.to_excel(batch_filename, index=False)
                        print(f"üíæ Saved with extra sanitization")

            # Final save
            total_time = time.time() - start_time
            results_df = pd.DataFrame(results)

            # Final sanitization
            for col in results_df.columns:
                if results_df[col].dtype == 'object':
                    results_df[col] = results_df[col].apply(sanitize_for_excel)

            print(f"\n{'='*60}")
            print("RESUMED ANALYSIS COMPLETE")
            print(f"{'='*60}")
            print(f"‚è±Ô∏è  Time: {total_time/60:.1f} minutes")
            print(f"‚ö° Rate: {total_rows/total_time:.2f} rows/second")
            print(f"üîç Edge cases: {len(edge_cases)}")

            final_filename = os.path.join(session_dir, f'RESUMED_results_{session_timestamp}.xlsx')
            results_df.to_excel(final_filename, index=False)
            print(f"\nüíæ Saved: {final_filename}")

            if edge_cases:
                edge_df = pd.DataFrame(edge_cases)
                for col in edge_df.columns:
                    if edge_df[col].dtype == 'object':
                        edge_df[col] = edge_df[col].apply(sanitize_for_excel)
                edge_filename = os.path.join(session_dir, f'edge_cases_{session_timestamp}.xlsx')
                edge_df.to_excel(edge_filename, index=False)
                print(f"üíæ Edge cases: {edge_filename}")

            print(f"\nüìä CODING SUMMARY:")
            print(f"   Exchange='yes': {(results_df['exchange']=='yes').sum()}")
            print(f"   Exchange='no': {(results_df['exchange']=='no').sum()}")
            print(f"   Errors: {(results_df['read_entry']=='ERROR').sum()}")

            return results_df, edge_cases, consistency_warnings, session_dir

        print("\n‚ö†Ô∏è  SECURITY REMINDER:")
        print("After completion, regenerate API key at:")
        print("https://console.anthropic.com/settings/keys\n")

        results_df, edge_cases, consistency_warnings, session_dir = process_remaining(df_remaining)

        print(f"\n{'='*60}")
        print("‚úÖ PROCESSING COMPLETE!")
        print(f"{'='*60}")
        print(f"\nüìÅ Results: {session_dir}")

"""
BEAD ANALYSIS RESUME SCRIPT - WITH EXCEL CHARACTER FIX
Fixes the IllegalCharacterError by sanitizing text before saving
Continues from last successful batch save
"""

# ============================================================================
# STEP 1: INSTALL AND IMPORT
# ============================================================================

print("üì¶ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "anthropic", "pandas", "openpyxl", "-q"])

import anthropic
import pandas as pd
import json
import time
import os
import re
from datetime import datetime
from typing import Dict, List, Any
from glob import glob

print("‚úÖ Packages installed\n")

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================

print("üìÅ Mounting Google Drive...")
from google.colab import drive
drive.mount('/content/drive')

DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Bead_Analysis_v5_Results'
os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ Google Drive mounted: {DRIVE_OUTPUT_DIR}\n")

# ============================================================================
# STEP 3: CONFIGURATION
# ============================================================================

ANTHROPIC_API_KEY = "YOUR API KEY HERE"

CONFIG = {
    'model': 'claude-sonnet-4-20250514',
    'max_tokens': 6000,
    'temperature': 0.0,
    'sample_size': None,
    'confidence_threshold': 0.65,
    'delay_between_calls': 0.2,
    'batch_save_frequency': 50,
    'input_filename': 'manufacturing_rerun_january_2026.xlsx',
    'text_column': 'text_page_gp',
    'id_column': 'snippet_ID',
    'drive_output_dir': DRIVE_OUTPUT_DIR,
}

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
print("‚úÖ Configuration loaded")
print(f"   Model: {CONFIG['model']}")
print(f"   With EXCEL CHARACTER FIX\n")

# ============================================================================
# STEP 4: EXCEL CHARACTER SANITIZATION (NEW!)
# ============================================================================

def sanitize_for_excel(text):
    """
    Remove illegal characters that Excel doesn't allow in cells
    This prevents IllegalCharacterError when saving
    """
    if pd.isna(text) or text is None:
        return text

    text = str(text)

    # Remove control characters (except tab, newline, carriage return)
    # Excel allows: tab (\t), newline (\n), carriage return (\r)
    # Excel rejects: other control characters (0x00-0x1F except 0x09, 0x0A, 0x0D)
    cleaned = []
    for char in text:
        code = ord(char)
        # Keep printable characters, tab, newline, carriage return
        if code >= 0x20 or code in [0x09, 0x0A, 0x0D]:
            cleaned.append(char)
        # Replace other control characters with space
        else:
            cleaned.append(' ')

    return ''.join(cleaned)

def sanitize_dict(result_dict):
    """
    Sanitize all string values in a dictionary for Excel compatibility
    """
    sanitized = {}
    for key, value in result_dict.items():
        if isinstance(value, str):
            sanitized[key] = sanitize_for_excel(value)
        else:
            sanitized[key] = value
    return sanitized

print("‚úÖ Excel character sanitization enabled\n")

# ============================================================================
# STEP 5: CODEBOOK PROMPT
# ============================================================================

CODEBOOK_V5_PROMPT = '''You are an expert historical coder analyzing texts about precolonial African bead exchange systems.

CRITICAL INSTRUCTIONS:
- Code ONLY what is EXPLICITLY stated in the text
- Do NOT infer, assume, or interpret beyond what is directly written
- When uncertain, be CONSERVATIVE and code as "unclear" or "N/A"
- Follow all decision rules strictly

PREPROCESSING FILTERS (Apply First):

P1: Text Length
- If text < 50 readable characters ‚Üí read_entry = "NaN", STOP coding
- If single character entry ‚Üí read_entry = "NaN", STOP coding

P2: Content Relevance
- If context/title page without bead content ‚Üí read_entry = "NaN", STOP coding
- Must have actual bead-related content, not just headers

P3: OCR Quality
- If 60%+ non-standard characters ‚Üí read_entry = "NaN", STOP coding
- Excessive symbols (####, |||) ‚Üí read_entry = "NaN", STOP coding

EXPERT DECISION RULES:

E1: Historical Generalizations
- Example: "natives formerly sold ivory for beads"
- Code: exchange = "no" (general pattern, not specific instance)
- BUT capture all details in other variables

E2: Intangible Exchanges
- Example: "bought secret for beads"
- Code: exchange = "yes" (knowledge/services are valid exchanges)

E3: Observational Contexts
- Example: "showed us his ceremonial beads"
- Code: exchange = "no" (observation, not transaction)
- BUT code functions if mentioned

E4: Gift-Giving
- Example: "offered presents of beads"
- Code: exchange = "yes", nature_of_exchange = "positive"
- Gifts are social exchanges

E5: Equipment/Tool Beads
- Example: "beads on horse saddle"
- Code: bead_function = "aesthetic", exchange = "no"

VARIABLES TO CODE (15 total):

1. read_entry: full text content OR "NaN" if fails P1-P3
2. exchange: "yes" = specific exchange/trade/purchase/gift; "no" = no exchange OR general statements OR observations
3. bead_function: aesthetic, currency, valuable, spiritual, social_marker, magical (can be multiple, comma-separated)
4. bead_material: glass, stone, metal, organic, mixed (ONLY if explicitly stated)
5. bead_color: exact color terms in quotes (N/A if not mentioned)
6. bead_shape: round, cylindrical, tubular, disk, or exact terms from text
7. price_of_bead: all pricing info, exact amounts, relative values, exchange rates
8. beads_exchanged: brief description of beads in transaction
9. other_exchanged: what was traded for beads (items/services)
10. process_of_exchange: how exchange occurred (market, ceremonial, diplomatic)
11. ethnic_group: exact names from text (multiple: list all)
12. nature_of_exchange: "positive", "conflictual", "neutral", "N/A"
13. time_reference: "contemporary", "historical", "hypothetical", "mixed"
14. manufactured_locally: "yes" = local manufacturing; "no" = imported; "unclear" = ambiguous; "N/A" = archaeological only
15. ancient_archaeological: "yes" = ONLY actual excavation/burial contexts; "no" = ALL currency/trade observations including historical narratives; "unclear" = museum without excavation context
16. confidence_score: 0.0 to 1.0

CRITICAL FOR VARIABLE 15:
- "yes" ONLY for: "excavated at", "dug up from", "found in tombs", "recovered from burials", "archaeological site"
- "no" for: Historical trade narratives, preserved customs, oral traditions, "lost manufacturing art", ANY observation about currency use

OUTPUT FORMAT: Return ONLY valid JSON with all 16 fields.

Now analyze this text:'''

print("‚úÖ Codebook prompt loaded\n")

# ============================================================================
# STEP 6: JSON EXTRACTION & HELPER FUNCTIONS
# ============================================================================

def extract_json_robustly(response_text):
    """Extract JSON with multiple fallback strategies"""
    if "```json" in response_text:
        try:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    if "```" in response_text:
        try:
            json_text = response_text.split("```")[1].split("```")[0].strip()
            return json.loads(json_text)
        except:
            pass

    try:
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except:
        pass

    try:
        start = response_text.find('{')
        end = response_text.rfind('}') + 1
        if start != -1 and end > start:
            json_text = response_text[start:end]
            return json.loads(json_text)
    except:
        pass

    try:
        return json.loads(response_text.strip())
    except:
        pass

    return None

def create_error_result(row_id: Any, error_type: str, error_msg: str) -> Dict[str, Any]:
    """Create standardized error result"""
    return {
        'row_id': row_id, 'read_entry': 'ERROR', 'exchange': 'ERROR',
        'bead_function': 'ERROR', 'bead_material': 'ERROR', 'bead_color': 'ERROR',
        'bead_shape': 'ERROR', 'price_of_bead': 'ERROR', 'beads_exchanged': 'ERROR',
        'other_exchanged': 'ERROR', 'process_of_exchange': 'ERROR', 'ethnic_group': 'ERROR',
        'nature_of_exchange': 'ERROR', 'time_reference': 'ERROR',
        'manufactured_locally': 'ERROR', 'ancient_archaeological': 'ERROR',
        'confidence_score': 0.0, 'edge_case_flag': True,
        'error_type': error_type, 'error_message': error_msg,
        'processing_timestamp': datetime.now().isoformat(),
        'model_used': CONFIG['model']
    }

def analyze_text(text: str, row_id: Any, max_retries: int = 1) -> Dict[str, Any]:
    """Analyze text with pre-filtering and retry logic"""

    # PRE-FILTER
    if not text or len(text.strip()) == 0 or len(text.strip()) < 50:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'empty_or_short'
        }

    non_standard = sum(1 for c in text if not c.isprintable() and c not in ['\n', '\t', '\r'])
    if len(text) > 0 and non_standard / len(text) > 0.6:
        return {
            'row_id': row_id, 'read_entry': 'NaN', 'exchange': 'NaN',
            'bead_function': 'NaN', 'bead_material': 'NaN', 'bead_color': 'NaN',
            'bead_shape': 'NaN', 'price_of_bead': 'NaN', 'beads_exchanged': 'NaN',
            'other_exchanged': 'NaN', 'process_of_exchange': 'NaN', 'ethnic_group': 'NaN',
            'nature_of_exchange': 'NaN', 'time_reference': 'NaN',
            'manufactured_locally': 'NaN', 'ancient_archaeological': 'NaN',
            'confidence_score': 1.0, 'edge_case_flag': False,
            'processing_timestamp': datetime.now().isoformat(),
            'model_used': CONFIG['model'], 'retry_attempt': 0,
            'prefilter': 'poor_ocr'
        }

    # PROCESS WITH CLAUDE
    STRICT_JSON_PROMPT = CODEBOOK_V5_PROMPT + '''

CRITICAL OUTPUT REQUIREMENT:
- Return ONLY the JSON object
- NO explanatory text before or after
- NO markdown formatting
- Start with { and end with }
- Ensure all strings are properly escaped'''

    for attempt in range(max_retries + 1):
        try:
            prompt_to_use = STRICT_JSON_PROMPT if attempt > 0 else CODEBOOK_V5_PROMPT

            message = client.messages.create(
                model=CONFIG['model'],
                max_tokens=6000,
                temperature=0.0,
                messages=[{"role": "user", "content": f"{prompt_to_use}\n\n{text}"}]
            )

            response_text = message.content[0].text
            result = extract_json_robustly(response_text)

            if result is not None:
                if 'confidence_score' in result:
                    try:
                        result['confidence_score'] = float(result['confidence_score'])
                    except:
                        result['confidence_score'] = 0.5

                result['row_id'] = row_id
                result['processing_timestamp'] = datetime.now().isoformat()
                result['model_used'] = CONFIG['model']
                result['retry_attempt'] = attempt
                result['prefilter'] = 'passed'

                try:
                    confidence = float(result.get('confidence_score', 1.0))
                    result['edge_case_flag'] = confidence < CONFIG['confidence_threshold']
                except:
                    result['edge_case_flag'] = True

                return result

            if attempt < max_retries:
                time.sleep(0.3)

        except Exception as e:
            if attempt < max_retries:
                time.sleep(0.3)
            else:
                return create_error_result(row_id, "PROCESSING_ERROR", str(e))

    return create_error_result(row_id, "JSON_PARSE_ERROR", f"Failed after {max_retries + 1} attempts")

def run_consistency_checks(result: Dict[str, Any]) -> List[str]:
    """Run consistency checks"""
    issues = []

    if result.get('exchange') == 'no':
        fields_should_be_na = ['price_of_bead', 'other_exchanged', 'process_of_exchange', 'nature_of_exchange']
        for field in fields_should_be_na:
            if result.get(field) not in ['N/A', 'NaN', None]:
                issues.append(f"{field} should be N/A when exchange=no")

    return issues

# ============================================================================
# STEP 7: FIND LAST PROCESSED ROW (ACROSS ALL SESSIONS)
# ============================================================================

print(f"{'='*60}")
print("FINDING LAST PROCESSED ROW ACROSS ALL SESSIONS")
print(f"{'='*60}\n")

session_dirs = glob(os.path.join(DRIVE_OUTPUT_DIR, 'session_*'))
if not session_dirs:
    print("‚ùå No previous session found!")
    last_processed_row = 0
else:
    print(f"üìÅ Found {len(session_dirs)} session(s):")
    for sess in sorted(session_dirs):
        print(f"   - {os.path.basename(sess)}")

    all_batch_numbers = []

    for session_dir in session_dirs:
        batch_files = glob(os.path.join(session_dir, 'progress_batch_*.xlsx'))

        for bf in batch_files:
            try:
                num = int(os.path.basename(bf).replace('progress_batch_', '').replace('.xlsx', ''))
                all_batch_numbers.append((num, bf))
            except:
                pass

    if not all_batch_numbers:
        print("‚ùå No batch files found!")
        last_processed_row = 0
    else:
        last_batch, last_batch_file = max(all_batch_numbers, key=lambda x: x[0])
        last_processed_row = last_batch

        print(f"\n‚úÖ Highest batch found: progress_batch_{last_batch}.xlsx")
        print(f"   Location: {last_batch_file}")
        print(f"‚úÖ Last processed row: {last_processed_row}")
        print(f"üîÑ Will resume from row: {last_processed_row + 1}")

        total_rows = 27060
        pct_complete = (last_processed_row / total_rows) * 100
        remaining = total_rows - last_processed_row
        print(f"\nüìä OVERALL PROGRESS:")
        print(f"   Completed: {last_processed_row:,} / {total_rows:,} ({pct_complete:.1f}%)")
        print(f"   Remaining: {remaining:,} rows")

        rows_per_12h = int(0.11 * 60 * 60 * 12)
        sessions_remaining = int(remaining / rows_per_12h) + 1
        print(f"\n‚è±Ô∏è ESTIMATED RESTARTS:")
        print(f"   ~{rows_per_12h:,} rows per 12-hour session")
        print(f"   ~{sessions_remaining} more session(s) needed")

# ============================================================================
# STEP 8: LOAD DATA AND FILTER
# ============================================================================

print(f"\n{'='*60}")
print("LOADING DATA")
print(f"{'='*60}\n")

filename = os.path.join(CONFIG['drive_output_dir'], CONFIG['input_filename'])

if not os.path.exists(filename):
    print(f"‚ùå File not found: {filename}")
else:
    df = pd.read_excel(filename)
    print(f"‚úÖ Loaded {len(df)} total rows")

    if CONFIG['id_column'] not in df.columns:
        print(f"‚ö†Ô∏è  Creating sequential IDs...")
        df[CONFIG['id_column']] = range(1, len(df) + 1)

    df_remaining = df[df[CONFIG['id_column']] > last_processed_row].copy()

    print(f"\nüìä PROCESSING PLAN:")
    print(f"   Total rows in dataset: {len(df)}")
    print(f"   Already processed: {last_processed_row}")
    print(f"   Remaining to process: {len(df_remaining)}")
    if len(df_remaining) > 0:
        print(f"   Starting from row ID: {df_remaining.iloc[0][CONFIG['id_column']]}")

    # ========================================================================
    # STEP 9: RESUME PROCESSING WITH EXCEL FIX
    # ========================================================================

    if len(df_remaining) == 0:
        print("\n‚úÖ All rows already processed!")
    else:
        def process_remaining(df_subset: pd.DataFrame) -> tuple:
            """Process remaining rows with Excel character sanitization"""
            results = []
            total_rows = len(df_subset)
            edge_cases = []
            consistency_warnings = []

            session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_dir = os.path.join(CONFIG['drive_output_dir'], f'session_RESUMED_{session_timestamp}')
            os.makedirs(session_dir, exist_ok=True)

            print(f"\n{'='*60}")
            print("RESUMING ANALYSIS (WITH EXCEL FIX)")
            print(f"{'='*60}\n")
            print(f"üöÄ Processing {total_rows} remaining rows...")
            print(f"üìÅ New session: {session_dir}")
            print(f"üíæ Saves every {CONFIG['batch_save_frequency']} rows")
            print(f"üîß Excel character sanitization ENABLED\n")

            start_time = time.time()

            for idx, row in df_subset.iterrows():
                if (len(results) + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = len(results) / elapsed if elapsed > 0 else 0
                    remaining_time = (total_rows - len(results)) / rate if rate > 0 else 0
                    print(f"Progress: {len(results)}/{total_rows} ({len(results)/total_rows*100:.1f}%) - "
                          f"Rate: {rate:.2f} rows/sec - ETA: {remaining_time/60:.1f} min")

                text_value = row[CONFIG['text_column']]
                row_id = row[CONFIG['id_column']]

                if pd.isna(text_value):
                    text = ""
                else:
                    text = str(text_value).strip()

                result = analyze_text(text, row_id)

                # SANITIZE ALL STRING VALUES FOR EXCEL
                result = sanitize_dict(result)

                issues = run_consistency_checks(result)
                if issues:
                    result['consistency_warnings'] = '; '.join(issues)
                    consistency_warnings.append({'row_id': row_id, 'issues': issues})

                if result.get('edge_case_flag'):
                    try:
                        conf_score = float(result.get('confidence_score', 0))
                    except:
                        conf_score = 0.0
                    edge_cases.append({
                        'row_id': row_id,
                        'confidence': conf_score,
                        'text_preview': sanitize_for_excel(text[:100]) if text else "(empty)"
                    })

                results.append(result)
                time.sleep(CONFIG['delay_between_calls'])

                # Batch save with sanitization
                if len(results) % CONFIG['batch_save_frequency'] == 0:
                    temp_df = pd.DataFrame(results)
                    # Sanitize entire dataframe before saving
                    for col in temp_df.columns:
                        if temp_df[col].dtype == 'object':
                            temp_df[col] = temp_df[col].apply(sanitize_for_excel)

                    batch_num = last_processed_row + len(results)
                    batch_filename = os.path.join(session_dir, f'progress_batch_{batch_num}.xlsx')

                    try:
                        temp_df.to_excel(batch_filename, index=False)
                        print(f"üíæ Saved: progress_batch_{batch_num}.xlsx")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Save error at batch {batch_num}: {e}")
                        print(f"   Attempting with additional sanitization...")
                        # Extra sanitization pass
                        for col in temp_df.columns:
                            temp_df[col] = temp_df[col].astype(str).str.replace(r'[\x00-\x1f\x7f-\x9f]', ' ', regex=True)
                        temp_df.to_excel(batch_filename, index=False)
                        print(f"üíæ Saved with extra sanitization")

            # Final save
            total_time = time.time() - start_time
            results_df = pd.DataFrame(results)

            # Final sanitization
            for col in results_df.columns:
                if results_df[col].dtype == 'object':
                    results_df[col] = results_df[col].apply(sanitize_for_excel)

            print(f"\n{'='*60}")
            print("RESUMED ANALYSIS COMPLETE")
            print(f"{'='*60}")
            print(f"‚è±Ô∏è  Time: {total_time/60:.1f} minutes")
            print(f"‚ö° Rate: {total_rows/total_time:.2f} rows/second")
            print(f"üîç Edge cases: {len(edge_cases)}")

            final_filename = os.path.join(session_dir, f'RESUMED_results_{session_timestamp}.xlsx')
            results_df.to_excel(final_filename, index=False)
            print(f"\nüíæ Saved: {final_filename}")

            if edge_cases:
                edge_df = pd.DataFrame(edge_cases)
                for col in edge_df.columns:
                    if edge_df[col].dtype == 'object':
                        edge_df[col] = edge_df[col].apply(sanitize_for_excel)
                edge_filename = os.path.join(session_dir, f'edge_cases_{session_timestamp}.xlsx')
                edge_df.to_excel(edge_filename, index=False)
                print(f"üíæ Edge cases: {edge_filename}")

            print(f"\nüìä CODING SUMMARY:")
            print(f"   Exchange='yes': {(results_df['exchange']=='yes').sum()}")
            print(f"   Exchange='no': {(results_df['exchange']=='no').sum()}")
            print(f"   Errors: {(results_df['read_entry']=='ERROR').sum()}")

            return results_df, edge_cases, consistency_warnings, session_dir

        print("\n‚ö†Ô∏è  SECURITY REMINDER:")
        print("After completion, regenerate API key at:")
        print("https://console.anthropic.com/settings/keys\n")

        results_df, edge_cases, consistency_warnings, session_dir = process_remaining(df_remaining)

        print(f"\n{'='*60}")
        print("‚úÖ PROCESSING COMPLETE!")
        print(f"{'='*60}")
        print(f"\nüìÅ Results: {session_dir}")
