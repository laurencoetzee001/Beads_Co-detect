{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# enhanced_bead_coding_structured.py\n",
        "\"\"\"\n",
        "Enhanced Bead Trade Coding Notebook - Structured Output Version\n",
        "================================================================\n",
        "\n",
        "Key Improvements:\n",
        "- Dual column approach: codebook values + descriptive details\n",
        "- Clean, non-nested output structure (~30 columns)\n",
        "- XML to JSON transformation in 2 steps\n",
        "- Claude Sonnet 4 with temperature 0.2\n",
        "- Edge cases handled\n",
        "- \"Beads observed\" for non-exchange mentions\n",
        "- No nested columns, clean statistical output\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# === INSTALL DEPENDENCIES IF NEEDED ===\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "for pkg in [\"anthropic\", \"openpyxl\", \"xlrd\", \"pandas\"]:\n",
        "    try:\n",
        "        __import__(pkg if pkg != \"openpyxl\" else \"openpyxl\")\n",
        "    except ImportError:\n",
        "        install(pkg)\n",
        "\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "import json\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# === GOOGLE DRIVE MOUNT ===\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "timestamp_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "GDRIVE_DIR = f\"/content/drive/MyDrive/bead_annotation/structured_session_{timestamp_str}\"\n",
        "os.makedirs(GDRIVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"🔬 STRUCTURED BEAD TRADE CODING - V3\")\n",
        "print(\"=\" * 50)\n",
        "print(\"🎯 Goal: Clean structured output with descriptive details\")\n",
        "print(\"🎯 Goal: Non-nested columns for statistical analysis\")\n",
        "print(\"🎯 Goal: Preserve rich historical context\")\n",
        "print(\"🎯 Using: Claude Sonnet 4 with temperature 0.2\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# === SETTINGS ===\n",
        "EXCEL_PATH = \"All_entries_beads_cleaned.xlsx\"\n",
        "ANTHROPIC_API_KEY = \"YourAPI Key Here"\n",
        "MODEL_NAME = \"claude-sonnet-4-20250514\"  # Update to claude-3-5-sonnet-20250514 when available\n",
        "TEMPERATURE = 0.2  # Lower temperature for consistency\n",
        "LOG_FILE = \"structured_annotation_log.txt\"\n",
        "SAVE_EVERY = 50\n",
        "\n",
        "# Processing settings\n",
        "BATCH_SIZE = 2000  # Process 2000 rows at a time\n",
        "START_ROW = 0  # Change this to continue from a specific row (e.g., 2000, 4000, 6000)\n",
        "END_ROW = None  # Set to None to process all rows, or specify an end point\n",
        "\n",
        "# Define the specific text column to use (no auto-detection)\n",
        "TEXT_COLUMN = \"text_page_gp\"  # Explicitly set the text column\n",
        "\n",
        "# === COMPREHENSIVE STRUCTURED CODEBOOK ===\n",
        "STRUCTURED_CODEBOOK = \"\"\"\n",
        "STRUCTURED BEAD TRADE CODING CODEBOOK - EXPERT-VALIDATED VERSION\n",
        "================================================================\n",
        "Based on highest human-AI agreement rates (July 2025) with edge case improvements\n",
        "\n",
        "You are analyzing historical texts about pre-colonial African bead trade.\n",
        "Extract information for EVERY variable below, providing both coded values and descriptive details.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Answer ALL variables in the order listed below\n",
        "2. Use \"none\" or \"no\" if information is not present\n",
        "3. Base answers ONLY on the text provided (temperature 0.2)\n",
        "4. Be CONSERVATIVE - require explicit evidence, do NOT infer\n",
        "5. The decision tree is for thinking only - still answer every variable\n",
        "6. Provide both coded answer and descriptive details for each variable\n",
        "\n",
        "### DATA QUALITY PRE-CHECK:\n",
        "Before coding, check if the text:\n",
        "- Has <50 readable characters → Mark as \"data_quality_issue\"\n",
        "- Is corrupted OCR (mostly symbols/numbers) → Mark as \"corrupted_ocr\"\n",
        "- Is a context page without bead content → Mark as \"context_page_only\"\n",
        "- Contains no relevant bead information → Mark as \"no_bead_content\"\n",
        "\n",
        "### COMPLETE VARIABLE LIST (ALL MUST BE ANSWERED):\n",
        "\n",
        "1. **4a_exchange**: Were beads exchanged in a specific transaction?\n",
        "   - CODE VALUES: \"no\" or \"xo\"\n",
        "   - \"xo\" = ONLY when text explicitly describes beads being traded/bartered/sold for something specific\n",
        "   - \"no\" = ALL other mentions including:\n",
        "     * Manufacturing/processing (\"beads melt down\", \"fashioned from glass\")\n",
        "     * Value descriptions (\"worth more than gold\", \"highly prized\")\n",
        "     * Personal adornment (\"wore strings of beads\", \"around necks\")\n",
        "     * Market displays (\"beads displayed in stalls\", \"exposed for sale\")\n",
        "     * General demand (\"beads in great demand\", \"scarce items\")\n",
        "     * Ceremonial use (\"offerings to gods\", \"ritual objects\")\n",
        "     * Payment systems (\"teaspoonful per day\", \"wages in beads\")\n",
        "     * Historical generalizations (\"natives sold ivory for beads\" without specific instance)\n",
        "     * Observational contexts (\"showed rosary beads\")\n",
        "   - SPECIAL CASES:\n",
        "     * Gift-giving with \"offered presents\" = \"xo\" (social exchange)\n",
        "     * Intangible exchanges (\"bought secret for beads\") = \"xo\"\n",
        "     * Equipment beads (saddlery) = \"no\" (aesthetic function)\n",
        "\n",
        "2. **4b_beads_exchanged**: What specific beads were exchanged?\n",
        "   - Extract: material, color, size, quantity, quality, origin\n",
        "   - If 4a_exchange=\"no\", use \"none\"\n",
        "\n",
        "3. **4c_exchanged_item**: What were beads exchanged FOR?\n",
        "   - What was received in return for beads\n",
        "   - If 4a_exchange=\"no\", use \"none\"\n",
        "\n",
        "4. **5_exchange_rate**: What was the exchange rate?\n",
        "   - Specific quantities/ratios mentioned\n",
        "   - Example: \"50 strings for one slave\"\n",
        "\n",
        "5. **6_bead_ethnic_group**: Which ethnic groups/traders were associated?\n",
        "   - List all groups, origins, trader types\n",
        "   - Include geographic origins\n",
        "\n",
        "6. **7_direction_of_trade**: What was the trade direction?\n",
        "   - Example: \"coast to interior\", \"north to south\"\n",
        "\n",
        "7. **8_location_name**: What locations are mentioned?\n",
        "   - All place names, geographic descriptors\n",
        "\n",
        "8. **9_place_of_manufacture**: Where were beads manufactured?\n",
        "   - Manufacturing locations, craft centers\n",
        "\n",
        "9. **10_beads_observed**: What beads are described (not necessarily exchanged)?\n",
        "   - ALL bead descriptions regardless of context\n",
        "   - Include beads that are worn, displayed, manufactured, valued\n",
        "   - Material, color, size, shape, quality\n",
        "\n",
        "10. **11_bead_value**: What value statements are made about beads?\n",
        "    - Price, worth, comparative value\n",
        "    - Example: \"worth more than gold\"\n",
        "\n",
        "11. **12_local_name**: Local/indigenous names for beads?\n",
        "    - Indigenous terms, meanings\n",
        "\n",
        "12. **13_notes**: Additional relevant information\n",
        "    - Context not captured elsewhere\n",
        "\n",
        "13. **14_trading_partners**: Specific individuals/groups in transactions\n",
        "    - Names, roles, relationships\n",
        "\n",
        "14. **15_temporal_context**: Time references\n",
        "    - Dates, seasons, periods mentioned\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Provide your response in XML format with both coded values and descriptive details:\n",
        "\n",
        "<analysis>\n",
        "<data_quality>\n",
        "[Check: readable/corrupted_ocr/context_page_only/no_bead_content]\n",
        "</data_quality>\n",
        "\n",
        "<thinking>\n",
        "[Your analysis process here - apply conservative decision rules]\n",
        "</thinking>\n",
        "\n",
        "<variables>\n",
        "<var_read_entry>0, 1, 2, or NaN</var_read_entry>\n",
        "<var_read_entry_detail>Entry readability and bead relevance</var_read_entry_detail>\n",
        "\n",
        "<var_1a_physical_function>2 or NA</var_1a_physical_function>\n",
        "<var_1a_physical_function_detail>Aesthetic/jewelry functions described</var_1a_physical_function_detail>\n",
        "\n",
        "<var_1b_trade_function>2 or NA</var_1b_trade_function>\n",
        "<var_1b_trade_function_detail>Currency/exchange functions described</var_1b_trade_function_detail>\n",
        "\n",
        "<var_1c_social_function>3 or NA</var_1c_social_function>\n",
        "<var_1c_social_function_detail>Ceremonial/social functions described</var_1c_social_function_detail>\n",
        "\n",
        "<var_2_nature_of_exchange>1, 2, 3, 4, or NA</var_2_nature_of_exchange>\n",
        "<var_2_nature_of_exchange_detail>Exchange dynamics if applicable</var_2_nature_of_exchange_detail>\n",
        "\n",
        "<var_3_between_groups>1, 2, 3, 4, or NA</var_3_between_groups>\n",
        "<var_3_between_groups_detail>Groups involved with names</var_3_between_groups_detail>\n",
        "\n",
        "<var_4a_exchange>no or xo</var_4a_exchange>\n",
        "<var_4a_exchange_detail>Explicit exchange evidence</var_4a_exchange_detail>\n",
        "\n",
        "<var_4b_beads_exchanged>description or none</var_4b_beads_exchanged>\n",
        "<var_4b_beads_exchanged_detail>Full bead characteristics</var_4b_beads_exchanged_detail>\n",
        "\n",
        "<var_4c_exchanged_item>item or none</var_4c_exchanged_item>\n",
        "<var_4c_exchanged_item_detail>What beads were exchanged for</var_4c_exchanged_item_detail>\n",
        "\n",
        "<var_5a_related_raw_materials>list or NA</var_5a_related_raw_materials>\n",
        "<var_5a_related_raw_materials_detail>Raw materials context</var_5a_related_raw_materials_detail>\n",
        "\n",
        "<var_5b_related_jewerly_fashion>list or NA</var_5b_related_jewerly_fashion>\n",
        "<var_5b_related_jewerly_fashion_detail>Fashion items context</var_5b_related_jewerly_fashion_detail>\n",
        "\n",
        "<var_5c_related_consumerables>list or NA</var_5c_related_consumerables>\n",
        "<var_5c_related_consumerables_detail>Consumables context</var_5c_related_consumerables_detail>\n",
        "\n",
        "<var_5d_related_decoratives>list or NA</var_5d_related_decoratives>\n",
        "<var_5d_related_decoratives_detail>Decorative items context</var_5d_related_decoratives_detail>\n",
        "\n",
        "<var_6_bead_ethnic_group>groups or none</var_6_bead_ethnic_group>\n",
        "<var_6_bead_ethnic_group_detail>Full ethnic context</var_6_bead_ethnic_group_detail>\n",
        "\n",
        "<var_7_market_town>0 or 1</var_7_market_town>\n",
        "<var_7_market_town_detail>Market designation</var_7_market_town_detail>\n",
        "\n",
        "<var_8_location_name>location or none</var_8_location_name>\n",
        "<var_8_location_name_detail>Complete geographic details</var_8_location_name_detail>\n",
        "\n",
        "<var_9_place_of_manufacture>place or none</var_9_place_of_manufacture>\n",
        "<var_9_place_of_manufacture_detail>Manufacturing origins</var_9_place_of_manufacture_detail>\n",
        "\n",
        "<var_10a_size>size or NA</var_10a_size>\n",
        "<var_10a_size_detail>Size descriptions</var_10a_size_detail>\n",
        "\n",
        "<var_10b_color>color or NA</var_10b_color>\n",
        "<var_10b_color_detail>Color descriptions</var_10b_color_detail>\n",
        "\n",
        "<var_10c_shape>shape or NA</var_10c_shape>\n",
        "<var_10c_shape_detail>Shape descriptions</var_10c_shape_detail>\n",
        "\n",
        "<var_10d_type>type or NA</var_10d_type>\n",
        "<var_10d_type_detail>Material type descriptions</var_10d_type_detail>\n",
        "\n",
        "<var_11_units_of_measurement>1, 2, 3, 4, or NA</var_11_units_of_measurement>\n",
        "<var_11_units_of_measurement_detail>Measurement methods</var_11_units_of_measurement_detail>\n",
        "\n",
        "<var_12_local_name>name or none</var_12_local_name>\n",
        "<var_12_local_name_detail>Indigenous terminology</var_12_local_name_detail>\n",
        "\n",
        "<var_13_notes>notes or none</var_13_notes>\n",
        "<var_13_notes_detail>Additional historical context</var_13_notes_detail>\n",
        "</variables>\n",
        "</analysis>\n",
        "\"\"\"\n",
        "\n",
        "# === XML to Clean JSON Parser ===\n",
        "def parse_xml_to_flat_json(xml_response):\n",
        "    \"\"\"\n",
        "    Parse XML response to flat JSON with no nesting.\n",
        "    Creates clean columns for statistical analysis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clean the response\n",
        "        xml_response = xml_response.strip()\n",
        "\n",
        "        # If it's not XML, try to extract XML from it\n",
        "        if not xml_response.startswith('<'):\n",
        "            # Look for XML content in the response\n",
        "            xml_match = re.search(r'<analysis>.*</analysis>', xml_response, re.DOTALL)\n",
        "            if xml_match:\n",
        "                xml_response = xml_match.group(0)\n",
        "            else:\n",
        "                return {\"error\": \"No XML found in response\", \"raw_response\": xml_response}\n",
        "\n",
        "        # Parse XML\n",
        "        root = ET.fromstring(xml_response)\n",
        "\n",
        "        # Initialize flat dictionary\n",
        "        flat_data = {}\n",
        "\n",
        "        # Extract variables section\n",
        "        variables = root.find('variables')\n",
        "        if variables is not None:\n",
        "            for child in variables:\n",
        "                tag_name = child.tag\n",
        "                # Remove 'var_' prefix if present for cleaner column names\n",
        "                clean_name = tag_name.replace('var_', '')\n",
        "                flat_data[clean_name] = child.text if child.text else \"none\"\n",
        "\n",
        "        return flat_data\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"error\": f\"XML parsing error: {str(e)}\",\n",
        "            \"raw_response\": xml_response[:500]  # Keep first 500 chars for debugging\n",
        "        }\n",
        "\n",
        "# === Enhanced Prompt Template ===\n",
        "def construct_structured_prompt(entry_text):\n",
        "    return f\"\"\"{STRUCTURED_CODEBOOK}\n",
        "\n",
        "IMPORTANT REMINDERS:\n",
        "- Answer EVERY variable listed above\n",
        "- Use \"none\" if information is not present\n",
        "- Base answers ONLY on this text (do not infer)\n",
        "- Provide both coded value and descriptive detail for each variable\n",
        "- Follow the exact XML format specified\n",
        "\n",
        "EXCERPT TO ANALYZE:\n",
        "{entry_text}\n",
        "\n",
        "Provide your complete XML response:\"\"\"\n",
        "\n",
        "# === Load Excel File with Multiple Format Support ===\n",
        "print(f\"\\n📁 Loading Excel file: {EXCEL_PATH}\")\n",
        "\n",
        "def load_data_file(filepath):\n",
        "    \"\"\"Try multiple methods to load the data file.\"\"\"\n",
        "\n",
        "    # Method 1: Try as .xlsx with openpyxl\n",
        "    try:\n",
        "        df = pd.read_excel(filepath, engine=\"openpyxl\")\n",
        "        print(f\"✅ Loaded as .xlsx with openpyxl: {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e1:\n",
        "        print(f\"❌ openpyxl failed: {e1}\")\n",
        "\n",
        "    # Method 2: Try as .xls with xlrd\n",
        "    try:\n",
        "        df = pd.read_excel(filepath, engine=\"xlrd\")\n",
        "        print(f\"✅ Loaded as .xls with xlrd: {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e2:\n",
        "        print(f\"❌ xlrd failed: {e2}\")\n",
        "\n",
        "    # Method 3: Try without specifying engine\n",
        "    try:\n",
        "        df = pd.read_excel(filepath)\n",
        "        print(f\"✅ Loaded with default engine: {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e3:\n",
        "        print(f\"❌ Default engine failed: {e3}\")\n",
        "\n",
        "    # Method 4: Try as CSV\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"✅ Loaded as CSV: {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e4:\n",
        "        print(f\"❌ CSV failed: {e4}\")\n",
        "\n",
        "    # Method 5: Try as TSV\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, sep='\\t')\n",
        "        print(f\"✅ Loaded as TSV: {len(df)} rows\")\n",
        "        return df\n",
        "    except Exception as e5:\n",
        "        print(f\"❌ TSV failed: {e5}\")\n",
        "\n",
        "    # Method 6: Check if file exists and show info\n",
        "    import os\n",
        "    if os.path.exists(filepath):\n",
        "        file_size = os.path.getsize(filepath)\n",
        "        print(f\"📊 File exists: {filepath}\")\n",
        "        print(f\"📊 File size: {file_size:,} bytes\")\n",
        "\n",
        "        # Try to read first few bytes to determine format\n",
        "        with open(filepath, 'rb') as f:\n",
        "            header = f.read(8)\n",
        "            print(f\"📊 File header (hex): {header.hex()}\")\n",
        "\n",
        "        # Check for common file signatures\n",
        "        if header.startswith(b'PK'):\n",
        "            print(\"📊 File appears to be a zip/xlsx file but may be corrupted\")\n",
        "        elif header.startswith(b'\\xd0\\xcf'):\n",
        "            print(\"📊 File appears to be an old Excel .xls file\")\n",
        "        elif b',' in header or b'\\t' in header:\n",
        "            print(\"📊 File might be CSV or TSV\")\n",
        "    else:\n",
        "        print(f\"❌ File not found: {filepath}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# Try to load the file\n",
        "df = load_data_file(EXCEL_PATH)\n",
        "\n",
        "if df is None:\n",
        "    print(\"\\n🔄 Trying alternative file names...\")\n",
        "    # Try common variations\n",
        "    alternatives = [\n",
        "        \"All_entries_beads_cleaned.csv\",\n",
        "        \"All_entries_beads_cleaned.xls\",\n",
        "        \"All_entries_beads_cleaned.tsv\",\n",
        "        \"All_entries_beads_cleaned.txt\",\n",
        "        \"/content/All_entries_beads_cleaned.xlsx\",\n",
        "        \"/content/drive/MyDrive/All_entries_beads_cleaned.xlsx\"\n",
        "    ]\n",
        "\n",
        "    for alt_path in alternatives:\n",
        "        if os.path.exists(alt_path):\n",
        "            print(f\"📁 Found alternative: {alt_path}\")\n",
        "            df = load_data_file(alt_path)\n",
        "            if df is not None:\n",
        "                EXCEL_PATH = alt_path\n",
        "                break\n",
        "\n",
        "    if df is None:\n",
        "        print(\"\\n❌ Could not load data file. Please check:\")\n",
        "        print(\"1. File is uploaded to Colab or accessible from Drive\")\n",
        "        print(\"2. File name is correct\")\n",
        "        print(\"3. File is not corrupted\")\n",
        "        print(\"\\n📁 Looking for data files in current directory:\")\n",
        "        import glob\n",
        "        data_files = glob.glob(\"*.xlsx\") + glob.glob(\"*.xls\") + glob.glob(\"*.csv\")\n",
        "        if data_files:\n",
        "            print(\"Found these data files:\")\n",
        "            for f in data_files:\n",
        "                print(f\"  - {f}\")\n",
        "        else:\n",
        "            print(\"No data files found in current directory\")\n",
        "        raise Exception(\"Could not load data file\")\n",
        "\n",
        "# Successfully loaded - now check columns\n",
        "print(f\"\\n✅ Successfully loaded data: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "# Check for text column\n",
        "if TEXT_COLUMN not in df.columns:\n",
        "    print(f\"⚠️ Specified column '{TEXT_COLUMN}' not found!\")\n",
        "    print(f\"📋 Available columns: {list(df.columns)}\")\n",
        "\n",
        "    # Try to find alternative text column\n",
        "    text_column_candidates = ['highlight', 'text', 'content', 'excerpt', 'passage', 'text_content']\n",
        "    for col in df.columns:\n",
        "        if 'text' in col.lower() or 'highlight' in col.lower() or 'excerpt' in col.lower():\n",
        "            TEXT_COLUMN = col\n",
        "            print(f\"📝 Using detected text column: {TEXT_COLUMN}\")\n",
        "            break\n",
        "    else:\n",
        "        # If still not found, use the first string column with substantial text\n",
        "        for col in df.columns:\n",
        "            sample = df[col].dropna().head(1)\n",
        "            if len(sample) > 0 and isinstance(sample.iloc[0], str) and len(sample.iloc[0]) > 50:\n",
        "                TEXT_COLUMN = col\n",
        "                print(f\"📝 Using first text column with content: {TEXT_COLUMN}\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"❌ No suitable text column found!\")\n",
        "            print(\"Please specify the correct column name in TEXT_COLUMN variable\")\n",
        "            raise Exception(\"No text column found\")\n",
        "else:\n",
        "    print(f\"📝 Using text column: {TEXT_COLUMN}\")\n",
        "\n",
        "# Display sample data\n",
        "non_null_texts = df[TEXT_COLUMN].dropna()\n",
        "if len(non_null_texts) > 0:\n",
        "    sample_text = str(non_null_texts.iloc[0])[:200]\n",
        "    print(f\"\\n🔍 Sample text: {sample_text}...\")\n",
        "    print(f\"📊 Non-empty entries: {len(non_null_texts)}/{len(df)}\")\n",
        "\n",
        "# === Set Up Anthropic Client ===\n",
        "client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "# === Resume Functionality ===\n",
        "def get_resume_index():\n",
        "    \"\"\"Check for previous session and offer resume option.\"\"\"\n",
        "    index_path = os.path.join(GDRIVE_DIR, 'last_index.txt')\n",
        "    try:\n",
        "        if os.path.exists(index_path):\n",
        "            with open(index_path, 'r') as f:\n",
        "                last_index = int(f.read().strip())\n",
        "            print(f\"\\n🔄 Previous session detected!\")\n",
        "            print(f\"Last processed row: {last_index}\")\n",
        "            resume = input(f\"Resume from row {last_index + 1}? (y/n): \").strip().lower()\n",
        "            if resume == 'y':\n",
        "                return last_index + 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"🔁 Starting from beginning\")\n",
        "    return 0\n",
        "\n",
        "# === Main Processing Function ===\n",
        "def process_entries_structured():\n",
        "    \"\"\"Process entries with structured output.\"\"\"\n",
        "\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    responses = []\n",
        "    start_index = get_resume_index()\n",
        "\n",
        "    print(f\"\\n🚀 STARTING STRUCTURED PROCESSING\")\n",
        "    print(f\"Model: {MODEL_NAME}\")\n",
        "    print(f\"Temperature: {TEMPERATURE}\")\n",
        "    print(f\"Starting from row: {start_index}\")\n",
        "    print(f\"Processing up to row: {min(2000, len(df))}\")\n",
        "\n",
        "    log_path = os.path.join(GDRIVE_DIR, LOG_FILE)\n",
        "\n",
        "    # Pre-fill responses if resuming\n",
        "    if start_index > 0:\n",
        "        responses = [None] * start_index\n",
        "\n",
        "    with open(log_path, \"a\", encoding=\"utf-8\") as log:\n",
        "        log.write(f\"\\n=== Structured annotation started at {datetime.datetime.now().isoformat()} ===\\n\")\n",
        "\n",
        "        for i in range(start_index, min(2000, len(df))):\n",
        "            try:\n",
        "                row = df.iloc[i]\n",
        "                entry_text = row.get(TEXT_COLUMN)\n",
        "\n",
        "                # Check for empty text\n",
        "                if pd.isna(entry_text) or entry_text is None:\n",
        "                    responses.append(None)\n",
        "                    print(f\"⏭️  Row {i}: Empty text, skipping\")\n",
        "                    continue\n",
        "\n",
        "                entry_text = str(entry_text).strip()\n",
        "\n",
        "                if not entry_text or entry_text == 'nan':\n",
        "                    responses.append(None)\n",
        "                    print(f\"⏭️  Row {i}: Empty text, skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Construct prompt\n",
        "                prompt = construct_structured_prompt(entry_text)\n",
        "\n",
        "                print(f\"\\n🔍 Processing row {i}/{min(2000, len(df))}\")\n",
        "                print(f\"📝 Text preview: {entry_text[:100]}...\")\n",
        "\n",
        "                # Call Claude API with temperature\n",
        "                response = client.messages.create(\n",
        "                    model=MODEL_NAME,\n",
        "                    max_tokens=3000,  # Increased for complete responses\n",
        "                    temperature=TEMPERATURE,  # Set to 0.2 for consistency\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                )\n",
        "\n",
        "                total_input_tokens += response.usage.input_tokens\n",
        "                total_output_tokens += response.usage.output_tokens\n",
        "                response_text = response.content[0].text.strip()\n",
        "\n",
        "                # Parse XML to flat JSON\n",
        "                parsed_data = parse_xml_to_flat_json(response_text)\n",
        "\n",
        "                # Add raw response for reference\n",
        "                parsed_data['raw_llm_response'] = response_text[:1000]  # Keep first 1000 chars\n",
        "\n",
        "                responses.append(parsed_data)\n",
        "\n",
        "                # Count successful fields\n",
        "                field_count = len([k for k in parsed_data.keys() if not k.startswith('error')])\n",
        "                print(f\"✅ Row {i}: Extracted {field_count} fields\")\n",
        "\n",
        "                timestamp = datetime.datetime.now().isoformat()\n",
        "                log.write(f\"[{timestamp}] Row {i}: Success - {field_count} fields\\n\")\n",
        "\n",
        "                # Auto-save\n",
        "                if (i + 1) % SAVE_EVERY == 0:\n",
        "                    save_intermediate_results(i, responses, total_input_tokens, total_output_tokens)\n",
        "\n",
        "                # Cost tracking\n",
        "                if i % 10 == 0:\n",
        "                    current_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "                    print(f\"💰 Current cost: ${current_cost:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                timestamp = datetime.datetime.now().isoformat()\n",
        "                error_msg = f\"[{timestamp}] Row {i}: Error - {str(e)}\\n\"\n",
        "                log.write(error_msg)\n",
        "                print(f\"❌ {error_msg.strip()}\")\n",
        "                responses.append({\"error\": str(e), \"row\": i})\n",
        "\n",
        "        log.write(f\"=== Annotation completed at {datetime.datetime.now().isoformat()} ===\\n\")\n",
        "\n",
        "    return responses, total_input_tokens, total_output_tokens\n",
        "\n",
        "def calculate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Calculate API cost based on token usage.\"\"\"\n",
        "    # Adjust these rates based on actual Claude Sonnet 4 pricing\n",
        "    input_rate = 0.003 / 1000  # $3 per million input tokens\n",
        "    output_rate = 0.015 / 1000  # $15 per million output tokens\n",
        "    return (input_tokens * input_rate) + (output_tokens * output_rate)\n",
        "\n",
        "def save_intermediate_results(current_row, responses, input_tokens, output_tokens, batch_start=0):\n",
        "    \"\"\"Save intermediate results with clean structure.\"\"\"\n",
        "\n",
        "    print(f\"\\n💾 SAVING INTERMEDIATE RESULTS (Row {current_row})\")\n",
        "\n",
        "    # Save current index\n",
        "    with open(os.path.join(GDRIVE_DIR, 'last_index.txt'), 'w') as f:\n",
        "        f.write(str(current_row))\n",
        "\n",
        "    # Create results dataframe - only for the current batch\n",
        "    rows_processed = current_row - batch_start + 1\n",
        "    df_partial = df.iloc[batch_start:current_row + 1].copy()\n",
        "\n",
        "    # Convert responses to dataframe and merge\n",
        "    if responses[batch_start:current_row + 1]:\n",
        "        # Get the relevant slice of responses\n",
        "        batch_responses = responses[batch_start:current_row + 1]\n",
        "        valid_responses = [r for r in batch_responses if r is not None]\n",
        "\n",
        "        if valid_responses:\n",
        "            # Create dataframe from responses\n",
        "            response_df = pd.DataFrame(valid_responses)\n",
        "\n",
        "            # Add each column to the partial dataframe\n",
        "            for col in response_df.columns:\n",
        "                # Create column values for this batch\n",
        "                col_values = []\n",
        "                for idx, resp in enumerate(batch_responses):\n",
        "                    if resp and isinstance(resp, dict):\n",
        "                        col_values.append(resp.get(col))\n",
        "                    else:\n",
        "                        col_values.append(None)\n",
        "\n",
        "                df_partial[col] = col_values\n",
        "\n",
        "    # Save batch-specific file\n",
        "    batch_num = (batch_start // BATCH_SIZE) + 1\n",
        "    intermediate_path = os.path.join(GDRIVE_DIR, f\"batch_{batch_num}_rows_{batch_start}_{current_row}.xlsx\")\n",
        "    df_partial.to_excel(intermediate_path, index=False)\n",
        "\n",
        "    # Save cost summary\n",
        "    cost = calculate_cost(input_tokens, output_tokens)\n",
        "    cost_summary = f\"\"\"Structured Bead Coding - Batch {batch_num}\n",
        "Rows: {batch_start} to {current_row}\n",
        "Rows processed in batch: {rows_processed}\n",
        "Input tokens: {input_tokens:,}\n",
        "Output tokens: {output_tokens:,}\n",
        "Estimated cost: ${cost:.4f}\n",
        "Timestamp: {datetime.datetime.now().isoformat()}\n",
        "\"\"\"\n",
        "\n",
        "    with open(os.path.join(GDRIVE_DIR, f'batch_{batch_num}_cost.txt'), 'w') as f:\n",
        "        f.write(cost_summary)\n",
        "\n",
        "    print(f\"✅ Saved batch {batch_num}: rows {batch_start}-{current_row}\")\n",
        "    print(f\"💰 Batch cost so far: ${cost:.4f}\")\n",
        "\n",
        "# === Execute Processing ===\n",
        "print(f\"\\n🚀 Starting structured processing...\")\n",
        "responses, total_input_tokens, total_output_tokens, batch_start, batch_end = process_entries_structured()\n",
        "\n",
        "# === Generate Final Results ===\n",
        "print(f\"\\n🎉 BATCH PROCESSING COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if responses[batch_start:batch_end]:\n",
        "    # Create final dataframe for this batch\n",
        "    batch_df = df.iloc[batch_start:batch_end].copy()\n",
        "\n",
        "    # Add all response columns\n",
        "    batch_responses = responses[batch_start:batch_end]\n",
        "    valid_responses = [r for r in batch_responses if r is not None and isinstance(r, dict)]\n",
        "\n",
        "    if valid_responses:\n",
        "        # Get all unique keys across all responses\n",
        "        all_keys = set()\n",
        "        for resp in valid_responses:\n",
        "            all_keys.update(resp.keys())\n",
        "\n",
        "        # Sort keys to maintain consistent column order\n",
        "        sorted_keys = sorted(all_keys)\n",
        "\n",
        "        # Add each column\n",
        "        for key in sorted_keys:\n",
        "            col_values = []\n",
        "            for resp in batch_responses:\n",
        "                if resp and isinstance(resp, dict):\n",
        "                    col_values.append(resp.get(key))\n",
        "                else:\n",
        "                    col_values.append(None)\n",
        "            batch_df[key] = col_values\n",
        "\n",
        "    # Save batch results\n",
        "    batch_num = (batch_start // BATCH_SIZE) + 1\n",
        "    final_path = os.path.join(GDRIVE_DIR, f\"batch_{batch_num}_final_rows_{batch_start}_{batch_end-1}.xlsx\")\n",
        "    batch_df.to_excel(final_path, index=False)\n",
        "\n",
        "    # Count columns\n",
        "    new_columns = [col for col in batch_df.columns if col not in df.columns]\n",
        "\n",
        "    # Create summary report\n",
        "    final_cost = calculate_cost(total_input_tokens, total_output_tokens)\n",
        "    total_batches = (len(df) // BATCH_SIZE) + (1 if len(df) % BATCH_SIZE else 0)\n",
        "\n",
        "    summary_report = f\"\"\"\n",
        "STRUCTURED BEAD CODING - BATCH {batch_num}/{total_batches} COMPLETE\n",
        "============================================================\n",
        "\n",
        "📊 BATCH SUMMARY:\n",
        "- Batch number: {batch_num} of {total_batches}\n",
        "- Rows processed: {batch_start} to {batch_end-1}\n",
        "- Total entries in batch: {batch_end - batch_start}\n",
        "- Successful extractions: {len([r for r in batch_responses if r and not r.get('error')])}\n",
        "- New columns created: {len(new_columns)}\n",
        "- Input tokens used: {total_input_tokens:,}\n",
        "- Output tokens used: {total_output_tokens:,}\n",
        "- Batch cost: ${final_cost:.4f}\n",
        "\n",
        "📊 OVERALL PROGRESS:\n",
        "- Total dataset: {len(df)} rows\n",
        "- Completed so far: {batch_end} rows ({(batch_end/len(df))*100:.1f}%)\n",
        "- Remaining: {len(df) - batch_end} rows\n",
        "- Batches remaining: {total_batches - batch_num}\n",
        "\n",
        "📁 OUTPUT FILES:\n",
        "- Batch results: batch_{batch_num}_final_rows_{batch_start}_{batch_end-1}.xlsx\n",
        "- Session logs: {LOG_FILE}\n",
        "- Cost summary: batch_{batch_num}_cost.txt\n",
        "\n",
        "🎯 NEXT STEPS:\n",
        "To continue processing:\n",
        "1. Run this notebook again\n",
        "2. Choose batch {batch_num + 1} when prompted\n",
        "3. Or modify START_ROW = {batch_end} to continue\n",
        "\n",
        "To combine all batches (after all processing):\n",
        "Use the merge script to combine all batch files into one master file\n",
        "\n",
        "Generated: {datetime.datetime.now().isoformat()}\n",
        "\"\"\"\n",
        "\n",
        "    print(summary_report)\n",
        "\n",
        "    # Save summary\n",
        "    with open(os.path.join(GDRIVE_DIR, f'batch_{batch_num}_summary.txt'), 'w') as f:\n",
        "        f.write(summary_report)\n",
        "\n",
        "    # Update batch info for next run\n",
        "    with open(os.path.join(GDRIVE_DIR, 'last_batch.txt'), 'w') as f:\n",
        "        f.write(str(batch_num))\n",
        "\n",
        "    print(f\"\\n🎉 BATCH {batch_num} SAVED TO: {GDRIVE_DIR}\")\n",
        "    print(f\"✅ Files saved: batch_{batch_num}_final_rows_{batch_start}_{batch_end-1}.xlsx\")\n",
        "\n",
        "    # Display column summary\n",
        "    print(f\"\\n📊 COLUMN SUMMARY:\")\n",
        "    print(f\"Original columns: {len(df.columns)}\")\n",
        "    print(f\"New variable columns: {len([c for c in new_columns if not c.endswith('_detail')])}\")\n",
        "    print(f\"New detail columns: {len([c for c in new_columns if c.endswith('_detail')])}\")\n",
        "\n",
        "    # Show instructions for next batch\n",
        "    if batch_end < len(df):\n",
        "        print(f\"\\n⏭️  TO CONTINUE:\")\n",
        "        print(f\"1. Run this notebook again\")\n",
        "        print(f\"2. When prompted, select batch {batch_num + 1}\")\n",
        "        print(f\"3. This will process rows {batch_end} to {min(batch_end + BATCH_SIZE, len(df))-1}\")\n",
        "    else:\n",
        "        print(f\"\\n✅ ALL ROWS PROCESSED!\")\n",
        "        print(f\"Total batches completed: {batch_num}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ No entries were processed. Please check your data and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U6pMTa6wuz67",
        "outputId": "b466eb25-5549-4576-9710-fa6cc46658ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "🔬 STRUCTURED BEAD TRADE CODING - V3\n",
            "==================================================\n",
            "🎯 Goal: Clean structured output with descriptive details\n",
            "🎯 Goal: Non-nested columns for statistical analysis\n",
            "🎯 Goal: Preserve rich historical context\n",
            "🎯 Using: Claude Sonnet 4 with temperature 0.2\n",
            "==================================================\n",
            "\n",
            "📁 Loading Excel file: All_entries_beads_cleaned.xlsx\n",
            "✅ Loaded as .xlsx with openpyxl: 27060 rows\n",
            "\n",
            "✅ Successfully loaded data: 27060 rows, 15 columns\n",
            "📝 Using text column: text_page_gp\n",
            "\n",
            "🔍 Sample text:  f WANDERINGS IN BIBLE LANDS. 205 south of Judah (1 Chron. 4: 32), which I have placed at the ruin of 'Aztun; and Sam/u (or as it may be otherwise rendered Sama) is the large ruin of Samah, on the hig...\n",
            "📊 Non-empty entries: 26916/27060\n",
            "\n",
            "🚀 Starting structured processing...\n",
            "🔁 Starting from beginning\n",
            "\n",
            "🚀 STARTING STRUCTURED PROCESSING\n",
            "Model: claude-sonnet-4-20250514\n",
            "Temperature: 0.2\n",
            "Starting from row: 0\n",
            "Processing up to row: 2000\n",
            "\n",
            "🔍 Processing row 0/2000\n",
            "📝 Text preview: f WANDERINGS IN BIBLE LANDS. 205 south of Judah (1 Chron. 4: 32), which I have placed at the ruin of...\n",
            "✅ Row 0: Extracted 49 fields\n",
            "💰 Current cost: $0.0311\n",
            "\n",
            "🔍 Processing row 1/2000\n",
            "📝 Text preview: KINGDOM OF MARR VECOS Fo.4. They wear woolen cloaks called cur?ias corcas that reach up to their elb...\n",
            "✅ Row 1: Extracted 49 fields\n",
            "⏭️  Row 2: Empty text, skipping\n",
            "\n",
            "🔍 Processing row 3/2000\n",
            "📝 Text preview: WANDERINGS IN BIBLE LANDS. 271 Before reaching the first cataract the scenery along the Nile changes...\n",
            "✅ Row 3: Extracted 49 fields\n",
            "\n",
            "🔍 Processing row 4/2000\n",
            "📝 Text preview: BOOK FOUR OF the king's all the penalties, ailments, where the warden resides together and slanders,...\n",
            "✅ Row 4: Extracted 49 fields\n",
            "\n",
            "🔍 Processing row 5/2000\n",
            "📝 Text preview: WANDERINGS IN BIBLE LANDS. 273 conducts himself in accordance with the standard rules of society he ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2140385915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;31m# === Execute Processing ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n🚀 Starting structured processing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m \u001b[0mresponses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_input_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_entries_structured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;31m# === Generate Final Results ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2140385915.py\u001b[0m in \u001b[0;36mprocess_entries_structured\u001b[0;34m()\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;31m# Call Claude API with temperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 response = client.messages.create(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Increased for complete responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m             )\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         )\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m   1048\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}